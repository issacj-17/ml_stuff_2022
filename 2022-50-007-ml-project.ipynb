{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","cell_id":"ac98df94-54a7-4e1a-8138-7f5c7571be69","deepnote_cell_height":369,"deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":14322146,"execution_start":1658682577993,"source_hash":"f2620bd0","trusted":true},"outputs":[],"source":["# # This Python 3 environment comes with many helpful analytics libraries installed\n","# # It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# # For example, here's several helpful packages to load\n","\n","# import numpy as np # linear algebra\n","# import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","# # Input data files are available in the read-only \"../input/\" directory\n","# # For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","# import os\n","# for dirname, _, filenames in os.walk('/kaggle/input'):\n","#     for filename in filenames:\n","#         print(os.path.join(dirname, filename))\n","\n","# # You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n","# # You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["import time\n","start_time = time.time()"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Process Initialised\n"]}],"source":["print(\"Process Initialised\")"]},{"cell_type":"markdown","metadata":{"cell_id":"00001-86291d65-f0ea-4c22-8d29-15f5f0273874","deepnote_cell_height":82,"deepnote_cell_type":"markdown"},"source":["# Load Training Dataset"]},{"cell_type":"code","execution_count":4,"metadata":{"cell_id":"eef26f675db14448bd91c5b241573c9a","deepnote_cell_height":99,"deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":11860273,"execution_start":1658682577995,"source_hash":"b7dcc9c8","tags":[],"trusted":true},"outputs":[],"source":["import numpy as np\n","import pandas as pd"]},{"cell_type":"code","execution_count":5,"metadata":{"cell_id":"00002-a0331af6-c3fd-496c-b8c9-2e9fbcf41caa","deepnote_cell_height":99,"deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":1,"execution_start":1658682578049,"source_hash":"3f76624c","trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>label</th>\n","      <th>0</th>\n","      <th>1</th>\n","      <th>2</th>\n","      <th>3</th>\n","      <th>4</th>\n","      <th>5</th>\n","      <th>6</th>\n","      <th>7</th>\n","      <th>...</th>\n","      <th>4990</th>\n","      <th>4991</th>\n","      <th>4992</th>\n","      <th>4993</th>\n","      <th>4994</th>\n","      <th>4995</th>\n","      <th>4996</th>\n","      <th>4997</th>\n","      <th>4998</th>\n","      <th>4999</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3</td>\n","      <td>1</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>4</td>\n","      <td>0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>5</td>\n","      <td>1</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>17179</th>\n","      <td>17180</td>\n","      <td>0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>17180</th>\n","      <td>17181</td>\n","      <td>0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>17181</th>\n","      <td>17182</td>\n","      <td>1</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>17182</th>\n","      <td>17183</td>\n","      <td>1</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>17183</th>\n","      <td>17184</td>\n","      <td>0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>17184 rows × 5002 columns</p>\n","</div>"],"text/plain":["          id  label    0    1    2    3    4    5    6    7  ...  4990  4991  \\\n","0          1      1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n","1          2      0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n","2          3      1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n","3          4      0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n","4          5      1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n","...      ...    ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   ...   ...   \n","17179  17180      0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n","17180  17181      0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n","17181  17182      1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n","17182  17183      1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n","17183  17184      0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n","\n","       4992  4993  4994  4995  4996  4997  4998  4999  \n","0       0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n","1       0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n","2       0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n","3       0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n","4       0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n","...     ...   ...   ...   ...   ...   ...   ...   ...  \n","17179   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n","17180   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n","17181   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n","17182   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n","17183   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n","\n","[17184 rows x 5002 columns]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["CPU times: total: 16.2 s\n","Wall time: 16.3 s\n"]}],"source":["%%time\n","df_train = pd.read_csv(r\"./source/train_tfidf_features.csv\")\n","display(df_train)"]},{"cell_type":"code","execution_count":6,"metadata":{"cell_id":"00003-d747b669-d32c-43a4-b111-dea206d21537","deepnote_cell_height":534.796875,"deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":16711,"execution_start":1658682597472,"source_hash":"849ce5a8","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["CPU times: total: 10.8 s\n","Wall time: 11 s\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>label</th>\n","      <th>0</th>\n","      <th>1</th>\n","      <th>2</th>\n","      <th>3</th>\n","      <th>4</th>\n","      <th>5</th>\n","      <th>6</th>\n","      <th>7</th>\n","      <th>...</th>\n","      <th>4990</th>\n","      <th>4991</th>\n","      <th>4992</th>\n","      <th>4993</th>\n","      <th>4994</th>\n","      <th>4995</th>\n","      <th>4996</th>\n","      <th>4997</th>\n","      <th>4998</th>\n","      <th>4999</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>count</th>\n","      <td>17184.000000</td>\n","      <td>17184.000000</td>\n","      <td>17184.000000</td>\n","      <td>17184.000000</td>\n","      <td>17184.000000</td>\n","      <td>17184.000000</td>\n","      <td>17184.000000</td>\n","      <td>17184.000000</td>\n","      <td>17184.000000</td>\n","      <td>17184.000000</td>\n","      <td>...</td>\n","      <td>17184.000000</td>\n","      <td>17184.000000</td>\n","      <td>17184.000000</td>\n","      <td>17184.000000</td>\n","      <td>17184.000000</td>\n","      <td>17184.000000</td>\n","      <td>17184.000000</td>\n","      <td>17184.000000</td>\n","      <td>17184.000000</td>\n","      <td>17184.000000</td>\n","    </tr>\n","    <tr>\n","      <th>mean</th>\n","      <td>8592.500000</td>\n","      <td>0.381227</td>\n","      <td>0.000150</td>\n","      <td>0.001066</td>\n","      <td>0.001532</td>\n","      <td>0.000369</td>\n","      <td>0.000140</td>\n","      <td>0.000066</td>\n","      <td>0.000270</td>\n","      <td>0.000483</td>\n","      <td>...</td>\n","      <td>0.000202</td>\n","      <td>0.000429</td>\n","      <td>0.000286</td>\n","      <td>0.000075</td>\n","      <td>0.000260</td>\n","      <td>0.000709</td>\n","      <td>0.000257</td>\n","      <td>0.000121</td>\n","      <td>0.000308</td>\n","      <td>0.000159</td>\n","    </tr>\n","    <tr>\n","      <th>std</th>\n","      <td>4960.737848</td>\n","      <td>0.485702</td>\n","      <td>0.008297</td>\n","      <td>0.019532</td>\n","      <td>0.024741</td>\n","      <td>0.012334</td>\n","      <td>0.008276</td>\n","      <td>0.005065</td>\n","      <td>0.009907</td>\n","      <td>0.013106</td>\n","      <td>...</td>\n","      <td>0.010215</td>\n","      <td>0.013178</td>\n","      <td>0.011378</td>\n","      <td>0.005866</td>\n","      <td>0.010864</td>\n","      <td>0.017641</td>\n","      <td>0.010246</td>\n","      <td>0.006529</td>\n","      <td>0.010526</td>\n","      <td>0.008536</td>\n","    </tr>\n","    <tr>\n","      <th>min</th>\n","      <td>1.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>...</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>25%</th>\n","      <td>4296.750000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>...</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>50%</th>\n","      <td>8592.500000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>...</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>75%</th>\n","      <td>12888.250000</td>\n","      <td>1.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>...</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>max</th>\n","      <td>17184.000000</td>\n","      <td>1.000000</td>\n","      <td>0.676327</td>\n","      <td>0.560830</td>\n","      <td>0.958430</td>\n","      <td>0.646740</td>\n","      <td>0.532789</td>\n","      <td>0.437760</td>\n","      <td>0.435835</td>\n","      <td>0.536746</td>\n","      <td>...</td>\n","      <td>0.611122</td>\n","      <td>0.540809</td>\n","      <td>0.566613</td>\n","      <td>0.592170</td>\n","      <td>0.617341</td>\n","      <td>0.850605</td>\n","      <td>0.484908</td>\n","      <td>0.398105</td>\n","      <td>0.430031</td>\n","      <td>0.528556</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>8 rows × 5002 columns</p>\n","</div>"],"text/plain":["                 id         label             0             1             2  \\\n","count  17184.000000  17184.000000  17184.000000  17184.000000  17184.000000   \n","mean    8592.500000      0.381227      0.000150      0.001066      0.001532   \n","std     4960.737848      0.485702      0.008297      0.019532      0.024741   \n","min        1.000000      0.000000      0.000000      0.000000      0.000000   \n","25%     4296.750000      0.000000      0.000000      0.000000      0.000000   \n","50%     8592.500000      0.000000      0.000000      0.000000      0.000000   \n","75%    12888.250000      1.000000      0.000000      0.000000      0.000000   \n","max    17184.000000      1.000000      0.676327      0.560830      0.958430   \n","\n","                  3             4             5             6             7  \\\n","count  17184.000000  17184.000000  17184.000000  17184.000000  17184.000000   \n","mean       0.000369      0.000140      0.000066      0.000270      0.000483   \n","std        0.012334      0.008276      0.005065      0.009907      0.013106   \n","min        0.000000      0.000000      0.000000      0.000000      0.000000   \n","25%        0.000000      0.000000      0.000000      0.000000      0.000000   \n","50%        0.000000      0.000000      0.000000      0.000000      0.000000   \n","75%        0.000000      0.000000      0.000000      0.000000      0.000000   \n","max        0.646740      0.532789      0.437760      0.435835      0.536746   \n","\n","       ...          4990          4991          4992          4993  \\\n","count  ...  17184.000000  17184.000000  17184.000000  17184.000000   \n","mean   ...      0.000202      0.000429      0.000286      0.000075   \n","std    ...      0.010215      0.013178      0.011378      0.005866   \n","min    ...      0.000000      0.000000      0.000000      0.000000   \n","25%    ...      0.000000      0.000000      0.000000      0.000000   \n","50%    ...      0.000000      0.000000      0.000000      0.000000   \n","75%    ...      0.000000      0.000000      0.000000      0.000000   \n","max    ...      0.611122      0.540809      0.566613      0.592170   \n","\n","               4994          4995          4996          4997          4998  \\\n","count  17184.000000  17184.000000  17184.000000  17184.000000  17184.000000   \n","mean       0.000260      0.000709      0.000257      0.000121      0.000308   \n","std        0.010864      0.017641      0.010246      0.006529      0.010526   \n","min        0.000000      0.000000      0.000000      0.000000      0.000000   \n","25%        0.000000      0.000000      0.000000      0.000000      0.000000   \n","50%        0.000000      0.000000      0.000000      0.000000      0.000000   \n","75%        0.000000      0.000000      0.000000      0.000000      0.000000   \n","max        0.617341      0.850605      0.484908      0.398105      0.430031   \n","\n","               4999  \n","count  17184.000000  \n","mean       0.000159  \n","std        0.008536  \n","min        0.000000  \n","25%        0.000000  \n","50%        0.000000  \n","75%        0.000000  \n","max        0.528556  \n","\n","[8 rows x 5002 columns]"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["%%time\n","df_train.describe()"]},{"cell_type":"markdown","metadata":{},"source":["# Task 1: Implement Logistic Regression\n","Recalled that you have learned about Logistic Regression in your earlier class. Your task is to implement a Logistic Regression model from scratch. \\\n","Note that you are NOT TO USE the sklearn logistic regression package or any other pre-defined logistic regression package for this task! \\\n","Usage of any logistic regression packages will result in 0 marks for this task.\n","\n","## Key Task Deliverables\n","1a. Code implementation of the Logistic Regression model. \\\n","1b. Prediction made by your Logistic Regression on the Test set. Note that you are welcome to submit your predicted labels to Kaggle but you will need to submit the final prediction output in the final project submission. Please label the file as \"LogRed_Prediction.csv\"."]},{"cell_type":"markdown","metadata":{"cell_id":"00005-49b9c0d6-4381-409b-82b2-2a298f466e01","deepnote_cell_height":52.390625,"deepnote_cell_type":"markdown"},"source":["-- `sigmoid(z)`: A function that takes in a Real Number input and returns an output value between 0 and 1."]},{"cell_type":"code","execution_count":7,"metadata":{"cell_id":"00006-2e3c692b-00a7-4160-acbf-0da222258054","deepnote_cell_height":135,"deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":0,"execution_start":1658687327448,"source_hash":"f9e00733","trusted":true},"outputs":[],"source":["def sigmoid(z):\n","    result = 1/(1 + np.exp(-z))\n","#     print(f\"sigmoid: {result}\")\n","    return result"]},{"cell_type":"markdown","metadata":{"cell_id":"00007-968b9856-b6f3-40bd-9301-356dee468412","deepnote_cell_height":97.1875,"deepnote_cell_type":"markdown"},"source":["-- `loss(y, y_hat)`: A loss function that allows us to minimize and determine the optimal parameters. The function takes in the actual labels y and the predicted labels yhat, and returns the overall training loss. Note that you should be using the Log Loss function taught in class."]},{"cell_type":"markdown","metadata":{"cell_id":"3bd8cc27f03146c58682dabe9bdefae8","deepnote_cell_height":97.1875,"deepnote_cell_type":"markdown","tags":[]},"source":["Note: We have decided the add a regulariser (denoted by the `lmb` term) to observe whether there is an improvement in utilising L2 regularisation in our Logisitic Regression Model. As such, we have decided to change the arguments of the function  to accomodate for regularisation."]},{"cell_type":"code","execution_count":8,"metadata":{"cell_id":"00008-79c60bbd-81a5-460c-91a3-d3d8b7cbfd6b","deepnote_cell_height":333,"deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":0,"execution_start":1658687327449,"source_hash":"296019a7","trusted":true},"outputs":[],"source":["def loss(y, X, w, b, lmb):\n","    y_hat = sigmoid(np.dot(X, w) + b)\n","    m = np.shape(y)[0]\n","    \n","    loss = -1 * np.where(y == 1, np.log(y_hat), np.log(1 - y_hat)).mean()\n","    reg = lmb * np.sum(w**2) / (2 * m)\n","    error = loss + reg\n","    \n","#     print(f\"training loss = {loss}, regularisation term = {reg}, training error = {error}\")\n","    return error"]},{"cell_type":"markdown","metadata":{"cell_id":"00009-9fef5aa0-38be-400f-a766-b0fbdaa5d8d6","deepnote_cell_height":97.1875,"deepnote_cell_type":"markdown"},"source":["-- `gradients(X, y, y_hat)`: The Gradient Descent Algorithm to find the optimal values of our parameters. The function takes in the training feature X, actual labels y and the predicted labels yhat, and returns the partial derivative of the Loss function with respect to weights (dw) and bias (db)."]},{"cell_type":"markdown","metadata":{"cell_id":"5d4d961b83aa42039d5853a59939b89a","deepnote_cell_height":52.390625,"deepnote_cell_type":"markdown","tags":[]},"source":["Likewise, the arguments of the `gradients` function has been altered to accommodate for L2 regularisation."]},{"cell_type":"code","execution_count":9,"metadata":{"cell_id":"00010-20d9c8e4-9d38-4ed5-95f4-3c2fd6c26fb1","deepnote_cell_height":243,"deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":2,"execution_start":1658687327454,"source_hash":"8919acd2","trusted":true},"outputs":[],"source":["def gradients(y, X, w, b, lmb):\n","    # m - number of training examples\n","    m = np.shape(X)[0]\n","    y_hat = sigmoid(np.dot(X, w) + b)\n","    \n","    dw = (1 / m) * (np.dot(X.T, (y_hat - y)) + lmb * w)\n","    db = (1 / m) * np.sum((y_hat - y))\n","    \n","#     print(f\"dw: {dw}, db: {db}\")\n","    return dw, db"]},{"cell_type":"markdown","metadata":{"cell_id":"00011-2af85165-6958-44ec-a0d5-2fa2a2a13a7c","deepnote_cell_height":52.390625,"deepnote_cell_type":"markdown"},"source":["-- `train(X, y, bs, epochs, lr)`: The training function for your model."]},{"cell_type":"markdown","metadata":{"cell_id":"59627bc57f214e5aab30f859f0436502","deepnote_cell_height":74.796875,"deepnote_cell_type":"markdown","tags":[]},"source":["We added the `C` term to represent the penalty term `lmb`. The relationship is that `lmb = 1/C` if `C != 0`. Otherwise,`lmb = 0`, where we do not apply regularisation."]},{"cell_type":"code","execution_count":10,"metadata":{"cell_id":"00012-1ef81ca6-ff76-4273-8db1-e073ae3b5b60","deepnote_cell_height":1377,"deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":42,"execution_start":1658685590567,"source_hash":"44826cb7","trusted":true},"outputs":[],"source":["# @param X - features\n","# @param y - labels\n","# @param bs - batch size\n","# @param epochs - number of iterations through dataset\n","# @param lr - learning rate\n","\n","def train(X, y, bs, epochs, lr, C):\n","    lmb = 0 if C == 0 else 1/C\n","    \n","    # n - number of training examples, d - number of features\n","    n, d = np.shape(X)\n","    \n","    randomize = np.arange(n)\n","    rng = np.random.default_rng(100)\n","    \n","    w = np.zeros((d, 1))\n","    b = 0\n","#     w = rng.uniform(size=(d,1))\n","#     b = rng.random()\n","    \n","    y = y.reshape(n, 1)\n","    \n","    old_losses = []\n","    old_w = []\n","    old_b = []\n","    \n","    old_w.append(w.copy())\n","    old_b.append(b)\n","    l = loss(y, X, w, b, lmb)\n","    old_losses.append(l)\n","    \n","    for epoch in range(epochs):\n","        limit = n // bs\n","#         print(f\"limit: {limit}\")\n","\n","        for i in range(limit):\n","            start = i * bs\n","            end = start + bs\n","#             print(f\"epoch: {epoch}, start: {start}, end: {end}\")\n","            \n","            rng.shuffle(randomize)\n","            choice = randomize[start:end]\n","#             print(f\"choice: {choice}\")\n","            X_batch = X[choice]\n","            y_batch = y[choice]\n","            \n","            loss_old = loss(y, X, w, b, lmb)\n","            \n","            dw, db = gradients(y_batch, X_batch, w, b, lmb)\n","            w_new = w.copy() - lr * dw\n","            b_new = b - lr * db\n","            loss_new = loss(y, X, w_new, b_new, lmb)\n","            \n","            if (loss_new < loss_old):\n","#                 print(w == w_new, b == b_new);\n","#                 print(f\"loss_new: {loss_new}, loss_old: {loss_old}\")\n","\n","                w = w_new\n","                b = b_new\n","                old_w.append(w_new)\n","                old_b.append(b_new)\n","                old_losses.append(loss_new)\n","    \n","    min_loss = min(old_losses)\n","    min_index = old_losses.index(min_loss)\n","    \n","#     print(f\"old_w: {old_w}\")\n","#     print(f\"old_b: {old_b}\")\n","#     print(f\"old_losses: {old_losses}\")\n","#     print(f\"min_loss: {min_loss}\")\n","#     print(f\"min_index:\", min_index)\n","    \n","    return old_w[min_index], old_b[min_index], old_losses"]},{"cell_type":"markdown","metadata":{"cell_id":"00019-ced096de-42a1-4c68-ade7-88807850e19b","deepnote_cell_height":52.390625,"deepnote_cell_type":"markdown"},"source":["-- `predict(X, w, b)`: The prediction function where you can apply your validation and test sets."]},{"cell_type":"code","execution_count":11,"metadata":{"cell_id":"00020-e9cb6e0c-03aa-42db-943c-b6bb4a9913f0","deepnote_cell_height":135,"deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":6,"execution_start":1658687691735,"source_hash":"7d794aee","trusted":true},"outputs":[],"source":["def predict(X, w, b):\n","    y_pred = sigmoid(np.dot(X, w) + b)\n","    pred_labels = np.array([1 if i >= 0.5 else 0 for i in y_pred])\n","    return pred_labels"]},{"cell_type":"markdown","metadata":{"cell_id":"00021-c62a3fc4-3c62-432f-9f13-64ea72f7e3da","deepnote_cell_height":212,"deepnote_cell_type":"markdown"},"source":["## Performance Evaluation\n","\n","As per the grading rubric - \"Perfect Implementation of the Logistics Regression algorithm. Successfully trained the implemented model with the train set and achieved comparative performance compared to SKLearn Logistic Regression package\", we shall compare the performance of our model with the SKLearn LogisticRegression and SGDClassifier package.\n","\n","We shall first implement a function to evaluate the accuracy of our model. The goal is to achieve a Macro-F1 score that is within 0.05 of the Macro-F1 score of the SKLearn Package"]},{"cell_type":"code","execution_count":12,"metadata":{"cell_id":"00024-700a0602-b8f9-47d5-85c9-dfb5b28bf780","deepnote_cell_height":117,"deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":3,"execution_start":1658687562415,"source_hash":"72d19504","trusted":true},"outputs":[],"source":["from sklearn.linear_model import LogisticRegression, SGDClassifier\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import f1_score"]},{"cell_type":"code","execution_count":13,"metadata":{"cell_id":"00022-a17b0e65-fcbb-44d1-8ded-a7923d6d255c","deepnote_cell_height":225,"deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":1,"execution_start":1658687514001,"source_hash":"754349d7","trusted":true},"outputs":[],"source":["def score(y, y_hat):\n","    accuracy = np.sum(y == y_hat) / np.shape(y)[0]\n","    f1score = f1_score(y, y_hat, average='macro')\n","\n","    print(f\"Accuracy: {accuracy}\")\n","    print(f\"Macro-F1 score: {f1score}\")\n","\n","    # Return Macro-F1 score of the model\n","    return f1score"]},{"cell_type":"code","execution_count":14,"metadata":{"cell_id":"81cc9876cd5a44c9bf079ddd3cc51847","deepnote_cell_height":315,"deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":2,"execution_start":1658685600957,"source_hash":"c6dbbe00","tags":[],"trusted":true},"outputs":[],"source":["def perform(LogReg, SGD, **scores):\n","    models = []\n","\n","    for model, score in scores.items():\n","        result = max(abs(LogReg - score), abs(SGD - score))\n","        print(f\"Model: {model}, Macro-F1 Score: {score}, Difference: {result}\")\n","\n","        if result <= 0.05:\n","            models.append(model)\n","    \n","    quality = \"Success\" if len(models) > 0 else \"Failed\"\n","    print(f\"Model {quality}\")\n","\n","    return models"]},{"cell_type":"code","execution_count":15,"metadata":{"cell_id":"00025-7a497f62-9e30-42a1-bb11-ac7f470daeb6","deepnote_cell_height":148,"deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":267,"execution_start":1658682615773,"source_hash":"87db3624","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["X: (17184, 5000), y: (17184,)\n","CPU times: total: 219 ms\n","Wall time: 234 ms\n"]}],"source":["%%time\n","X = df_train.iloc[:, 2:5002].to_numpy()\n","y = df_train.iloc[:,1].to_numpy()\n","print(f\"X: {X.shape}, y: {y.shape}\")"]},{"cell_type":"markdown","metadata":{"cell_id":"c46261c92089447cacdad83b77304580","deepnote_cell_height":52.390625,"deepnote_cell_type":"markdown","tags":[]},"source":["Splitting of Training Data Set to Perform Internal Validation of ML Model"]},{"cell_type":"code","execution_count":16,"metadata":{"cell_id":"00026-6f193c9e-3618-4484-8208-a051961372e4","deepnote_cell_height":168,"deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":1079,"execution_start":1658682616031,"source_hash":"2253195f","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["X_train: (12888, 5000), y_train: (12888,)\n","X_test: (4296, 5000), y_test: (4296,)\n","CPU times: total: 2.09 s\n","Wall time: 2.21 s\n"]}],"source":["%%time\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=100)\n","print(f\"X_train: {X_train.shape}, y_train: {y_train.shape}\")\n","print(f\"X_test: {X_test.shape}, y_test: {y_test.shape}\")"]},{"cell_type":"markdown","metadata":{"cell_id":"7c27a48a55ce4ae78c6976019a616738","deepnote_cell_height":52.390625,"deepnote_cell_type":"markdown","tags":[]},"source":["Initialisation of Hyperparameters for Logistic Regression Model"]},{"cell_type":"code","execution_count":17,"metadata":{"cell_id":"00027-ee4a0808-cb79-407d-9897-e37bd6ea5fe3","deepnote_cell_height":184,"deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":5,"execution_start":1658682617112,"source_hash":"13936e3c","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["128 16 0.1 0\n"]}],"source":["bs = 128\n","epochs = 16\n","lr = 0.1\n","C = 0\n","print(bs, epochs, lr, C)"]},{"cell_type":"markdown","metadata":{"cell_id":"565f4ff1d2444030a1be477b5bfbce67","deepnote_cell_height":52.390625,"deepnote_cell_type":"markdown","tags":[]},"source":["Evaluation of Logisitic Regression Model"]},{"cell_type":"code","execution_count":18,"metadata":{"cell_id":"5209c131890d49f7b7e89ee7958f4402","deepnote_cell_height":492.1875,"deepnote_cell_type":"code","deepnote_output_heights":[null,21.1875],"deepnote_to_be_reexecuted":false,"execution_millis":187364,"execution_start":1658682617138,"source_hash":"5b601220","tags":[],"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["[[-0.00172004]\n"," [-0.02492764]\n"," [ 0.00550406]\n"," ...\n"," [ 0.00015613]\n"," [ 0.01175477]\n"," [ 0.00490417]]\n","-0.5020697905473851\n","0.6493706267318076\n","1404\n","Accuracy: 0.6259310986964618\n","Macro-F1 score: 0.3867287807981557\n","CPU times: total: 10min 23s\n","Wall time: 1min 36s\n"]},{"data":{"text/plain":["0.3867287807981557"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["%%time\n","w, b, l = train(X_train, y_train, bs, epochs, lr, C)\n","print(w)\n","print(b)\n","print(min(l))\n","print(l.index(min(l)))\n","Model = score(y_test, predict(X_test, w, b))\n","Model"]},{"cell_type":"markdown","metadata":{"cell_id":"9fbda5801d0e4375a6c0ed673aa4581d","deepnote_cell_height":52.390625,"deepnote_cell_type":"markdown","tags":[]},"source":["Evaluation of SKLearn Logistic Regression Model (LogisticRegression and SGDClassifier)"]},{"cell_type":"code","execution_count":19,"metadata":{"cell_id":"1c2de54ac3904efeaeace8e6bf90af98","deepnote_cell_height":316.1875,"deepnote_cell_type":"code","deepnote_output_heights":[null,21.1875],"deepnote_to_be_reexecuted":false,"execution_millis":5404,"execution_start":1658682804499,"source_hash":"3125af87","tags":[],"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["[[-0.58187735 -1.02280517 -0.39681143 ... -0.04059766  0.49543838\n","   0.40047111]]\n","[-0.84683029]\n","Accuracy: 0.7302141527001862\n","Macro-F1 score: 0.6906115962801043\n","CPU times: total: 20.7 s\n","Wall time: 2.85 s\n"]},{"data":{"text/plain":["0.6906115962801043"]},"execution_count":19,"metadata":{},"output_type":"execute_result"}],"source":["%%time\n","clf1 = LogisticRegression(random_state = 100).fit(X_train, y_train)\n","print(clf1.coef_)\n","print(clf1.intercept_)\n","SKLearnLogReg = score(y_test, clf1.predict(X_test))\n","SKLearnLogReg"]},{"cell_type":"code","execution_count":20,"metadata":{"cell_id":"c4a0d020717347f8be43aaf0b5405db5","deepnote_cell_height":334.1875,"deepnote_cell_type":"code","deepnote_output_heights":[null,21.1875],"deepnote_to_be_reexecuted":false,"execution_millis":3477,"execution_start":1658682809910,"source_hash":"f7a528bd","tags":[],"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Users\\issac\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[[-0.47493728 -0.87421955 -0.31040593 ... -0.0494175   0.42273107\n","   0.31604817]]\n","[-0.75282034]\n","Accuracy: 0.728584729981378\n","Macro-F1 score: 0.6895107183766978\n","CPU times: total: 2.84 s\n","Wall time: 2.89 s\n"]},{"data":{"text/plain":["0.6895107183766978"]},"execution_count":20,"metadata":{},"output_type":"execute_result"}],"source":["%%time\n","clf2 = SGDClassifier(loss=\"log\", random_state=100).fit(X_train, y_train)\n","# clf2 = SGDClassifier(loss=\"log_loss\", random_state=100).fit(X_train, y_train)\n","print(clf2.coef_)\n","print(clf2.intercept_)\n","SKLearnSGD = score(y_test, clf2.predict(X_test))\n","SKLearnSGD"]},{"cell_type":"code","execution_count":21,"metadata":{"cell_id":"a0a7f0b441634a08bb9ffa3969fbe745","deepnote_cell_height":184.1875,"deepnote_cell_type":"code","deepnote_output_heights":[null,21.1875],"deepnote_to_be_reexecuted":false,"execution_millis":16,"execution_start":1658682813394,"source_hash":"5dbd2ce0","tags":[],"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: Model, Macro-F1 Score: 0.3867287807981557, Difference: 0.30388281548194856\n","Model Failed\n"]},{"data":{"text/plain":["[]"]},"execution_count":21,"metadata":{},"output_type":"execute_result"}],"source":["perform(SKLearnLogReg, SKLearnSGD, Model=Model)"]},{"cell_type":"markdown","metadata":{"cell_id":"38e4ab11c51d42afbcb9b3ce1379977a","deepnote_cell_height":214.796875,"deepnote_cell_type":"markdown","tags":[]},"source":["Based on the above results, we can observe that our Logistic Regression Model is severely underperforming as compared to the SKLearn Packages. We believe that this can be due to 2 possible reasons - The initialisation of the parameters `w` and `b` and the greedy approach adopted in navigating the gradient descent algorithm. \n","\n","We shall try a random initialisation and try 2 additional variations in developing our gradient descent algorithm (One where we choose the parameters that ensures minimum training error after every epoch and another where we choose the the parameters that ensure minimum training error at the end of the algorithm).\n","\n","We will evaluate these 3 models and compare their scores with those of the SKLearn Packages."]},{"cell_type":"markdown","metadata":{"cell_id":"1a044d9873f446408ad2aa101bb7d9af","deepnote_cell_height":70,"deepnote_cell_type":"markdown","tags":[]},"source":["## Tuning the Logistic Regression Model"]},{"cell_type":"markdown","metadata":{"cell_id":"a38debc715c64e2fa2dcb9a4f93a9479","deepnote_cell_height":62,"deepnote_cell_type":"markdown","tags":[]},"source":["### Tuning the Gradient Descent Algorithm"]},{"cell_type":"markdown","metadata":{"cell_id":"9349f4342add4e35a19655934163f24a","deepnote_cell_height":74.796875,"deepnote_cell_type":"markdown","tags":[]},"source":["For the `train1` function, we will implement a random initialisation of parameters `w` and `b` whose values are very close to 0 using the uniform distribution [0, 1)."]},{"cell_type":"code","execution_count":22,"metadata":{"cell_id":"00014-6addd038-1b3b-427b-8ffe-cbfefb160449","deepnote_cell_height":1377,"deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":11,"execution_start":1658682813420,"source_hash":"7a9d88a4","trusted":true},"outputs":[],"source":["# @param X - features\n","# @param y - labels\n","# @param bs - batch size\n","# @param epochs - number of iterations through dataset\n","# @param lr - learning rate\n","\n","def train1(X, y, bs, epochs, lr, C):\n","    lmb = 0 if C == 0 else 1/C\n","    \n","    # n - number of training examples, d - number of features\n","    n, d = np.shape(X)\n","    \n","    randomize = np.arange(n)\n","    rng = np.random.default_rng(100)\n","    \n","#     w = np.zeros((d, 1))\n","#     b = 0\n","    w = rng.uniform(size=(d,1))\n","    b = rng.random()\n","    \n","    y = y.reshape(n, 1)\n","    \n","    old_losses = []\n","    old_w = []\n","    old_b = []\n","    \n","    old_w.append(w.copy())\n","    old_b.append(b)\n","    l = loss(y, X, w, b, lmb)\n","    old_losses.append(l)\n","    \n","    for epoch in range(epochs):\n","        limit = n // bs\n","#         print(f\"limit: {limit}\")\n","\n","        for i in range(limit):\n","            start = i * bs\n","            end = start + bs\n","#             print(f\"epoch: {epoch}, start: {start}, end: {end}\")\n","            \n","            rng.shuffle(randomize)\n","            choice = randomize[start:end]\n","#             print(f\"choice: {choice}\")\n","            X_batch = X[choice]\n","            y_batch = y[choice]\n","            \n","            loss_old = loss(y, X, w, b, lmb)\n","            \n","            dw, db = gradients(y_batch, X_batch, w, b, lmb)\n","            w_new = w.copy() - lr * dw\n","            b_new = b - lr * db\n","            loss_new = loss(y, X, w_new, b_new, lmb)\n","            \n","            if (loss_new < loss_old):\n","#                 print(w == w_new, b == b_new);\n","#                 print(f\"loss_new: {loss_new}, loss_old: {loss_old}\")\n","\n","                w = w_new\n","                b = b_new\n","                old_w.append(w_new)\n","                old_b.append(b_new)\n","                old_losses.append(loss_new)\n","    \n","    min_loss = min(old_losses)\n","    min_index = old_losses.index(min_loss)\n","    \n","#     print(f\"old_w: {old_w}\")\n","#     print(f\"old_b: {old_b}\")\n","#     print(f\"old_losses: {old_losses}\")\n","#     print(f\"min_loss: {min_loss}\")\n","#     print(f\"min_index:\", min_index)\n","    \n","    return old_w[min_index], old_b[min_index], old_losses"]},{"cell_type":"markdown","metadata":{"cell_id":"6071a755307649bdb4eecc970033e4b6","deepnote_cell_height":97.1875,"deepnote_cell_type":"markdown","tags":[]},"source":["For the `train2` function, we will implement a random initialisation of parameters `w` and `b` whose values are very close to 0 using the uniform distribution [0, 1). Also, we will only choose the parameters that ensure minimum training error only at the end of the algorithm."]},{"cell_type":"code","execution_count":23,"metadata":{"cell_id":"00013-87646db4-96ee-4ef0-8640-5664dccf1c4f","deepnote_cell_height":1305,"deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":7,"execution_start":1658682813446,"source_hash":"af40613f","trusted":true},"outputs":[],"source":["# @param X - features\n","# @param y - labels\n","# @param bs - batch size\n","# @param epochs - number of iterations through dataset\n","# @param lr - learning rate\n","\n","def train2(X, y, bs, epochs, lr, C):\n","    lmb = 0 if C == 0 else 1/C\n","\n","    # n - number of training examples, d - number of features\n","    n, d = np.shape(X)\n","    \n","    randomize = np.arange(n)\n","    rng = np.random.default_rng(100)\n","    \n","    w = rng.uniform(size=(d,1))\n","    b = rng.random()\n","    \n","    y = y.reshape(n, 1)\n","    \n","    old_losses = []\n","    old_w = []\n","    old_b = []\n","    \n","    old_w.append(w.copy())\n","    old_b.append(b)\n","    l = loss(y, X, w, b, lmb)\n","    old_losses.append(l)\n","    \n","    for epoch in range(epochs):\n","        limit = n // bs\n","#         print(f\"limit: {limit}\")\n","\n","        for i in range(limit):\n","            start = i * bs\n","            end = start + bs\n","#             print(f\"epoch: {epoch}, start: {start}, end: {end}\")\n","            \n","            rng.shuffle(randomize)\n","            choice = randomize[start:end]\n","#             print(f\"choice: {choice}\")\n","            X_batch = X[choice]\n","            y_batch = y[choice]\n","            \n","            loss_old = loss(y, X, w, b, lmb)\n","            \n","            dw, db = gradients(y_batch, X_batch, w, b, lmb)\n","            w_new = w.copy() - lr * dw\n","            b_new = b - lr * db\n","            loss_new = loss(y, X, w_new, b_new, lmb)\n","#             print(w == w_new, b == b_new);\n","#             print(f\"loss_new: {loss_new}, loss_old: {loss_old}\")\n","\n","            w = w_new\n","            b = b_new\n","            old_w.append(w_new)\n","            old_b.append(b_new)\n","            old_losses.append(loss_new)\n","\n","    min_loss = min(old_losses)\n","    min_index = old_losses.index(min_loss)\n","    \n","#     print(f\"old_w: {old_w}\")\n","#     print(f\"old_b: {old_b}\")\n","#     print(f\"old_losses: {old_losses}\")\n","#     print(f\"min_loss: {min_loss}\")\n","#     print(f\"min_index:\", min_index)\n","    \n","    return old_w[min_index], old_b[min_index], old_losses"]},{"cell_type":"markdown","metadata":{"cell_id":"0eb861086da24e4798148ff9390189c8","deepnote_cell_height":74.796875,"deepnote_cell_type":"markdown","tags":[]},"source":["For the `train3` function, we will implement a random initialisation of parameters `w` and `b` whose values are very close to 0 using the uniform distribution [0, 1). Also, we will only choose the parameters that ensure minimum training error after every epoch."]},{"cell_type":"code","execution_count":24,"metadata":{"cell_id":"00015-9fce8db2-a072-46fa-8931-9c1bcf1e8ae3","deepnote_cell_height":1449,"deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":8,"execution_start":1658682813469,"source_hash":"f961edc1","trusted":true},"outputs":[],"source":["# @param X - features\n","# @param y - labels\n","# @param bs - batch size\n","# @param epochs - number of iterations through dataset\n","# @param lr - learning rate\n","\n","def train3(X, y, bs, epochs, lr, C):\n","    lmb = 0 if C == 0 else 1/C\n","    \n","    # n - number of training examples, d - number of features\n","    n, d = np.shape(X)\n","    \n","    randomize = np.arange(n)\n","    rng = np.random.default_rng(100)\n","    \n","#     w = np.zeros((d, 1))\n","#     b = 0\n","    w = rng.uniform(size=(d,1))\n","    b = rng.random()\n","    \n","    y = y.reshape(n, 1)\n","    \n","    old_losses = []\n","    old_w = []\n","    old_b = []\n","    \n","    old_w.append(w.copy())\n","    old_b.append(b)\n","    l = loss(y, X, w, b, lmb)\n","    old_losses.append(l)\n","    \n","    for epoch in range(epochs):\n","        limit = n // bs\n","#         print(f\"limit: {limit}\")\n","\n","        for i in range(limit):\n","            start = i * bs\n","            end = start + bs\n","#             print(f\"epoch: {epoch}, start: {start}, end: {end}\")\n","            \n","            rng.shuffle(randomize)\n","            choice = randomize[start:end]\n","#             print(f\"choice: {choice}\")\n","            X_batch = X[choice]\n","            y_batch = y[choice]\n","            \n","            loss_old = loss(y, X, w, b, lmb)\n","            \n","            dw, db = gradients(y_batch, X_batch, w, b, lmb)\n","            w_new = w.copy() - lr * dw\n","            b_new = b - lr * db\n","            loss_new = loss(y, X, w_new, b_new, lmb)\n","            \n","#             print(w == w_new, b == b_new);\n","#             print(f\"loss_new: {loss_new}, loss_old: {loss_old}\")\n","            \n","            w = w_new\n","            b = b_new\n","            old_w.append(w_new)\n","            old_b.append(b_new)\n","            old_losses.append(loss_new)\n","        \n","        min_loss = min(old_losses)\n","        min_index = old_losses.index(min_loss)\n","        w = old_w[min_index]\n","        b = old_b[min_index]\n","        \n","    min_loss = min(old_losses)\n","    min_index = old_losses.index(min_loss)\n","    \n","#     print(f\"old_w: {old_w}\")\n","#     print(f\"old_b: {old_b}\")\n","#     print(f\"old_losses: {old_losses}\")\n","#     print(f\"min_loss: {min_loss}\")\n","#     print(f\"min_index:\", min_index)\n","    \n","    return old_w[min_index], old_b[min_index], old_losses"]},{"cell_type":"code","execution_count":25,"metadata":{"cell_id":"fa96d2cb0d7544ad8f6577e60c12b238","deepnote_cell_height":492.1875,"deepnote_cell_type":"code","deepnote_output_heights":[null,21.1875],"deepnote_to_be_reexecuted":false,"execution_millis":188007,"execution_start":1658682813480,"source_hash":"142cfe6d","tags":[],"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["[[0.82953088]\n"," [0.56112497]\n"," [0.28441661]\n"," ...\n"," [0.54737681]\n"," [0.20428492]\n"," [0.14764031]]\n","-1.7553462188993563\n","0.6582263563878277\n","1477\n","Accuracy: 0.6240689013035382\n","Macro-F1 score: 0.48659922937049865\n","CPU times: total: 10min 20s\n","Wall time: 1min 39s\n"]},{"data":{"text/plain":["0.48659922937049865"]},"execution_count":25,"metadata":{},"output_type":"execute_result"}],"source":["%%time\n","w, b, l = train1(X_train, y_train, bs, epochs, lr, C)\n","print(w)\n","print(b)\n","print(min(l))\n","print(l.index(min(l)))\n","Model1 = score(y_test, predict(X_test, w, b))\n","Model1"]},{"cell_type":"code","execution_count":26,"metadata":{"cell_id":"a22fce9348614d9892f976e6e9fdf754","deepnote_cell_height":492.1875,"deepnote_cell_type":"code","deepnote_output_heights":[null,21.1875],"deepnote_to_be_reexecuted":false,"execution_millis":196553,"execution_start":1658683000816,"source_hash":"8629bebf","tags":[],"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["[[0.82917508]\n"," [0.55758287]\n"," [0.28212895]\n"," ...\n"," [0.54710171]\n"," [0.2049865 ]\n"," [0.14875909]]\n","-1.7440156421330273\n","0.6568636717097833\n","1600\n","Accuracy: 0.6243016759776536\n","Macro-F1 score: 0.4920613897314961\n","CPU times: total: 10min 9s\n","Wall time: 1min 39s\n"]},{"data":{"text/plain":["0.4920613897314961"]},"execution_count":26,"metadata":{},"output_type":"execute_result"}],"source":["%%time\n","w, b, l = train2(X_train, y_train, bs, epochs, lr, C)\n","print(w)\n","print(b)\n","print(min(l))\n","print(l.index(min(l)))\n","Model2 = score(y_test, predict(X_test, w, b))\n","Model2"]},{"cell_type":"code","execution_count":27,"metadata":{"cell_id":"59dce1de6aff487aa46898e2e7156489","deepnote_cell_height":492,"deepnote_cell_type":"code","deepnote_output_heights":[null,21.1875],"deepnote_to_be_reexecuted":false,"execution_millis":31925,"execution_start":1658683197315,"source_hash":"d6b6ea82","tags":[],"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["[[0.82917524]\n"," [0.55793472]\n"," [0.28220347]\n"," ...\n"," [0.5470061 ]\n"," [0.20511076]\n"," [0.14884337]]\n","-1.7438202768654791\n","0.656952130767089\n","1600\n","Accuracy: 0.6243016759776536\n","Macro-F1 score: 0.4920613897314961\n","CPU times: total: 10min 6s\n","Wall time: 1min 21s\n"]},{"data":{"text/plain":["0.4920613897314961"]},"execution_count":27,"metadata":{},"output_type":"execute_result"}],"source":["%%time\n","w, b, l = train3(X_train, y_train, bs, epochs, lr, C)\n","print(w)\n","print(b)\n","print(min(l))\n","print(l.index(min(l)))\n","Model3 = score(y_test, predict(X_test, w, b))\n","Model3"]},{"cell_type":"code","execution_count":28,"metadata":{"cell_id":"6aa2a0555ce94cafb6a7fedc78c72891","deepnote_cell_height":262.1875,"deepnote_cell_type":"code","deepnote_output_heights":[null,21.1875],"deepnote_to_be_reexecuted":true,"execution_millis":2,"execution_start":1658671969346,"source_hash":"c1d0bc52","tags":[],"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: Model, Macro-F1 Score: 0.3867287807981557, Difference: 0.30388281548194856\n","Model: Model1, Macro-F1 Score: 0.48659922937049865, Difference: 0.2040123669096056\n","Model: Model2, Macro-F1 Score: 0.4920613897314961, Difference: 0.19855020654860817\n","Model: Model3, Macro-F1 Score: 0.4920613897314961, Difference: 0.19855020654860817\n","Model Failed\n"]},{"data":{"text/plain":["[]"]},"execution_count":28,"metadata":{},"output_type":"execute_result"}],"source":["perform(SKLearnLogReg, SKLearnSGD, Model=Model, \\\n","Model1=Model1, Model2=Model2, Model3=Model3)"]},{"cell_type":"markdown","metadata":{"cell_id":"68ffe1f2501642fdba5f5ca7c6f9c718","deepnote_cell_height":489.1875,"deepnote_cell_type":"markdown","tags":[]},"source":["Based on the above results, we can see that the random initialisation of `w` and `b` has improved the performance of the gradient descent algorithm. However, we have noted that varying the design of the gradient descent algorithm at this stage has minimal effect on the model performance.\n","\n","Upon inspection of the training dataset, we have noted that the training data is extremely sparse. Hence, we need to incorporate the idea of momentum and adaptive learning rate into our gradient descent algorithm. The introduction of momentum is to ensure that the gradient descent algorithm moves in the direction of the trend (weighted average of gradients) even in the presence of anomalous gradient values or zero gradient values. This would help accelerate the training process.\n","\n","Additionally, as a result of sparse data, we require an adaptive learning rate that is able to boost the respective parameters of `w_i` and `b` appropriately such that the algorithm is more sensitive to valuable data that are present within the training set. This would help improve the correction and accelerate the training process.\n","\n","Based on the 2 requirements, we have decided to use the `AdamW` optimiser, which uses a combination of momentum and RMSProp. Moreover, `AdamW` is an improvement over the `Adam` optimiser, in that it supports regularisation by introducing a Weight Decay component to the gradient descent algorithm. This is necessary due to the use of adaptive learning rate that skews the regularisation of parameters in the `Adam` optimiser.\n","\n","Likewise, we will introduce 3 different variants of the `AdamW` optimiser. \\\n","    1. One where we choose the parameters that ensure a smaller training error after every update (Greedy Approach) \\\n","    2. One where we choose the parameters that ensure minimum training error at the end of the algorithm \\\n","    3. One where we choose the parameters that ensures minimum training error after every epoch"]},{"cell_type":"markdown","metadata":{"cell_id":"80fa9c837cfb4b0d8459d540dd17e727","deepnote_cell_height":74.796875,"deepnote_cell_type":"markdown","tags":[]},"source":["For the `train4` function, it is an improvement of the `train1` function. We will use the `AdamW` optimiser instead of the conventional gradient descent algorithm. We will maintain the random initialisation in this algorithm."]},{"cell_type":"code","execution_count":29,"metadata":{"cell_id":"00017-1ee80d48-ede9-4bb5-8887-503569bd3183","deepnote_cell_height":1719,"deepnote_cell_type":"code","deepnote_to_be_reexecuted":true,"execution_millis":39,"execution_start":1658671969347,"source_hash":"cd1189da","trusted":true},"outputs":[],"source":["# @param X - features\n","# @param y - labels\n","# @param bs - batch size\n","# @param epochs - number of iterations through dataset\n","# @param lr - learning rate\n","\n","def train4(X, y, bs, epochs, lr, C, beta_m, beta_v, err):\n","    lmb = 0 if C == 0 else 1/C\n","    \n","    # n - number of training examples, d - number of features\n","    n, d = np.shape(X)\n","    \n","    randomize = np.arange(n)\n","    rng = np.random.default_rng(100)\n","    \n","#     w = np.zeros((d, 1))\n","#     b = 0\n","    w = rng.uniform(size=(d,1))\n","    b = rng.random()\n","    \n","    m_w = np.zeros((d, 1))\n","    m_b = 0\n","    v_w = np.zeros((d, 1))\n","    v_b = 0\n","    \n","    y = y.reshape(n, 1)\n","    \n","    old_losses = []\n","    old_w = []\n","    old_b = []\n","    \n","    old_w.append(w.copy())\n","    old_b.append(b)\n","    l = loss(y, X, w, b, lmb)\n","    old_losses.append(l)\n","    \n","    for epoch in range(epochs):\n","        limit = n // bs\n","#         print(f\"limit: {limit}\")\n","\n","        for i in range(limit):\n","            start = i * bs\n","            end = start + bs\n","#             print(f\"epoch: {epoch}, start: {start}, end: {end}\")\n","            \n","            rng.shuffle(randomize)\n","            choice = randomize[start:end]\n","#             print(f\"choice: {choice}\")\n","            X_batch = X[choice]\n","            y_batch = y[choice]\n","            \n","            loss_old = loss(y, X, w, b, lmb)\n","            \n","            dw, db = gradients(y_batch, X_batch, w, b, lmb)\n","            \n","            #AdamW\n","            m_w = beta_m * m_w + (1 - beta_m) * dw\n","            m_b = beta_m * m_b + (1 - beta_m) * db\n","            v_w = beta_v * v_w + (1 - beta_v) * np.square(dw)\n","            v_b = beta_v * v_b + (1 - beta_v) * (db**2)\n","            \n","            #bias correction\n","            t = len(old_losses)\n","            m_what = m_w /(1 - beta_m**t)\n","            m_bhat = m_b /(1 - beta_m**t)\n","            v_what = v_w /(1 - beta_v**t)\n","            v_bhat = v_b /(1 - beta_v**t)\n","            \n","            w_new = w.copy() - lr * (m_what/(np.sqrt(v_what) + err) + lmb * w.copy() / bs)\n","            b_new = b - lr * lr * (m_bhat/(np.sqrt(v_bhat) + err))\n","            loss_new = loss(y, X, w_new, b_new, lmb)\n","            \n","            if (loss_new < loss_old):\n","#                 print(w == w_new, b == b_new);\n","#                 print(f\"loss_new: {loss_new}, loss_old: {loss_old}\")\n","\n","                w = w_new\n","                b = b_new\n","                old_w.append(w_new)\n","                old_b.append(b_new)\n","                old_losses.append(loss_new)\n","    \n","    min_loss = min(old_losses)\n","    min_index = old_losses.index(min_loss)\n","    \n","#     print(f\"old_w: {old_w}\")\n","#     print(f\"old_b: {old_b}\")\n","#     print(f\"old_losses: {old_losses}\")\n","#     print(f\"min_loss: {min_loss}\")\n","#     print(f\"min_index:\", min_index)\n","    \n","    return old_w[min_index], old_b[min_index], old_losses"]},{"cell_type":"markdown","metadata":{"cell_id":"120b7bb1dfb344c58acb38d783fc6401","deepnote_cell_height":74.796875,"deepnote_cell_type":"markdown","tags":[]},"source":["For the `train5` function, it is an improvement of the `train2` function. We will use the `AdamW` optimiser instead of the conventional gradient descent algorithm. We will maintain the random initialisation in this algorithm."]},{"cell_type":"code","execution_count":30,"metadata":{"cell_id":"00016-e5039b6e-272f-46e1-bc5a-11cafd259faf","deepnote_cell_height":1665,"deepnote_cell_type":"code","deepnote_to_be_reexecuted":true,"execution_millis":0,"execution_start":1658671969386,"source_hash":"48058778","trusted":true},"outputs":[],"source":["# @param X - features\n","# @param y - labels\n","# @param bs - batch size\n","# @param epochs - number of iterations through dataset\n","# @param lr - learning rate\n","\n","def train5(X, y, bs, epochs, lr, C, beta_m, beta_v, err):\n","    lmb = 0 if C == 0 else 1/C\n","    \n","    # n - number of training examples, d - number of features\n","    n, d = np.shape(X)\n","    \n","    randomize = np.arange(n)\n","    rng = np.random.default_rng(100)\n","    \n","#     w = np.zeros((d, 1))\n","#     b = 0\n","    w = rng.uniform(size=(d,1))\n","    b = rng.random()\n","    \n","    m_w = np.zeros((d, 1))\n","    m_b = 0\n","    v_w = np.zeros((d, 1))\n","    v_b = 0\n","    \n","    y = y.reshape(n, 1)\n","    \n","    old_losses = []\n","    old_w = []\n","    old_b = []\n","    \n","    old_w.append(w.copy())\n","    old_b.append(b)\n","    l = loss(y, X, w, b, lmb)\n","    old_losses.append(l)\n","    \n","    for epoch in range(epochs):\n","        limit = n // bs\n","#         print(f\"limit: {limit}\")\n","\n","        for i in range(limit):\n","            start = i * bs\n","            end = start + bs\n","#             print(f\"epoch: {epoch}, start: {start}, end: {end}\")\n","            \n","            rng.shuffle(randomize)\n","            choice = randomize[start:end]\n","            X_batch = X[choice]\n","            y_batch = y[choice]\n","            \n","            loss_old = loss(y, X, w, b, lmb)\n","            \n","            dw, db = gradients(y_batch, X_batch, w, b, lmb)\n","            \n","            #AdamW\n","            m_w = beta_m * m_w + (1 - beta_m) * dw\n","            m_b = beta_m * m_b + (1 - beta_m) * db\n","            v_w = beta_v * v_w + (1 - beta_v) * np.square(dw)\n","            v_b = beta_v * v_b + (1 - beta_v) * (db**2)\n","            \n","            #bias correction\n","            t = len(old_losses)\n","            m_what = m_w /(1 - beta_m**t)\n","            m_bhat = m_b /(1 - beta_m**t)\n","            v_what = v_w /(1 - beta_v**t)\n","            v_bhat = v_b /(1 - beta_v**t)\n","            \n","            w_new = w.copy() - lr * (m_what/(np.sqrt(v_what) + err) + lmb * w.copy() / bs)\n","            b_new = b - lr * lr * (m_bhat/(np.sqrt(v_bhat) + err))\n","            loss_new = loss(y, X, w_new, b_new, lmb)\n","#             print(w == w_new, b == b_new);\n","#             print(f\"loss_new: {loss_new}, loss_old: {loss_old}\")\n","            \n","            w = w_new\n","            b = b_new\n","            old_w.append(w_new)\n","            old_b.append(b_new)\n","            old_losses.append(loss_new)\n","\n","    min_loss = min(old_losses)\n","    min_index = old_losses.index(min_loss)\n","    \n","#     print(f\"old_w: {old_w}\")\n","#     print(f\"old_b: {old_b}\")\n","#     print(f\"old_losses: {old_losses}\")\n","#     print(f\"min_loss: {min_loss}\")\n","#     print(f\"min_index:\", min_index)\n","    \n","    return old_w[min_index], old_b[min_index], old_losses"]},{"cell_type":"markdown","metadata":{"cell_id":"eae2c6e325b646bc88b45a58a813eb3f","deepnote_cell_height":74.796875,"deepnote_cell_type":"markdown","tags":[]},"source":["For the `train6` function, it is an improvement of the `train3` function. We will use the `AdamW` optimiser instead of the conventional gradient descent algorithm. We will maintain the random initialisation in this algorithm."]},{"cell_type":"code","execution_count":31,"metadata":{"cell_id":"00018-45469d69-b680-4896-aa5a-a35ec13eeae1","deepnote_cell_height":1791,"deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":0,"execution_start":1658687341941,"source_hash":"cee706ad","trusted":true},"outputs":[],"source":["# @param X - features\n","# @param y - labels\n","# @param bs - batch size\n","# @param epochs - number of iterations through dataset\n","# @param lr - learning rate\n","\n","def train6(X, y, bs, epochs, lr, C, beta_m, beta_v, err):\n","    lmb = 0 if C == 0 else 1/C\n","    \n","    # n - number of training examples, d - number of features\n","    n, d = np.shape(X)\n","    \n","    randomize = np.arange(n)\n","    rng = np.random.default_rng(100)\n","    \n","#     w = np.zeros((d, 1))\n","#     b = 0\n","    w = rng.uniform(size=(d,1))\n","    b = rng.random()\n","    \n","    m_w = np.zeros((d, 1))\n","    m_b = 0\n","    v_w = np.zeros((d, 1))\n","    v_b = 0\n","    \n","    y = y.reshape(n, 1)\n","    \n","    old_losses = []\n","    old_w = []\n","    old_b = []\n","    \n","    old_w.append(w.copy())\n","    old_b.append(b)\n","    l = loss(y, X, w, b, lmb)\n","    old_losses.append(l)\n","    \n","    for epoch in range(epochs):\n","        limit = n // bs\n","#         print(f\"limit: {limit}\")\n","\n","        for i in range(limit):\n","            start = i * bs\n","            end = start + bs\n","#             print(f\"epoch: {epoch}, start: {start}, end: {end}\")\n","            \n","            rng.shuffle(randomize)\n","            choice = randomize[start:end]\n","#             print(f\"choice: {choice}\")\n","            X_batch = X[choice]\n","            y_batch = y[choice]\n","            \n","            loss_old = loss(y, X, w, b, lmb)\n","            \n","            dw, db = gradients(y_batch, X_batch, w, b, lmb)\n","            \n","            #AdamW\n","            m_w = beta_m * m_w + (1 - beta_m) * dw\n","            m_b = beta_m * m_b + (1 - beta_m) * db\n","            v_w = beta_v * v_w + (1 - beta_v) * np.square(dw)\n","            v_b = beta_v * v_b + (1 - beta_v) * (db**2)\n","            \n","            #bias correction\n","            t = len(old_losses)\n","            m_what = m_w /(1 - beta_m**t)\n","            m_bhat = m_b /(1 - beta_m**t)\n","            v_what = v_w /(1 - beta_v**t)\n","            v_bhat = v_b /(1 - beta_v**t)\n","            \n","            w_new = w.copy() - lr * (m_what/(np.sqrt(v_what) + err) + lmb * w.copy() / bs)\n","            b_new = b - lr * lr * (m_bhat/(np.sqrt(v_bhat) + err))\n","            loss_new = loss(y, X, w_new, b_new, lmb)\n","            \n","#             print(w == w_new, b == b_new);\n","#             print(f\"loss_new: {loss_new}, loss_old: {loss_old}\")\n","            \n","            w = w_new\n","            b = b_new\n","            old_w.append(w_new)\n","            old_b.append(b_new)\n","            old_losses.append(loss_new)\n","        \n","        min_loss = min(old_losses)\n","        min_index = old_losses.index(min_loss)\n","        w = old_w[min_index]\n","        b = old_b[min_index]\n","        \n","    min_loss = min(old_losses)\n","    min_index = old_losses.index(min_loss)\n","    \n","#     print(f\"old_w: {old_w}\")\n","#     print(f\"old_b: {old_b}\")\n","#     print(f\"old_losses: {old_losses}\")\n","#     print(f\"min_loss: {min_loss}\")\n","#     print(f\"min_index:\", min_index)\n","    \n","    return old_w[min_index], old_b[min_index], old_losses"]},{"cell_type":"markdown","metadata":{"cell_id":"1a7db9c26cea4c4eb02e5d4db1fb3ece","deepnote_cell_height":74.796875,"deepnote_cell_type":"markdown","tags":[]},"source":["Based on our research, the recommended values of the hyperparameters for the AdamW optimizer are `beta_m = 0.9`, `beta_v = 0.999` and `err = 1e-8`."]},{"cell_type":"code","execution_count":32,"metadata":{"cell_id":"c2ea795b5d2f47ff96be49f0fd2ca797","deepnote_cell_height":238,"deepnote_cell_type":"code","deepnote_to_be_reexecuted":true,"execution_millis":4,"execution_start":1658671969407,"source_hash":"a6484cd3","tags":[],"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["128 16 0.1 0 0.9 0.999 1e-08\n"]}],"source":["bs = 128\n","epochs = 16\n","lr = 0.1\n","C = 0\n","beta_m = 0.9\n","beta_v = 0.999\n","err = 1e-8\n","print(bs, epochs, lr, C, beta_m, beta_v, err)"]},{"cell_type":"code","execution_count":33,"metadata":{"cell_id":"00032-4e0a19ee-70b6-47ee-bc12-542dbcd9c1d1","deepnote_cell_height":492.1875,"deepnote_cell_type":"code","deepnote_output_heights":[null,21.1875],"deepnote_to_be_reexecuted":true,"execution_millis":202294,"execution_start":1658671969431,"source_hash":"aafebce6","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["[[-8.47741643]\n"," [-8.10057621]\n"," [-4.33031263]\n"," ...\n"," [ 1.54335685]\n"," [ 3.65972176]\n"," [ 3.82466584]]\n","-1.1785629010376373\n","0.2810261570260205\n","1462\n","Accuracy: 0.6722532588454376\n","Macro-F1 score: 0.6494362017804154\n","CPU times: total: 10min 15s\n","Wall time: 1min 19s\n"]},{"data":{"text/plain":["0.6494362017804154"]},"execution_count":33,"metadata":{},"output_type":"execute_result"}],"source":["%%time\n","w, b, l = train4(X_train, y_train, bs, epochs, lr, C, beta_m, beta_v, err)\n","print(w)\n","print(b)\n","print(min(l))\n","print(l.index(min(l)))\n","Model4 = score(y_test, predict(X_test, w, b))\n","Model4"]},{"cell_type":"code","execution_count":34,"metadata":{"cell_id":"00033-823f1f81-4f98-42b9-b244-7aa6caad1dba","deepnote_cell_height":492.1875,"deepnote_cell_type":"code","deepnote_output_heights":[null,21.1875],"deepnote_to_be_reexecuted":true,"execution_millis":211254,"execution_start":1658672171550,"source_hash":"d445cb11","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["[[-9.15852804]\n"," [-8.63754918]\n"," [-4.72636096]\n"," ...\n"," [ 1.62198725]\n"," [ 3.88039144]\n"," [ 3.71492837]]\n","-1.2682779368539099\n","0.28378523874847067\n","1598\n","Accuracy: 0.6715549348230913\n","Macro-F1 score: 0.6496150778344336\n","CPU times: total: 10min 2s\n","Wall time: 1min 20s\n"]},{"data":{"text/plain":["0.6496150778344336"]},"execution_count":34,"metadata":{},"output_type":"execute_result"}],"source":["%%time\n","w, b, l = train5(X_train, y_train, bs, epochs, lr, C, beta_m, beta_v, err)\n","print(w)\n","print(b)\n","print(min(l))\n","print(l.index(min(l)))\n","Model5 = score(y_test, predict(X_test, w, b))\n","Model5"]},{"cell_type":"code","execution_count":35,"metadata":{"cell_id":"00034-67b8b9dc-2b70-457d-8257-6d3ac7a14ac0","deepnote_cell_height":492.1875,"deepnote_cell_type":"code","deepnote_output_heights":[null,21.1875],"deepnote_to_be_reexecuted":true,"execution_millis":203230,"execution_start":1658672382798,"source_hash":"253fb0c6","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["[[-8.22759819]\n"," [-8.33745542]\n"," [-3.9632901 ]\n"," ...\n"," [ 1.95402751]\n"," [ 3.52822577]\n"," [ 3.67153536]]\n","-1.1590263536783283\n","0.2840447771365454\n","1532\n","Accuracy: 0.672951582867784\n","Macro-F1 score: 0.6507565038781207\n","CPU times: total: 10min 12s\n","Wall time: 1min 19s\n"]},{"data":{"text/plain":["0.6507565038781207"]},"execution_count":35,"metadata":{},"output_type":"execute_result"}],"source":["%%time\n","w, b, l = train6(X_train, y_train, bs, epochs, lr, C, beta_m, beta_v, err)\n","print(w)\n","print(b)\n","print(min(l))\n","print(l.index(min(l)))\n","Model6 = score(y_test, predict(X_test, w, b))\n","Model6"]},{"cell_type":"code","execution_count":36,"metadata":{"cell_id":"bed05c58066f43559152cfe6d491fda7","deepnote_cell_height":340.1875,"deepnote_cell_type":"code","deepnote_output_heights":[null,21.1875],"deepnote_to_be_reexecuted":true,"execution_millis":285,"execution_start":1658672585746,"source_hash":"47f11e70","tags":[],"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: Model, Macro-F1 Score: 0.3867287807981557, Difference: 0.30388281548194856\n","Model: Model1, Macro-F1 Score: 0.48659922937049865, Difference: 0.2040123669096056\n","Model: Model2, Macro-F1 Score: 0.4920613897314961, Difference: 0.19855020654860817\n","Model: Model3, Macro-F1 Score: 0.4920613897314961, Difference: 0.19855020654860817\n","Model: Model4, Macro-F1 Score: 0.6494362017804154, Difference: 0.04117539449968888\n","Model: Model5, Macro-F1 Score: 0.6496150778344336, Difference: 0.04099651844567065\n","Model: Model6, Macro-F1 Score: 0.6507565038781207, Difference: 0.039855092401983594\n","Model Success\n"]},{"data":{"text/plain":["['Model4', 'Model5', 'Model6']"]},"execution_count":36,"metadata":{},"output_type":"execute_result"}],"source":["perform(SKLearnLogReg, SKLearnSGD, Model=Model, \\\n","Model1=Model1, Model2=Model2, Model3=Model3, \\\n","Model4=Model4, Model5=Model5, Model6=Model6)"]},{"cell_type":"markdown","metadata":{"cell_id":"00052-0bd975ab-19ef-4aff-86d1-65a85656d013","deepnote_cell_height":97.1875,"deepnote_cell_type":"markdown"},"source":["Based on the above score, we can deem that the performance of our Logistic Regression Model is comparable to that of SKLearn Logistic Regression Package. Moreover, our best performing model is Model6. Hence, we shall refine our Logistic Regression Model by tuning the hyperparameters."]},{"cell_type":"markdown","metadata":{"cell_id":"ae11d50cb29e4b888746694fde2f6fe7","deepnote_cell_height":62,"deepnote_cell_type":"markdown","tags":[]},"source":["### Hyperparameter Tuning"]},{"cell_type":"markdown","metadata":{"cell_id":"1b5f0d3cd5a84c7a8139b176069da8fe","deepnote_cell_height":97.1875,"deepnote_cell_type":"markdown","tags":[]},"source":["To further improve our model performance, we will first find the best learning rate, regularisation coefficient, momentum coefficient, RMSProp coefficient and the Error Term. Once we have chosen the optimal hyperparameters, we will then decide on the Batch Size and Epoch Size to train our model."]},{"cell_type":"code","execution_count":37,"metadata":{"cell_id":"00035-fc897196-e51a-4ee4-9bd3-f9063c3d47c6","deepnote_cell_height":372,"deepnote_cell_type":"code","deepnote_to_be_reexecuted":true,"execution_millis":233,"execution_start":1658677091895,"source_hash":"8b69acce","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Learning Rates: [1, 0.1, 0.01, 0.001]\n","Regularisation Coefficients: [0.1, 1, 10, 100, 1000, 0]\n","Momentum Coefficients: [0.9, 0.8, 0.7, 0.6, 0.5, 0.4, 0.3, 0.2, 0.1, 0.0]\n","RMSProp Coefficients: [0.999, 0.997, 0.995, 0.993, 0.991, 0.989]\n","Error Terms: [1, 0.01, 0.0001, 1e-06, 1e-08]\n"]}],"source":["Ls = [10**(-i) for i in range(4)]\n","Cs = [0.1, 1, 10, 100, 1000, 0]\n","Bm = [(9 - i)/10 for i in range(10)]\n","Bv = [(999 - 2*i)/1000 for i in range(6)]\n","Errs = [10**(-2*i) for i in range(5)]\n","\n","print(f\"Learning Rates: {Ls}\")\n","print(f\"Regularisation Coefficients: {Cs}\")\n","print(f\"Momentum Coefficients: {Bm}\")\n","print(f\"RMSProp Coefficients: {Bv}\")\n","print(f\"Error Terms: {Errs}\")"]},{"cell_type":"code","execution_count":38,"metadata":{"cell_id":"00037-9aee6fca-0854-4294-bc7f-d7ec6603904e","deepnote_cell_height":848,"deepnote_cell_type":"code","deepnote_to_be_reexecuted":true,"execution_millis":189719,"execution_start":1658675006802,"source_hash":"7c0af1ce","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy: 0.6538640595903166\n","Macro-F1 score: 0.60183066275978\n","303\n","1 0 0.9 0.999 1e-08 0.60183066275978\n","Accuracy: 0.6890130353817505\n","Macro-F1 score: 0.6662472846585037\n","400\n","0.1 0 0.9 0.999 1e-08 0.6662472846585037\n","Accuracy: 0.6778398510242085\n","Macro-F1 score: 0.6537110228158275\n","400\n","0.01 0 0.9 0.999 1e-08 0.6537110228158275\n","Accuracy: 0.3743016759776536\n","Macro-F1 score: 0.2723577235772358\n","400\n","0.001 0 0.9 0.999 1e-08 0.2723577235772358\n","{0.60183066275978: 1, 0.6662472846585037: 0.1, 0.6537110228158275: 0.01, 0.2723577235772358: 0.001}\n","[0.1] 0.6662472846585037\n","CPU times: total: 10min 18s\n","Wall time: 1min 19s\n"]}],"source":["%%time\n","vals = {}\n","bs = 128\n","epochs = 4\n","lr = 0.1\n","C = 0\n","beta_m = 0.9\n","beta_v = 0.999\n","err = 1e-8\n","\n","for lr in Ls:\n","    w, b, l = train6(X_train, y_train, bs, epochs, lr, C, beta_m, beta_v, err)\n","    result = score(y_test, predict(X_test, w, b))\n","    # print(w)\n","    # print(b)\n","    # print(min(l))\n","    print(l.index(min(l)))\n","    print(lr, C, beta_m, beta_v, err, result)\n","    vals[result] = lr\n","\n","print(vals)\n","maxval = max(vals.keys())\n","res = [v for k, v in vals.items() if k==maxval]\n","print(res, maxval)"]},{"cell_type":"markdown","metadata":{"cell_id":"2bd880bb5f8d45159419ada1dbf55c62","deepnote_cell_height":52.390625,"deepnote_cell_type":"markdown","tags":[]},"source":["We shall choose a learning rate of `lr = 0.1`."]},{"cell_type":"code","execution_count":39,"metadata":{"cell_id":"49aed85a49d6433bb21c9f510a9c61ef","deepnote_cell_height":1012.796875,"deepnote_cell_type":"code","deepnote_to_be_reexecuted":true,"execution_millis":301417,"execution_start":1658673145534,"source_hash":"48c55687","tags":[],"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy: 0.625465549348231\n","Macro-F1 score: 0.384791636832307\n","320\n","0.1 0.1 0.9 0.999 1e-08 0.384791636832307\n","Accuracy: 0.6261638733705773\n","Macro-F1 score: 0.3879851144518121\n","323\n","0.1 1 0.9 0.999 1e-08 0.3879851144518121\n","Accuracy: 0.680633147113594\n","Macro-F1 score: 0.5902111978259219\n","338\n","0.1 10 0.9 0.999 1e-08 0.5902111978259219\n","Accuracy: 0.7178770949720671\n","Macro-F1 score: 0.6847822296614012\n","315\n","0.1 100 0.9 0.999 1e-08 0.6847822296614012\n","Accuracy: 0.699487895716946\n","Macro-F1 score: 0.6741020618444631\n","400\n","0.1 1000 0.9 0.999 1e-08 0.6741020618444631\n","Accuracy: 0.6890130353817505\n","Macro-F1 score: 0.6662472846585037\n","400\n","0.1 0 0.9 0.999 1e-08 0.6662472846585037\n","{0.384791636832307: 0.1, 0.3879851144518121: 1, 0.5902111978259219: 10, 0.6847822296614012: 100, 0.6741020618444631: 1000, 0.6662472846585037: 0}\n","[100] 0.6847822296614012\n","CPU times: total: 15min 3s\n","Wall time: 1min 59s\n"]}],"source":["%%time\n","vals = {}\n","bs = 128\n","epochs = 4\n","lr = 0.1\n","C = 0\n","beta_m = 0.9\n","beta_v = 0.999\n","err = 1e-8\n","\n","for C in Cs:\n","    w, b, l = train6(X_train, y_train, bs, epochs, lr, C, beta_m, beta_v, err)\n","    result = score(y_test, predict(X_test, w, b))\n","    # print(w)\n","    # print(b)\n","    # print(min(l))\n","    print(l.index(min(l)))\n","    print(lr, C, beta_m, beta_v, err, result)\n","    vals[result] = C\n","\n","print(vals)\n","maxval = max(vals.keys())\n","res = [v for k, v in vals.items() if k==maxval]\n","print(res, maxval)"]},{"cell_type":"markdown","metadata":{"cell_id":"654fc227f0a3475496364d46011a61fd","deepnote_cell_height":52.390625,"deepnote_cell_type":"markdown","tags":[]},"source":["We shall choose a regularisation coefficient of `C = 100`."]},{"cell_type":"code","execution_count":40,"metadata":{"cell_id":"ef8f9fd43d2347328baa1bc48f2c59d9","deepnote_cell_height":1097,"deepnote_cell_type":"code","deepnote_to_be_reexecuted":true,"execution_millis":532213,"execution_start":1658678203178,"source_hash":"994d17cb","tags":[],"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy: 0.6890130353817505\n","Macro-F1 score: 0.6662472846585037\n","400\n","0.1 0 0.9 0.999 1e-08 0.6662472846585037\n","Accuracy: 0.6904096834264432\n","Macro-F1 score: 0.6673977263492802\n","400\n","0.1 0 0.8 0.999 1e-08 0.6673977263492802\n","Accuracy: 0.6901769087523277\n","Macro-F1 score: 0.6668398896756464\n","400\n","0.1 0 0.7 0.999 1e-08 0.6668398896756464\n","Accuracy: 0.6908752327746741\n","Macro-F1 score: 0.6674584588869584\n","400\n","0.1 0 0.6 0.999 1e-08 0.6674584588869584\n","Accuracy: 0.6901769087523277\n","Macro-F1 score: 0.6664851322495824\n","400\n","0.1 0 0.5 0.999 1e-08 0.6664851322495824\n","Accuracy: 0.6911080074487895\n","Macro-F1 score: 0.6672200359410805\n","400\n","0.1 0 0.4 0.999 1e-08 0.6672200359410805\n","Accuracy: 0.6901769087523277\n","Macro-F1 score: 0.6673659689525142\n","398\n","0.1 0 0.3 0.999 1e-08 0.6673659689525142\n","Accuracy: 0.6901769087523277\n","Macro-F1 score: 0.6673659689525142\n","398\n","0.1 0 0.2 0.999 1e-08 0.6673659689525142\n","Accuracy: 0.6894785847299814\n","Macro-F1 score: 0.6666598418821515\n","398\n","0.1 0 0.1 0.999 1e-08 0.6666598418821515\n","Accuracy: 0.688780260707635\n","Macro-F1 score: 0.6660410286078096\n","398\n","0.1 0 0.0 0.999 1e-08 0.6660410286078096\n","{0.6662472846585037: 0.9, 0.6673977263492802: 0.8, 0.6668398896756464: 0.7, 0.6674584588869584: 0.6, 0.6664851322495824: 0.5, 0.6672200359410805: 0.4, 0.6673659689525142: 0.2, 0.6666598418821515: 0.1, 0.6660410286078096: 0.0}\n","[0.6] 0.6674584588869584\n","CPU times: total: 25min 12s\n","Wall time: 3min 14s\n"]}],"source":["%%time\n","vals = {}\n","bs = 128\n","epochs = 4\n","lr = 0.1\n","C = 0\n","beta_m = 0.9\n","beta_v = 0.999\n","err = 1e-8\n","\n","for beta_m in Bm:\n","    w, b, l = train6(X_train, y_train, bs, epochs, lr, C, beta_m, beta_v, err)\n","    result = score(y_test, predict(X_test, w, b))\n","    # print(w)\n","    # print(b)\n","    # print(min(l))\n","    print(l.index(min(l)))\n","    print(lr, C, beta_m, beta_v, err, result)\n","    vals[result] = beta_m\n","\n","print(vals)\n","maxval = max(vals.keys())\n","res = [v for k, v in vals.items() if k==maxval]\n","print(res, maxval)"]},{"cell_type":"markdown","metadata":{"cell_id":"c7c1bbe3b3134a8a9ec62f6210a7a5d1","deepnote_cell_height":52.390625,"deepnote_cell_type":"markdown","tags":[]},"source":["We shall choose a Momentum coefficient of `beta_m = 0.6`."]},{"cell_type":"code","execution_count":41,"metadata":{"cell_id":"da16fd14ed2246e18f62329dfc8047b6","deepnote_cell_height":1012.796875,"deepnote_cell_type":"code","deepnote_to_be_reexecuted":true,"execution_millis":305369,"execution_start":1658678735406,"source_hash":"2f130d35","tags":[],"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy: 0.6890130353817505\n","Macro-F1 score: 0.6662472846585037\n","400\n","0.1 0 0.9 0.999 1e-08 0.6662472846585037\n","Accuracy: 0.6885474860335196\n","Macro-F1 score: 0.6656603211681549\n","400\n","0.1 0 0.9 0.997 1e-08 0.6656603211681549\n","Accuracy: 0.6894785847299814\n","Macro-F1 score: 0.6661331817080218\n","400\n","0.1 0 0.9 0.995 1e-08 0.6661331817080218\n","Accuracy: 0.6885474860335196\n","Macro-F1 score: 0.6651320818030984\n","400\n","0.1 0 0.9 0.993 1e-08 0.6651320818030984\n","Accuracy: 0.6883147113594041\n","Macro-F1 score: 0.6646593875910989\n","400\n","0.1 0 0.9 0.991 1e-08 0.6646593875910989\n","Accuracy: 0.688780260707635\n","Macro-F1 score: 0.6650710789626108\n","400\n","0.1 0 0.9 0.989 1e-08 0.6650710789626108\n","{0.6662472846585037: 0.999, 0.6656603211681549: 0.997, 0.6661331817080218: 0.995, 0.6651320818030984: 0.993, 0.6646593875910989: 0.991, 0.6650710789626108: 0.989}\n","[0.999] 0.6662472846585037\n","CPU times: total: 15min 6s\n","Wall time: 1min 54s\n"]}],"source":["%%time\n","vals = {}\n","bs = 128\n","epochs = 4\n","lr = 0.1\n","C = 0\n","beta_m = 0.9\n","beta_v = 0.999\n","err = 1e-8\n","\n","for beta_v in Bv:\n","    w, b, l = train6(X_train, y_train, bs, epochs, lr, C, beta_m, beta_v, err)\n","    result = score(y_test, predict(X_test, w, b))\n","    # print(w)\n","    # print(b)\n","    # print(min(l))\n","    print(l.index(min(l)))\n","    print(lr, C, beta_m, beta_v, err, result)\n","    vals[result] = beta_v\n","\n","print(vals)\n","maxval = max(vals.keys())\n","res = [v for k, v in vals.items() if k==maxval]\n","print(res, maxval)"]},{"cell_type":"markdown","metadata":{"cell_id":"8d44fa87a5b741daaf184e4b1c9cefa3","deepnote_cell_height":52.390625,"deepnote_cell_type":"markdown","tags":[]},"source":["We shall choose a RMSProp coefficient of `beta_v = 0.999`."]},{"cell_type":"code","execution_count":42,"metadata":{"cell_id":"9a061a8962f545b487bd86565bd3326b","deepnote_cell_height":928,"deepnote_cell_type":"code","deepnote_to_be_reexecuted":true,"execution_millis":240096,"execution_start":1658673935844,"source_hash":"61d62e58","tags":[],"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy: 0.41783054003724396\n","Macro-F1 score: 0.3849706750029923\n","400\n","0.1 0 0.9 0.999 1 0.3849706750029923\n","Accuracy: 0.7178770949720671\n","Macro-F1 score: 0.677383393357214\n","400\n","0.1 0 0.9 0.999 0.01 0.677383393357214\n","Accuracy: 0.6978584729981379\n","Macro-F1 score: 0.6746235436795763\n","400\n","0.1 0 0.9 0.999 0.0001 0.6746235436795763\n","Accuracy: 0.6892458100558659\n","Macro-F1 score: 0.6664535557239946\n","400\n","0.1 0 0.9 0.999 1e-06 0.6664535557239946\n","Accuracy: 0.6890130353817505\n","Macro-F1 score: 0.6662472846585037\n","400\n","0.1 0 0.9 0.999 1e-08 0.6662472846585037\n","{0.3849706750029923: 1, 0.677383393357214: 0.01, 0.6746235436795763: 0.0001, 0.6664535557239946: 1e-06, 0.6662472846585037: 1e-08}\n","[0.01] 0.677383393357214\n","CPU times: total: 12min 8s\n","Wall time: 1min 51s\n"]}],"source":["%%time\n","vals = {}\n","bs = 128\n","epochs = 4\n","lr = 0.1\n","C = 0\n","beta_m = 0.9\n","beta_v = 0.999\n","err = 1e-8\n","\n","for err in Errs:\n","    w, b, l = train6(X_train, y_train, bs, epochs, lr, C, beta_m, beta_v, err)\n","    result = score(y_test, predict(X_test, w, b))\n","    # print(w)\n","    # print(b)\n","    # print(min(l))\n","    print(l.index(min(l)))\n","    print(lr, C, beta_m, beta_v, err, result)\n","    vals[result] = err\n","\n","print(vals)\n","maxval = max(vals.keys())\n","res = [v for k, v in vals.items() if k==maxval]\n","print(res, maxval)"]},{"cell_type":"markdown","metadata":{"cell_id":"399c9be926c8452a849f1915e51d7eda","deepnote_cell_height":52.390625,"deepnote_cell_type":"markdown","tags":[]},"source":["We shall choose an Error Term of `err = 0.01`."]},{"cell_type":"markdown","metadata":{"cell_id":"e180cf97009646918155b28b3ed4f16e","deepnote_cell_height":74.796875,"deepnote_cell_type":"markdown","tags":[]},"source":["Assuming that the hyperparameters are independent of one another, we will now decide on the Batch Size and Epoch Size to train our model."]},{"cell_type":"code","execution_count":43,"metadata":{"cell_id":"1d54624481ed4d52a55a467afccd3320","deepnote_cell_height":222,"deepnote_cell_type":"code","deepnote_to_be_reexecuted":true,"execution_millis":2,"execution_start":1658681589053,"source_hash":"f873e1e6","tags":[],"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Batch Sizes: [4096, 2048, 1024, 512, 256, 128, 64]\n","Epoch Sizes: [1, 2, 4, 8, 16, 32, 64, 128]\n"]}],"source":["Bs = [64*(2**i) for i in range(7)]\n","Bs.reverse()\n","Es = [2**(i) for i in range(8)]\n","\n","print(f\"Batch Sizes: {Bs}\")\n","print(f\"Epoch Sizes: {Es}\")"]},{"cell_type":"code","execution_count":44,"metadata":{"cell_id":"6299f80d74ff43aa91fac42121dc9712","deepnote_cell_height":934.796875,"deepnote_cell_type":"code","deepnote_to_be_reexecuted":true,"execution_millis":372691,"execution_start":1658677441479,"source_hash":"6153f8ec","tags":[],"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy: 0.3961824953445065\n","Macro-F1 score: 0.32737814572174595\n","4096 8 0.1 100 0.6 0.999 0.01 0.32737814572174595\n","Accuracy: 0.4590316573556797\n","Macro-F1 score: 0.45136005106620936\n","2048 8 0.1 100 0.6 0.999 0.01 0.45136005106620936\n","Accuracy: 0.5833333333333334\n","Macro-F1 score: 0.5710142883665134\n","1024 8 0.1 100 0.6 0.999 0.01 0.5710142883665134\n","Accuracy: 0.6820297951582868\n","Macro-F1 score: 0.6424082898028336\n","512 8 0.1 100 0.6 0.999 0.01 0.6424082898028336\n","Accuracy: 0.7176443202979516\n","Macro-F1 score: 0.6713070335546949\n","256 8 0.1 100 0.6 0.999 0.01 0.6713070335546949\n","Accuracy: 0.7225325884543762\n","Macro-F1 score: 0.6757364652101494\n","128 8 0.1 100 0.6 0.999 0.01 0.6757364652101494\n","Accuracy: 0.7155493482309124\n","Macro-F1 score: 0.6692042924893535\n","64 8 0.1 100 0.6 0.999 0.01 0.6692042924893535\n","{0.32737814572174595: (4096, 8), 0.45136005106620936: (2048, 8), 0.5710142883665134: (1024, 8), 0.6424082898028336: (512, 8), 0.6713070335546949: (256, 8), 0.6757364652101494: (128, 8), 0.6692042924893535: (64, 8)}\n","[(128, 8)] 0.6757364652101494\n","CPU times: total: 19min 33s\n","Wall time: 2min 39s\n"]}],"source":["%%time\n","vals = {}\n","epochs = 8\n","lr = 0.1\n","C = 100\n","beta_m = 0.6\n","beta_v = 0.999\n","err = 0.01\n","\n","for bs in Bs:\n","    w, b, l = train6(X_train, y_train, bs, epochs, lr, C, beta_m, beta_v, err)\n","    result = score(y_test, predict(X_test, w, b))\n","    # print(w)\n","    # print(b)\n","    # print(min(l))\n","    # print(l.index(min(l)))\n","    print(bs, epochs, lr, C, beta_m, beta_v, err, result)\n","    vals[result] = (bs, epochs)\n","\n","print(vals)\n","maxval = max(vals.keys())\n","res = [v for k, v in vals.items() if k==maxval]\n","print(res, maxval)"]},{"cell_type":"markdown","metadata":{"cell_id":"d1d28c1877104b2688bed296c91551dc","deepnote_cell_height":97.1875,"deepnote_cell_type":"markdown","tags":[]},"source":["Based on the above, we can observe that for fixed epoch, the general trend is that the smaller the batch size, the better the performance. However, we have noted that the performance drops when the batch size decreases from 128 to 64, implying that the model may have overfitted with the training dataset."]},{"cell_type":"code","execution_count":45,"metadata":{"cell_id":"00036-3b9435e2-6c4c-4e47-b4db-98d47bc454b7","deepnote_cell_height":934.796875,"deepnote_cell_type":"code","deepnote_to_be_reexecuted":true,"execution_millis":852126,"execution_start":1658679040793,"source_hash":"74277725","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy: 0.728584729981378\n","Macro-F1 score: 0.6950824941075637\n","4096 256 0.1 100 0.6 0.999 0.01 0.6950824941075637\n","Accuracy: 0.7316108007448789\n","Macro-F1 score: 0.6966186243774475\n","2048 128 0.1 100 0.6 0.999 0.01 0.6966186243774475\n","Accuracy: 0.729050279329609\n","Macro-F1 score: 0.6937789760740024\n","1024 64 0.1 100 0.6 0.999 0.01 0.6937789760740024\n","Accuracy: 0.7309124767225326\n","Macro-F1 score: 0.6957748632444369\n","512 32 0.1 100 0.6 0.999 0.01 0.6957748632444369\n","Accuracy: 0.728584729981378\n","Macro-F1 score: 0.6903260458584444\n","256 16 0.1 100 0.6 0.999 0.01 0.6903260458584444\n","Accuracy: 0.7225325884543762\n","Macro-F1 score: 0.6757364652101494\n","128 8 0.1 100 0.6 0.999 0.01 0.6757364652101494\n","Accuracy: 0.7167132216014898\n","Macro-F1 score: 0.6630075908232557\n","64 4 0.1 100 0.6 0.999 0.01 0.6630075908232557\n","{0.6950824941075637: (4096, 256), 0.6966186243774475: (2048, 128), 0.6937789760740024: (1024, 64), 0.6957748632444369: (512, 32), 0.6903260458584444: (256, 16), 0.6757364652101494: (128, 8), 0.6630075908232557: (64, 4)}\n","[(2048, 128)] 0.6966186243774475\n","CPU times: total: 39min 17s\n","Wall time: 6min 12s\n"]}],"source":["%%time\n","vals = {}\n","lr = 0.1\n","C = 100\n","beta_m = 0.6\n","beta_v = 0.999\n","err = 0.01\n","\n","for bs in Bs:\n","    epochs = bs // 16\n","    w, b, l = train6(X_train, y_train, bs, epochs, lr, C, beta_m, beta_v, err)\n","    result = score(y_test, predict(X_test, w, b))\n","    # print(w)\n","    # print(b)\n","    # print(min(l))\n","    # print(l.index(min(l)))\n","    print(bs, epochs, lr, C, beta_m, beta_v, err, result)\n","    vals[result] = (bs, epochs)\n","\n","print(vals)\n","maxval = max(vals.keys())\n","res = [v for k, v in vals.items() if k==maxval]\n","print(res, maxval)"]},{"cell_type":"markdown","metadata":{"cell_id":"aa8adfffda094951a754372820042fa1","deepnote_cell_height":52.390625,"deepnote_cell_type":"markdown","tags":[]},"source":["Based on the above, a batch size of `bs = 2048` will result in the best model performance."]},{"cell_type":"code","execution_count":46,"metadata":{"cell_id":"949382c344aa4e0b81afbfc266a29986","deepnote_cell_height":994.796875,"deepnote_cell_type":"code","deepnote_to_be_reexecuted":true,"execution_millis":255644,"execution_start":1658681600341,"source_hash":"b875ab39","tags":[],"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy: 0.3745344506517691\n","Macro-F1 score: 0.2724809483488569\n","2048 1 0.1 100 0.6 0.999 0.01 0.2724809483488569\n","Accuracy: 0.37756052141527\n","Macro-F1 score: 0.28019622016520107\n","2048 2 0.1 100 0.6 0.999 0.01 0.28019622016520107\n","Accuracy: 0.3957169459962756\n","Macro-F1 score: 0.32663601058793984\n","2048 4 0.1 100 0.6 0.999 0.01 0.32663601058793984\n","Accuracy: 0.4590316573556797\n","Macro-F1 score: 0.45136005106620936\n","2048 8 0.1 100 0.6 0.999 0.01 0.45136005106620936\n","Accuracy: 0.5833333333333334\n","Macro-F1 score: 0.5703066677276047\n","2048 16 0.1 100 0.6 0.999 0.01 0.5703066677276047\n","Accuracy: 0.675512104283054\n","Macro-F1 score: 0.6373252069053643\n","2048 32 0.1 100 0.6 0.999 0.01 0.6373252069053643\n","Accuracy: 0.7206703910614525\n","Macro-F1 score: 0.6803361332031991\n","2048 64 0.1 100 0.6 0.999 0.01 0.6803361332031991\n","Accuracy: 0.7316108007448789\n","Macro-F1 score: 0.6966186243774475\n","2048 128 0.1 100 0.6 0.999 0.01 0.6966186243774475\n","{0.2724809483488569: (2048, 1), 0.28019622016520107: (2048, 2), 0.32663601058793984: (2048, 4), 0.45136005106620936: (2048, 8), 0.5703066677276047: (2048, 16), 0.6373252069053643: (2048, 32), 0.6803361332031991: (2048, 64), 0.6966186243774475: (2048, 128)}\n","[(2048, 128)] 0.6966186243774475\n","CPU times: total: 12min 38s\n","Wall time: 2min 38s\n"]}],"source":["%%time\n","vals = {}\n","bs = 2048\n","lr = 0.1\n","C = 100\n","beta_m = 0.6\n","beta_v = 0.999\n","err = 0.01\n","\n","for epochs in Es:\n","    w, b, l = train6(X_train, y_train, bs, epochs, lr, C, beta_m, beta_v, err)\n","    result = score(y_test, predict(X_test, w, b))\n","    # print(w)\n","    # print(b)\n","    # print(min(l))\n","    # print(l.index(min(l)))\n","    print(bs, epochs, lr, C, beta_m, beta_v, err, result)\n","    vals[result] = (bs, epochs)\n","\n","print(vals)\n","maxval = max(vals.keys())\n","res = [v for k, v in vals.items() if k==maxval]\n","print(res, maxval)"]},{"cell_type":"markdown","metadata":{"cell_id":"483816e99eca4b4296d6a329b5f2d5dd","deepnote_cell_height":74.796875,"deepnote_cell_type":"markdown","tags":[]},"source":["Based on the above, we can clearly see that a larger epoch size will improve the model performance. However, there is a tradeoff in terms of the time spent to train the model. As such, we shall choose an epoch size of `epochs = 256` to train our model."]},{"cell_type":"code","execution_count":47,"metadata":{"cell_id":"77b57ac8664f47079777bdb4f59f0959","deepnote_cell_height":189,"deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":2,"execution_start":1658683807833,"source_hash":"1d06537f","tags":[],"trusted":true},"outputs":[],"source":["bs = 2048\n","epochs = 256\n","lr = 0.1 #best: 0.1 (btw 0.10 and 0.16)\n","C = 100 #best: 100\n","beta_m = 0.6 #No pattern could be clearly determined (classic rec: 0.9)\n","beta_v = 0.999 #(as close to 1) (classic rec: 0.999)\n","err = 0.01 #0.01 (classic rec: 1e-8)"]},{"cell_type":"code","execution_count":48,"metadata":{"cell_id":"1030069b017a421c99dfc46ae7cbfde7","deepnote_cell_height":492.1875,"deepnote_cell_type":"code","deepnote_output_heights":[null,21.1875],"deepnote_to_be_reexecuted":false,"execution_millis":275476,"execution_start":1658683811716,"source_hash":"7abb0f5","tags":[],"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["[[-0.15609145]\n"," [-1.1398634 ]\n"," [-0.47299625]\n"," ...\n"," [ 0.31361622]\n"," [ 0.59016236]\n"," [ 0.52905051]]\n","-1.3197810454686933\n","0.43972440359598974\n","1536\n","Accuracy: 0.7358007448789572\n","Macro-F1 score: 0.7062293681825118\n","CPU times: total: 11min 25s\n","Wall time: 2min 29s\n"]},{"data":{"text/plain":["0.7062293681825118"]},"execution_count":48,"metadata":{},"output_type":"execute_result"}],"source":["%%time\n","w, b, l = train6(X_train, y_train, bs, epochs, lr, C, beta_m, beta_v, err)\n","print(w)\n","print(b)\n","print(min(l))\n","print(l.index(min(l)))\n","Model7 = score(y_test, predict(X_test, w, b))\n","Model7"]},{"cell_type":"markdown","metadata":{},"source":["Based on the above, we have achieved a macro f1-score of `0.70623`, which has surpassed the performance of the default setup of the SKLearn Logistic Regression Package. We shall now try to implement robust scaling to increase the separation of the training data. We assume that our model will perform better when the data points are scaled as the model would be able to find a better decision boundary that can clearly separate the labels."]},{"cell_type":"code","execution_count":49,"metadata":{"trusted":true},"outputs":[],"source":["from sklearn.preprocessing import RobustScaler as Scaler"]},{"cell_type":"code","execution_count":56,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>0</th>\n","      <th>1</th>\n","      <th>2</th>\n","      <th>3</th>\n","      <th>4</th>\n","      <th>5</th>\n","      <th>6</th>\n","      <th>7</th>\n","      <th>8</th>\n","      <th>9</th>\n","      <th>...</th>\n","      <th>4990</th>\n","      <th>4991</th>\n","      <th>4992</th>\n","      <th>4993</th>\n","      <th>4994</th>\n","      <th>4995</th>\n","      <th>4996</th>\n","      <th>4997</th>\n","      <th>4998</th>\n","      <th>4999</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>count</th>\n","      <td>17184.000000</td>\n","      <td>17184.000000</td>\n","      <td>17184.000000</td>\n","      <td>17184.000000</td>\n","      <td>17184.000000</td>\n","      <td>17184.000000</td>\n","      <td>17184.000000</td>\n","      <td>17184.000000</td>\n","      <td>17184.000000</td>\n","      <td>17184.000000</td>\n","      <td>...</td>\n","      <td>17184.000000</td>\n","      <td>17184.000000</td>\n","      <td>17184.000000</td>\n","      <td>17184.000000</td>\n","      <td>17184.000000</td>\n","      <td>17184.000000</td>\n","      <td>17184.000000</td>\n","      <td>17184.000000</td>\n","      <td>17184.000000</td>\n","      <td>17184.000000</td>\n","    </tr>\n","    <tr>\n","      <th>mean</th>\n","      <td>0.000150</td>\n","      <td>0.001066</td>\n","      <td>0.001532</td>\n","      <td>0.000369</td>\n","      <td>0.000140</td>\n","      <td>0.000066</td>\n","      <td>0.000270</td>\n","      <td>0.000483</td>\n","      <td>0.000406</td>\n","      <td>0.000418</td>\n","      <td>...</td>\n","      <td>0.000202</td>\n","      <td>0.000429</td>\n","      <td>0.000286</td>\n","      <td>0.000075</td>\n","      <td>0.000260</td>\n","      <td>0.000709</td>\n","      <td>0.000257</td>\n","      <td>0.000121</td>\n","      <td>0.000308</td>\n","      <td>0.000159</td>\n","    </tr>\n","    <tr>\n","      <th>std</th>\n","      <td>0.008297</td>\n","      <td>0.019532</td>\n","      <td>0.024741</td>\n","      <td>0.012334</td>\n","      <td>0.008276</td>\n","      <td>0.005065</td>\n","      <td>0.009907</td>\n","      <td>0.013106</td>\n","      <td>0.012402</td>\n","      <td>0.013211</td>\n","      <td>...</td>\n","      <td>0.010215</td>\n","      <td>0.013178</td>\n","      <td>0.011378</td>\n","      <td>0.005866</td>\n","      <td>0.010864</td>\n","      <td>0.017641</td>\n","      <td>0.010246</td>\n","      <td>0.006529</td>\n","      <td>0.010526</td>\n","      <td>0.008536</td>\n","    </tr>\n","    <tr>\n","      <th>min</th>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>...</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>25%</th>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>...</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>50%</th>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>...</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>75%</th>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>...</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>max</th>\n","      <td>0.676327</td>\n","      <td>0.560830</td>\n","      <td>0.958430</td>\n","      <td>0.646740</td>\n","      <td>0.532789</td>\n","      <td>0.437760</td>\n","      <td>0.435835</td>\n","      <td>0.536746</td>\n","      <td>0.546247</td>\n","      <td>0.550237</td>\n","      <td>...</td>\n","      <td>0.611122</td>\n","      <td>0.540809</td>\n","      <td>0.566613</td>\n","      <td>0.592170</td>\n","      <td>0.617341</td>\n","      <td>0.850605</td>\n","      <td>0.484908</td>\n","      <td>0.398105</td>\n","      <td>0.430031</td>\n","      <td>0.528556</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>8 rows × 5000 columns</p>\n","</div>"],"text/plain":["                  0             1             2             3             4  \\\n","count  17184.000000  17184.000000  17184.000000  17184.000000  17184.000000   \n","mean       0.000150      0.001066      0.001532      0.000369      0.000140   \n","std        0.008297      0.019532      0.024741      0.012334      0.008276   \n","min        0.000000      0.000000      0.000000      0.000000      0.000000   \n","25%        0.000000      0.000000      0.000000      0.000000      0.000000   \n","50%        0.000000      0.000000      0.000000      0.000000      0.000000   \n","75%        0.000000      0.000000      0.000000      0.000000      0.000000   \n","max        0.676327      0.560830      0.958430      0.646740      0.532789   \n","\n","                  5             6             7             8             9  \\\n","count  17184.000000  17184.000000  17184.000000  17184.000000  17184.000000   \n","mean       0.000066      0.000270      0.000483      0.000406      0.000418   \n","std        0.005065      0.009907      0.013106      0.012402      0.013211   \n","min        0.000000      0.000000      0.000000      0.000000      0.000000   \n","25%        0.000000      0.000000      0.000000      0.000000      0.000000   \n","50%        0.000000      0.000000      0.000000      0.000000      0.000000   \n","75%        0.000000      0.000000      0.000000      0.000000      0.000000   \n","max        0.437760      0.435835      0.536746      0.546247      0.550237   \n","\n","       ...          4990          4991          4992          4993  \\\n","count  ...  17184.000000  17184.000000  17184.000000  17184.000000   \n","mean   ...      0.000202      0.000429      0.000286      0.000075   \n","std    ...      0.010215      0.013178      0.011378      0.005866   \n","min    ...      0.000000      0.000000      0.000000      0.000000   \n","25%    ...      0.000000      0.000000      0.000000      0.000000   \n","50%    ...      0.000000      0.000000      0.000000      0.000000   \n","75%    ...      0.000000      0.000000      0.000000      0.000000   \n","max    ...      0.611122      0.540809      0.566613      0.592170   \n","\n","               4994          4995          4996          4997          4998  \\\n","count  17184.000000  17184.000000  17184.000000  17184.000000  17184.000000   \n","mean       0.000260      0.000709      0.000257      0.000121      0.000308   \n","std        0.010864      0.017641      0.010246      0.006529      0.010526   \n","min        0.000000      0.000000      0.000000      0.000000      0.000000   \n","25%        0.000000      0.000000      0.000000      0.000000      0.000000   \n","50%        0.000000      0.000000      0.000000      0.000000      0.000000   \n","75%        0.000000      0.000000      0.000000      0.000000      0.000000   \n","max        0.617341      0.850605      0.484908      0.398105      0.430031   \n","\n","               4999  \n","count  17184.000000  \n","mean       0.000159  \n","std        0.008536  \n","min        0.000000  \n","25%        0.000000  \n","50%        0.000000  \n","75%        0.000000  \n","max        0.528556  \n","\n","[8 rows x 5000 columns]"]},"execution_count":56,"metadata":{},"output_type":"execute_result"}],"source":["df_train.iloc[:, 2:5002].describe()"]},{"cell_type":"code","execution_count":58,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["X: (17184, 5000), y: (17184,)\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>0</th>\n","      <th>1</th>\n","      <th>2</th>\n","      <th>3</th>\n","      <th>4</th>\n","      <th>5</th>\n","      <th>6</th>\n","      <th>7</th>\n","      <th>8</th>\n","      <th>9</th>\n","      <th>...</th>\n","      <th>4990</th>\n","      <th>4991</th>\n","      <th>4992</th>\n","      <th>4993</th>\n","      <th>4994</th>\n","      <th>4995</th>\n","      <th>4996</th>\n","      <th>4997</th>\n","      <th>4998</th>\n","      <th>4999</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>count</th>\n","      <td>17184.000000</td>\n","      <td>17184.000000</td>\n","      <td>17184.000000</td>\n","      <td>17184.000000</td>\n","      <td>17184.000000</td>\n","      <td>17184.000000</td>\n","      <td>17184.000000</td>\n","      <td>17184.000000</td>\n","      <td>17184.000000</td>\n","      <td>17184.000000</td>\n","      <td>...</td>\n","      <td>17184.000000</td>\n","      <td>17184.000000</td>\n","      <td>17184.000000</td>\n","      <td>17184.000000</td>\n","      <td>17184.000000</td>\n","      <td>17184.000000</td>\n","      <td>17184.000000</td>\n","      <td>17184.000000</td>\n","      <td>17184.000000</td>\n","      <td>17184.000000</td>\n","    </tr>\n","    <tr>\n","      <th>mean</th>\n","      <td>0.000202</td>\n","      <td>0.001437</td>\n","      <td>0.002067</td>\n","      <td>0.000498</td>\n","      <td>0.000189</td>\n","      <td>0.000089</td>\n","      <td>0.000364</td>\n","      <td>0.000652</td>\n","      <td>0.000548</td>\n","      <td>0.000564</td>\n","      <td>...</td>\n","      <td>0.000272</td>\n","      <td>0.000579</td>\n","      <td>0.000386</td>\n","      <td>0.000101</td>\n","      <td>0.000350</td>\n","      <td>0.000957</td>\n","      <td>0.000347</td>\n","      <td>0.000163</td>\n","      <td>0.000415</td>\n","      <td>0.000214</td>\n","    </tr>\n","    <tr>\n","      <th>std</th>\n","      <td>0.011192</td>\n","      <td>0.026349</td>\n","      <td>0.033375</td>\n","      <td>0.016639</td>\n","      <td>0.011164</td>\n","      <td>0.006832</td>\n","      <td>0.013364</td>\n","      <td>0.017679</td>\n","      <td>0.016730</td>\n","      <td>0.017821</td>\n","      <td>...</td>\n","      <td>0.013780</td>\n","      <td>0.017776</td>\n","      <td>0.015349</td>\n","      <td>0.007914</td>\n","      <td>0.014656</td>\n","      <td>0.023798</td>\n","      <td>0.013821</td>\n","      <td>0.008808</td>\n","      <td>0.014199</td>\n","      <td>0.011514</td>\n","    </tr>\n","    <tr>\n","      <th>min</th>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>...</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>25%</th>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>...</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>50%</th>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>...</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>75%</th>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>...</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>max</th>\n","      <td>0.912352</td>\n","      <td>0.756548</td>\n","      <td>1.292902</td>\n","      <td>0.872439</td>\n","      <td>0.718721</td>\n","      <td>0.590529</td>\n","      <td>0.587933</td>\n","      <td>0.724059</td>\n","      <td>0.736876</td>\n","      <td>0.742259</td>\n","      <td>...</td>\n","      <td>0.824391</td>\n","      <td>0.729541</td>\n","      <td>0.764349</td>\n","      <td>0.798826</td>\n","      <td>0.832780</td>\n","      <td>1.147449</td>\n","      <td>0.654131</td>\n","      <td>0.537036</td>\n","      <td>0.580103</td>\n","      <td>0.713011</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>8 rows × 5000 columns</p>\n","</div>"],"text/plain":["               0             1             2             3             4     \\\n","count  17184.000000  17184.000000  17184.000000  17184.000000  17184.000000   \n","mean       0.000202      0.001437      0.002067      0.000498      0.000189   \n","std        0.011192      0.026349      0.033375      0.016639      0.011164   \n","min        0.000000      0.000000      0.000000      0.000000      0.000000   \n","25%        0.000000      0.000000      0.000000      0.000000      0.000000   \n","50%        0.000000      0.000000      0.000000      0.000000      0.000000   \n","75%        0.000000      0.000000      0.000000      0.000000      0.000000   \n","max        0.912352      0.756548      1.292902      0.872439      0.718721   \n","\n","               5             6             7             8             9     \\\n","count  17184.000000  17184.000000  17184.000000  17184.000000  17184.000000   \n","mean       0.000089      0.000364      0.000652      0.000548      0.000564   \n","std        0.006832      0.013364      0.017679      0.016730      0.017821   \n","min        0.000000      0.000000      0.000000      0.000000      0.000000   \n","25%        0.000000      0.000000      0.000000      0.000000      0.000000   \n","50%        0.000000      0.000000      0.000000      0.000000      0.000000   \n","75%        0.000000      0.000000      0.000000      0.000000      0.000000   \n","max        0.590529      0.587933      0.724059      0.736876      0.742259   \n","\n","       ...          4990          4991          4992          4993  \\\n","count  ...  17184.000000  17184.000000  17184.000000  17184.000000   \n","mean   ...      0.000272      0.000579      0.000386      0.000101   \n","std    ...      0.013780      0.017776      0.015349      0.007914   \n","min    ...      0.000000      0.000000      0.000000      0.000000   \n","25%    ...      0.000000      0.000000      0.000000      0.000000   \n","50%    ...      0.000000      0.000000      0.000000      0.000000   \n","75%    ...      0.000000      0.000000      0.000000      0.000000   \n","max    ...      0.824391      0.729541      0.764349      0.798826   \n","\n","               4994          4995          4996          4997          4998  \\\n","count  17184.000000  17184.000000  17184.000000  17184.000000  17184.000000   \n","mean       0.000350      0.000957      0.000347      0.000163      0.000415   \n","std        0.014656      0.023798      0.013821      0.008808      0.014199   \n","min        0.000000      0.000000      0.000000      0.000000      0.000000   \n","25%        0.000000      0.000000      0.000000      0.000000      0.000000   \n","50%        0.000000      0.000000      0.000000      0.000000      0.000000   \n","75%        0.000000      0.000000      0.000000      0.000000      0.000000   \n","max        0.832780      1.147449      0.654131      0.537036      0.580103   \n","\n","               4999  \n","count  17184.000000  \n","mean       0.000214  \n","std        0.011514  \n","min        0.000000  \n","25%        0.000000  \n","50%        0.000000  \n","75%        0.000000  \n","max        0.713011  \n","\n","[8 rows x 5000 columns]"]},"execution_count":58,"metadata":{},"output_type":"execute_result"}],"source":["X = df_train.iloc[:, 2:5002].to_numpy()\n","y = df_train.iloc[:,1].to_numpy()\n","print(f\"X: {X.shape}, y: {y.shape}\")\n","\n","scaler = Scaler(with_centering=False, unit_variance=True).fit(X)\n","X_scaler = scaler.transform(X)\n","scaled = pd.DataFrame(X_scaler)\n","scaled.describe()"]},{"cell_type":"code","execution_count":54,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["X_train: (12888, 5000), y_train: (12888,)\n","X_test: (4296, 5000), y_test: (4296,)\n","CPU times: total: 2.06 s\n","Wall time: 2.06 s\n"]}],"source":["%%time\n","X_train, X_test, y_train, y_test = train_test_split(X_scaler, y, test_size=0.25, random_state=100)\n","print(f\"X_train: {X_train.shape}, y_train: {y_train.shape}\")\n","print(f\"X_test: {X_test.shape}, y_test: {y_test.shape}\")"]},{"cell_type":"code","execution_count":55,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["[[-0.40477117]\n"," [-1.21947652]\n"," [-0.52880814]\n"," ...\n"," [ 0.26783656]\n"," [ 0.56554289]\n"," [ 0.60708187]]\n","-1.4062877696318705\n","0.40790460884152074\n","1535\n","Accuracy: 0.7267225325884544\n","Macro-F1 score: 0.7004623928115308\n","CPU times: total: 11min 59s\n","Wall time: 2min 35s\n"]},{"data":{"text/plain":["0.7004623928115308"]},"execution_count":55,"metadata":{},"output_type":"execute_result"}],"source":["%%time\n","bs = 2048\n","epochs = 256\n","lr = 0.1 #best: 0.1 (btw 0.10 and 0.16)\n","C = 100 #best: 100\n","beta_m = 0.6 #No pattern could be clearly determined (classic rec: 0.9)\n","beta_v = 0.999 #(as close to 1) (classic rec: 0.999)\n","err = 0.01 #0.01 (classic rec: 1e-8)\n","\n","w, b, l = train6(X_train, y_train, bs, epochs, lr, C, beta_m, beta_v, err)\n","print(w)\n","print(b)\n","print(min(l))\n","print(l.index(min(l)))\n","Model8 = score(y_test, predict(X_test, w, b))\n","Model8"]},{"cell_type":"markdown","metadata":{},"source":["Based on the above, we have achieved a score of `0.70046`, which is a slight decrease in model performance. Hence, it would seem that scaling results in a negligible degradation in model performance."]},{"cell_type":"markdown","metadata":{"cell_id":"00068-89a646ec-2454-4320-b8c4-f5548f796b36","deepnote_cell_height":153.1875,"deepnote_cell_type":"markdown"},"source":["## Exporting Prediction\n","Prediction made by your Logistic Regression on the Test set. Note that you are welcome to submit your predicted labels to Kaggle but you will need to submit the final prediction output in the final project submission. Please label the file as \"LogRed_Prediction.csv\"."]},{"cell_type":"code","execution_count":59,"metadata":{"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>0</th>\n","      <th>1</th>\n","      <th>2</th>\n","      <th>3</th>\n","      <th>4</th>\n","      <th>5</th>\n","      <th>6</th>\n","      <th>7</th>\n","      <th>8</th>\n","      <th>...</th>\n","      <th>4990</th>\n","      <th>4991</th>\n","      <th>4992</th>\n","      <th>4993</th>\n","      <th>4994</th>\n","      <th>4995</th>\n","      <th>4996</th>\n","      <th>4997</th>\n","      <th>4998</th>\n","      <th>4999</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>17185</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>17186</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>17187</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>17188</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>17189</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>4291</th>\n","      <td>21476</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>4292</th>\n","      <td>21477</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>4293</th>\n","      <td>21478</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>4294</th>\n","      <td>21479</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>4295</th>\n","      <td>21480</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>4296 rows × 5001 columns</p>\n","</div>"],"text/plain":["         id    0    1    2    3    4    5    6    7    8  ...  4990  4991  \\\n","0     17185  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n","1     17186  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n","2     17187  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n","3     17188  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n","4     17189  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n","...     ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   ...   ...   \n","4291  21476  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n","4292  21477  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n","4293  21478  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n","4294  21479  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n","4295  21480  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n","\n","      4992  4993  4994  4995  4996  4997  4998  4999  \n","0      0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n","1      0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n","2      0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n","3      0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n","4      0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n","...    ...   ...   ...   ...   ...   ...   ...   ...  \n","4291   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n","4292   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n","4293   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n","4294   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n","4295   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n","\n","[4296 rows x 5001 columns]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["CPU times: total: 5.91 s\n","Wall time: 5.99 s\n"]}],"source":["%%time\n","df_test = pd.read_csv(r\"./source/test_tfidf_features.csv\")\n","display(df_test)"]},{"cell_type":"code","execution_count":61,"metadata":{"cell_id":"b98d59a7657446668ec90741a9492258","deepnote_cell_height":172.1875,"deepnote_cell_type":"code","deepnote_output_heights":[21.1875],"deepnote_to_be_reexecuted":false,"execution_millis":174,"execution_start":1658684807590,"source_hash":"341533dd","tags":[],"trusted":true},"outputs":[{"data":{"text/plain":["array([0, 0, 1, ..., 1, 0, 0])"]},"metadata":{},"output_type":"display_data"}],"source":["features = df_test.iloc[:,1:]\n","\n","results = predict(features, w, b)\n","display(results)"]},{"cell_type":"code","execution_count":62,"metadata":{"cell_id":"ebef1c3a31934cdda95c464e20cd5788","deepnote_cell_height":148,"deepnote_cell_type":"code","deepnote_output_heights":[null,21.1875],"deepnote_to_be_reexecuted":false,"execution_millis":97,"execution_start":1658684807669,"source_hash":"774f40c6","tags":[],"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["(4296, 1) (4296, 1)\n"]}],"source":["df_ids = df_test.iloc[:, 0].to_frame()\n","df_results = pd.DataFrame(results)\n","print(df_results.shape, df_ids.shape)"]},{"cell_type":"code","execution_count":63,"metadata":{"cell_id":"5e35eb9faffc4389a0b402260b3b6d95","deepnote_cell_height":636,"deepnote_cell_type":"code","deepnote_output_heights":[21.1875],"deepnote_table_invalid":false,"deepnote_table_loading":false,"deepnote_table_state":{"filters":[],"pageIndex":0,"pageSize":10,"sortBy":[]},"deepnote_to_be_reexecuted":false,"execution_millis":65,"execution_start":1658684807702,"source_hash":"510702b8","tags":[],"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>17185</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>17186</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>17187</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>17188</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>17189</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>4291</th>\n","      <td>21476</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4292</th>\n","      <td>21477</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4293</th>\n","      <td>21478</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4294</th>\n","      <td>21479</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4295</th>\n","      <td>21480</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>4296 rows × 2 columns</p>\n","</div>"],"text/plain":["         id  label\n","0     17185      0\n","1     17186      0\n","2     17187      1\n","3     17188      0\n","4     17189      0\n","...     ...    ...\n","4291  21476      0\n","4292  21477      0\n","4293  21478      1\n","4294  21479      0\n","4295  21480      0\n","\n","[4296 rows x 2 columns]"]},"metadata":{},"output_type":"display_data"}],"source":["df_submission = pd.concat([df_ids, df_results], axis =1)\n","df_submission = df_submission.rename(columns={0: 'label'})\n","display(df_submission)"]},{"cell_type":"code","execution_count":64,"metadata":{"cell_id":"e3da92dbbe6c42f28d0802f31f076964","deepnote_cell_height":99,"deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":23,"execution_start":1658684807735,"source_hash":"21d4263a","tags":[],"trusted":true},"outputs":[],"source":["# Write to csv\n","# df_submission.to_csv(\"LogRed_Prediction.csv\", index=False)"]},{"cell_type":"markdown","metadata":{"cell_id":"00053-672d396b-4f1d-48be-b852-56817a09cd49","deepnote_cell_height":399.1875,"deepnote_cell_type":"markdown"},"source":["# Task 2: Apply Dimension Reduction Techniques\n","\n","Dimension reduction is the transformation of data from a high-dimensional space into a low-dimensional space so that the low-dimensional representation retains some meaningful properties of the original data. \\\n","The train dataset contains 5000 TD-IDF features. In this task, you are to apply PCA to reduce the dimension of features.\n","\n","## Key Task Deliverables\n","\n","2a. Code implementation of PCA on the train and test sets. Note that you are allowed to use the sklearn package for this task. \\\n","2b. Report the Macro F1 scores for applying 2000, 1000, 500, and 100 components on the test set. Note that you will have to submit your predicted labels to Kaggle to retrieve the Macro F1 scores for the test set and report the results in your final report. \\\n","Use KNN as the machine learning model for your training and prediction (You are also allowed to use the sklearn package for KNN implementation) (set n_neighbors=2)."]},{"cell_type":"markdown","metadata":{},"source":["## Initial Setup\n","\n","Import Relevant Packages for Task 2"]},{"cell_type":"code","execution_count":1,"metadata":{"cell_id":"00054-b8072742-0be5-4e46-b68e-f2ede4fc6333","deepnote_cell_height":171,"deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":0,"execution_start":1658689638099,"source_hash":"b9067867","trusted":true},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import time\n","\n","from sklearn.decomposition import PCA, IncrementalPCA, KernelPCA, TruncatedSVD\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import f1_score\n","from sklearn.neighbors import KNeighborsClassifier as KNN"]},{"cell_type":"markdown","metadata":{},"source":["Define Functions to Evaluate the Model Performance from adopting Dimensionality Reduction. This will be done simultaneously to observe any improvement in Model Performance along with KNN from using Dimensionality Reduction Techniques"]},{"cell_type":"code","execution_count":2,"metadata":{"trusted":true},"outputs":[],"source":["def sigmoid(z):\n","    result = 1/(1 + np.exp(-z))\n","#     print(f\"sigmoid: {result}\")\n","    return result"]},{"cell_type":"code","execution_count":3,"metadata":{"trusted":true},"outputs":[],"source":["def loss(y, X, w, b, lmb):\n","    y_hat = sigmoid(np.dot(X, w) + b)\n","    m = np.shape(y)[0]\n","    \n","    loss = -1 * np.where(y == 1, np.log(y_hat), np.log(1 - y_hat)).mean()\n","    reg = lmb * np.sum(w**2) / (2 * m)\n","    error = loss + reg\n","    \n","#     print(f\"training loss = {loss}, regularisation term = {reg}, training error = {error}\")\n","    return error"]},{"cell_type":"code","execution_count":4,"metadata":{"trusted":true},"outputs":[],"source":["def gradients(y, X, w, b, lmb):\n","    # m - number of training examples\n","    m = np.shape(X)[0]\n","    y_hat = sigmoid(np.dot(X, w) + b)\n","    \n","    dw = (1 / m) * (np.dot(X.T, (y_hat - y)) + lmb * w)\n","    db = (1 / m) * np.sum((y_hat - y))\n","    \n","#     print(f\"dw: {dw}, db: {db}\")\n","    return dw, db"]},{"cell_type":"code","execution_count":5,"metadata":{"trusted":true},"outputs":[],"source":["# @param X - features\n","# @param y - labels\n","# @param bs - batch size\n","# @param epochs - number of iterations through dataset\n","# @param lr - learning rate\n","\n","def train3(X, y, bs, epochs, lr, C):\n","    lmb = 0 if C == 0 else 1/C\n","    \n","    # n - number of training examples, d - number of features\n","    n, d = np.shape(X)\n","    \n","    randomize = np.arange(n)\n","    rng = np.random.default_rng(100)\n","    \n","    w = rng.uniform(size=(d,1))\n","    b = rng.random()\n","    \n","    y = y.reshape(n, 1)\n","    \n","    old_losses = []\n","    old_w = []\n","    old_b = []\n","    \n","    old_w.append(w.copy())\n","    old_b.append(b)\n","    l = loss(y, X, w, b, lmb)\n","    old_losses.append(l)\n","    \n","    for epoch in range(epochs):\n","        limit = n // bs\n","\n","        for i in range(limit):\n","            start = i * bs\n","            end = start + bs\n","            \n","            rng.shuffle(randomize)\n","            choice = randomize[start:end]\n","            X_batch = X[choice]\n","            y_batch = y[choice]\n","            \n","            loss_old = loss(y, X, w, b, lmb)\n","            \n","            dw, db = gradients(y_batch, X_batch, w, b, lmb)\n","            w_new = w.copy() - lr * dw\n","            b_new = b - lr * db\n","            loss_new = loss(y, X, w_new, b_new, lmb)\n","            \n","            w = w_new\n","            b = b_new\n","            old_w.append(w_new)\n","            old_b.append(b_new)\n","            old_losses.append(loss_new)\n","        \n","        min_loss = min(old_losses)\n","        min_index = old_losses.index(min_loss)\n","        w = old_w[min_index]\n","        b = old_b[min_index]\n","        \n","    min_loss = min(old_losses)\n","    min_index = old_losses.index(min_loss)\n","    \n","    return old_w[min_index], old_b[min_index], old_losses"]},{"cell_type":"code","execution_count":6,"metadata":{"trusted":true},"outputs":[],"source":["# @param X - features\n","# @param y - labels\n","# @param bs - batch size\n","# @param epochs - number of iterations through dataset\n","# @param lr - learning rate\n","\n","def train4(X, y, bs, epochs, lr, C, beta_m, beta_v, err):\n","    lmb = 0 if C == 0 else 1/C\n","    \n","    # n - number of training examples, d - number of features\n","    n, d = np.shape(X)\n","    \n","    randomize = np.arange(n)\n","    rng = np.random.default_rng(100)\n","    \n","    w = rng.uniform(size=(d,1))\n","    b = rng.random()\n","    \n","    m_w = np.zeros((d, 1))\n","    m_b = 0\n","    v_w = np.zeros((d, 1))\n","    v_b = 0\n","    \n","    y = y.reshape(n, 1)\n","    \n","    old_losses = []\n","    old_w = []\n","    old_b = []\n","    \n","    old_w.append(w.copy())\n","    old_b.append(b)\n","    l = loss(y, X, w, b, lmb)\n","    old_losses.append(l)\n","    \n","    for epoch in range(epochs):\n","        limit = n // bs\n","\n","        for i in range(limit):\n","            start = i * bs\n","            end = start + bs\n","            \n","            rng.shuffle(randomize)\n","            choice = randomize[start:end]\n","            X_batch = X[choice]\n","            y_batch = y[choice]\n","            \n","            loss_old = loss(y, X, w, b, lmb)\n","            \n","            dw, db = gradients(y_batch, X_batch, w, b, lmb)\n","            \n","            #AdamW\n","            m_w = beta_m * m_w + (1 - beta_m) * dw\n","            m_b = beta_m * m_b + (1 - beta_m) * db\n","            v_w = beta_v * v_w + (1 - beta_v) * np.square(dw)\n","            v_b = beta_v * v_b + (1 - beta_v) * (db**2)\n","            \n","            #bias correction\n","            t = len(old_losses)\n","            m_what = m_w /(1 - beta_m**t)\n","            m_bhat = m_b /(1 - beta_m**t)\n","            v_what = v_w /(1 - beta_v**t)\n","            v_bhat = v_b /(1 - beta_v**t)\n","            \n","            w_new = w.copy() - lr * (m_what/(np.sqrt(v_what) + err) + lmb * w.copy() / bs)\n","            b_new = b - lr * lr * (m_bhat/(np.sqrt(v_bhat) + err))\n","            loss_new = loss(y, X, w_new, b_new, lmb)\n","            \n","            if (loss_new < loss_old):\n","                w = w_new\n","                b = b_new\n","                old_w.append(w_new)\n","                old_b.append(b_new)\n","                old_losses.append(loss_new)\n","    \n","    min_loss = min(old_losses)\n","    min_index = old_losses.index(min_loss)\n","    \n","    return old_w[min_index], old_b[min_index], old_losses"]},{"cell_type":"code","execution_count":7,"metadata":{"trusted":true},"outputs":[],"source":["# @param X - features\n","# @param y - labels\n","# @param bs - batch size\n","# @param epochs - number of iterations through dataset\n","# @param lr - learning rate\n","\n","def train5(X, y, bs, epochs, lr, C, beta_m, beta_v, err):\n","    lmb = 0 if C == 0 else 1/C\n","    \n","    # n - number of training examples, d - number of features\n","    n, d = np.shape(X)\n","    \n","    randomize = np.arange(n)\n","    rng = np.random.default_rng(100)\n","    \n","    w = rng.uniform(size=(d,1))\n","    b = rng.random()\n","    \n","    m_w = np.zeros((d, 1))\n","    m_b = 0\n","    v_w = np.zeros((d, 1))\n","    v_b = 0\n","    \n","    y = y.reshape(n, 1)\n","    \n","    old_losses = []\n","    old_w = []\n","    old_b = []\n","    \n","    old_w.append(w.copy())\n","    old_b.append(b)\n","    l = loss(y, X, w, b, lmb)\n","    old_losses.append(l)\n","    \n","    for epoch in range(epochs):\n","        limit = n // bs\n","\n","        for i in range(limit):\n","            start = i * bs\n","            end = start + bs\n","            \n","            rng.shuffle(randomize)\n","            choice = randomize[start:end]\n","            X_batch = X[choice]\n","            y_batch = y[choice]\n","            \n","            loss_old = loss(y, X, w, b, lmb)\n","            \n","            dw, db = gradients(y_batch, X_batch, w, b, lmb)\n","            \n","            #AdamW\n","            m_w = beta_m * m_w + (1 - beta_m) * dw\n","            m_b = beta_m * m_b + (1 - beta_m) * db\n","            v_w = beta_v * v_w + (1 - beta_v) * np.square(dw)\n","            v_b = beta_v * v_b + (1 - beta_v) * (db**2)\n","            \n","            #bias correction\n","            t = len(old_losses)\n","            m_what = m_w /(1 - beta_m**t)\n","            m_bhat = m_b /(1 - beta_m**t)\n","            v_what = v_w /(1 - beta_v**t)\n","            v_bhat = v_b /(1 - beta_v**t)\n","            \n","            w_new = w.copy() - lr * (m_what/(np.sqrt(v_what) + err) + lmb * w.copy() / bs)\n","            b_new = b - lr * lr * (m_bhat/(np.sqrt(v_bhat) + err))\n","            loss_new = loss(y, X, w_new, b_new, lmb)\n","            \n","            w = w_new\n","            b = b_new\n","            old_w.append(w_new)\n","            old_b.append(b_new)\n","            old_losses.append(loss_new)\n","\n","    min_loss = min(old_losses)\n","    min_index = old_losses.index(min_loss)\n","    \n","    return old_w[min_index], old_b[min_index], old_losses"]},{"cell_type":"code","execution_count":8,"metadata":{"trusted":true},"outputs":[],"source":["# @param X - features\n","# @param y - labels\n","# @param bs - batch size\n","# @param epochs - number of iterations through dataset\n","# @param lr - learning rate\n","\n","def train6(X, y, bs, epochs, lr, C, beta_m, beta_v, err):\n","    lmb = 0 if C == 0 else 1/C\n","    \n","    # n - number of training examples, d - number of features\n","    n, d = np.shape(X)\n","    \n","    randomize = np.arange(n)\n","    rng = np.random.default_rng(100)\n","    \n","    w = rng.uniform(size=(d,1))\n","    b = rng.random()\n","    \n","    m_w = np.zeros((d, 1))\n","    m_b = 0\n","    v_w = np.zeros((d, 1))\n","    v_b = 0\n","    \n","    y = y.reshape(n, 1)\n","    \n","    old_losses = []\n","    old_w = []\n","    old_b = []\n","    \n","    old_w.append(w.copy())\n","    old_b.append(b)\n","    l = loss(y, X, w, b, lmb)\n","    old_losses.append(l)\n","    \n","    for epoch in range(epochs):\n","        limit = n // bs\n","\n","        for i in range(limit):\n","            start = i * bs\n","            end = start + bs\n","            \n","            rng.shuffle(randomize)\n","            choice = randomize[start:end]\n","            X_batch = X[choice]\n","            y_batch = y[choice]\n","            \n","            loss_old = loss(y, X, w, b, lmb)\n","            \n","            dw, db = gradients(y_batch, X_batch, w, b, lmb)\n","            \n","            #AdamW\n","            m_w = beta_m * m_w + (1 - beta_m) * dw\n","            m_b = beta_m * m_b + (1 - beta_m) * db\n","            v_w = beta_v * v_w + (1 - beta_v) * np.square(dw)\n","            v_b = beta_v * v_b + (1 - beta_v) * (db**2)\n","            \n","            #bias correction\n","            t = len(old_losses)\n","            m_what = m_w /(1 - beta_m**t)\n","            m_bhat = m_b /(1 - beta_m**t)\n","            v_what = v_w /(1 - beta_v**t)\n","            v_bhat = v_b /(1 - beta_v**t)\n","            \n","            w_new = w.copy() - lr * (m_what/(np.sqrt(v_what) + err) + lmb * w.copy() / bs)\n","            b_new = b - lr * lr * (m_bhat/(np.sqrt(v_bhat) + err))\n","            loss_new = loss(y, X, w_new, b_new, lmb)\n","            \n","            w = w_new\n","            b = b_new\n","            old_w.append(w_new)\n","            old_b.append(b_new)\n","            old_losses.append(loss_new)\n","        \n","        min_loss = min(old_losses)\n","        min_index = old_losses.index(min_loss)\n","        w = old_w[min_index]\n","        b = old_b[min_index]\n","        \n","    min_loss = min(old_losses)\n","    min_index = old_losses.index(min_loss)\n","    \n","    return old_w[min_index], old_b[min_index], old_losses"]},{"cell_type":"code","execution_count":9,"metadata":{"trusted":true},"outputs":[],"source":["def predict(X, w, b):\n","    y_pred = sigmoid(np.dot(X, w) + b)\n","    pred_labels = np.array([1 if i >= 0.5 else 0 for i in y_pred])\n","    return pred_labels"]},{"cell_type":"code","execution_count":10,"metadata":{"trusted":true},"outputs":[],"source":["def score(y, y_hat):\n","    accuracy = np.sum(y == y_hat) / np.shape(y)[0]\n","    f1score = f1_score(y, y_hat, average='macro')\n","\n","    print(f\"Accuracy: {accuracy}\")\n","    print(f\"Macro-F1 score: {f1score}\")\n","\n","    # Return Macro-F1 score of the model\n","    return f1score"]},{"cell_type":"markdown","metadata":{},"source":["Initialise Training Data for Task 2."]},{"cell_type":"code","execution_count":11,"metadata":{"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>label</th>\n","      <th>0</th>\n","      <th>1</th>\n","      <th>2</th>\n","      <th>3</th>\n","      <th>4</th>\n","      <th>5</th>\n","      <th>6</th>\n","      <th>7</th>\n","      <th>...</th>\n","      <th>4990</th>\n","      <th>4991</th>\n","      <th>4992</th>\n","      <th>4993</th>\n","      <th>4994</th>\n","      <th>4995</th>\n","      <th>4996</th>\n","      <th>4997</th>\n","      <th>4998</th>\n","      <th>4999</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3</td>\n","      <td>1</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>4</td>\n","      <td>0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>5</td>\n","      <td>1</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>17179</th>\n","      <td>17180</td>\n","      <td>0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>17180</th>\n","      <td>17181</td>\n","      <td>0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>17181</th>\n","      <td>17182</td>\n","      <td>1</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>17182</th>\n","      <td>17183</td>\n","      <td>1</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>17183</th>\n","      <td>17184</td>\n","      <td>0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>17184 rows × 5002 columns</p>\n","</div>"],"text/plain":["          id  label    0    1    2    3    4    5    6    7  ...  4990  4991  \\\n","0          1      1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n","1          2      0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n","2          3      1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n","3          4      0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n","4          5      1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n","...      ...    ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   ...   ...   \n","17179  17180      0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n","17180  17181      0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n","17181  17182      1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n","17182  17183      1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n","17183  17184      0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n","\n","       4992  4993  4994  4995  4996  4997  4998  4999  \n","0       0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n","1       0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n","2       0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n","3       0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n","4       0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n","...     ...   ...   ...   ...   ...   ...   ...   ...  \n","17179   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n","17180   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n","17181   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n","17182   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n","17183   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n","\n","[17184 rows x 5002 columns]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["CPU times: total: 19.4 s\n","Wall time: 19.6 s\n"]}],"source":["%%time\n","df_train = pd.read_csv(r\"./source/train_tfidf_features.csv\")\n","display(df_train)"]},{"cell_type":"code","execution_count":12,"metadata":{"cell_id":"9c9a773b88434f6db5fcea744c70d847","deepnote_cell_height":117,"deepnote_cell_type":"code","deepnote_to_be_reexecuted":true,"execution_millis":6,"execution_start":1658689631971,"source_hash":"db08c607","tags":[],"trusted":true},"outputs":[],"source":["# Define features and label\n","df_features = df_train.iloc[:,2:]\n","df_label = df_train.iloc[:, 1]"]},{"cell_type":"code","execution_count":13,"metadata":{"cell_id":"2e1a6b992de1448c8ab6e1b63c8d16af","deepnote_cell_height":616.796875,"deepnote_cell_type":"code","deepnote_to_be_reexecuted":true,"execution_millis":2024,"execution_start":1658688637833,"source_hash":"55cf74dd","tags":[],"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>0</th>\n","      <th>1</th>\n","      <th>2</th>\n","      <th>3</th>\n","      <th>4</th>\n","      <th>5</th>\n","      <th>6</th>\n","      <th>7</th>\n","      <th>8</th>\n","      <th>9</th>\n","      <th>...</th>\n","      <th>4990</th>\n","      <th>4991</th>\n","      <th>4992</th>\n","      <th>4993</th>\n","      <th>4994</th>\n","      <th>4995</th>\n","      <th>4996</th>\n","      <th>4997</th>\n","      <th>4998</th>\n","      <th>4999</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>17179</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>17180</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>17181</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>17182</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>17183</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>17184 rows × 5000 columns</p>\n","</div>"],"text/plain":["         0    1    2    3    4    5    6    7    8    9  ...  4990  4991  \\\n","0      0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n","1      0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n","2      0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n","3      0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n","4      0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n","...    ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   ...   ...   \n","17179  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n","17180  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n","17181  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n","17182  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n","17183  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n","\n","       4992  4993  4994  4995  4996  4997  4998  4999  \n","0       0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n","1       0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n","2       0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n","3       0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n","4       0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n","...     ...   ...   ...   ...   ...   ...   ...   ...  \n","17179   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n","17180   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n","17181   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n","17182   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n","17183   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n","\n","[17184 rows x 5000 columns]"]},"metadata":{},"output_type":"display_data"}],"source":["display(df_features)"]},{"cell_type":"code","execution_count":14,"metadata":{"cell_id":"969eabaf5a124d86887172291e438e29","deepnote_cell_height":600,"deepnote_cell_type":"code","deepnote_output_heights":[232.390625],"deepnote_to_be_reexecuted":true,"execution_millis":26,"execution_start":1658688639858,"source_hash":"ef276ad2","tags":[],"trusted":true},"outputs":[{"data":{"text/plain":["0        1\n","1        0\n","2        1\n","3        0\n","4        1\n","        ..\n","17179    0\n","17180    0\n","17181    1\n","17182    1\n","17183    0\n","Name: label, Length: 17184, dtype: int64"]},"metadata":{},"output_type":"display_data"}],"source":["display(df_label)"]},{"cell_type":"markdown","metadata":{},"source":["## Initializing the Control Model for Evaluation"]},{"cell_type":"markdown","metadata":{},"source":["We will first train the model without dimensionality reduction. We will use this as a control model for evaluation."]},{"cell_type":"code","execution_count":17,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["X_train: (12888, 5000), y_train: (12888,)\n","X_test: (4296, 5000), y_test: (4296,)\n","CPU times: total: 609 ms\n","Wall time: 629 ms\n"]}],"source":["%%time\n","X_train, X_test, y_train, y_test = train_test_split(df_features, df_label, \\\n","test_size=0.25, random_state=100)\n","print(f\"X_train: {X_train.shape}, y_train: {y_train.shape}\")\n","print(f\"X_test: {X_test.shape}, y_test: {y_test.shape}\")"]},{"cell_type":"code","execution_count":18,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy: 0.45763500931098694\n","Macro-F1 score: 0.44500777124850965\n","CPU times: total: 43.5 s\n","Wall time: 7.14 s\n"]},{"data":{"text/plain":["0.44500777124850965"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["%%time\n","knn = KNN(n_neighbors=2)\n","knn.fit(X_train, y_train)\n","score(y_test, knn.predict(X_test))"]},{"cell_type":"markdown","metadata":{},"source":["We see that the KNearestNeighbor Model has resulted in a score of `0.44500` which indicates that KNN is not a good algorithm without dimensionality reduction."]},{"cell_type":"markdown","metadata":{},"source":["## Applying Principal Component Analysis (PCA)"]},{"cell_type":"markdown","metadata":{},"source":["We shall first do a simple evaluation of PCA by using 1000 components after dimensionality reduction. We started with 1000 components as we believed that using 100 components is too small and using 2000 components may result in overfitting. Hence, 1000 components is a good litmus test to evaluate the utility of dimensionality reduction."]},{"cell_type":"code","execution_count":19,"metadata":{"trusted":true},"outputs":[],"source":["n_components = 1000\n","random_state = 100"]},{"cell_type":"code","execution_count":20,"metadata":{"cell_id":"00056-8acc8aa1-45e7-4e0b-8a3c-4562d28d4d7d","deepnote_cell_height":292,"deepnote_cell_type":"code","deepnote_output_heights":[194],"deepnote_to_be_reexecuted":true,"execution_millis":147893,"execution_start":1658687140783,"source_hash":"9be852a0","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["CPU times: total: 3min 23s\n","Wall time: 27.8 s\n"]}],"source":["%%time\n","pca = PCA(n_components=n_components, random_state=random_state)\n","pca.fit(df_features.to_numpy())\n","# print(pca.explained_variance_ratio_)\n","X_pca = pca.transform(df_features.to_numpy())\n","\n","# plt.figure(figsize=(8,6))\n","# plt.scatter(x_pca[:,0], x_3d[:,1], c=y_label) # show plotting of labels visually\n","# plt.show()"]},{"cell_type":"code","execution_count":21,"metadata":{"cell_id":"00057-9ee82dfa-b05d-4471-91b1-da119e05dea0","deepnote_cell_height":484,"deepnote_cell_type":"code","deepnote_to_be_reexecuted":true,"execution_millis":1,"execution_start":1658687288706,"source_hash":"e6cf271d","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Total Variance Explained: 0.647878600154715\n","(17184, 1000)\n","(17184,)\n"]}],"source":["# check before running\n","print(\"Total Variance Explained:\", np.sum(pca.explained_variance_ratio_))\n","# print(X_pca)\n","print(X_pca.shape)\n","print(df_label.shape)"]},{"cell_type":"markdown","metadata":{},"source":["Based on the above, we can see that the top 1000 principal components explain `64.788%` of the variance of the dataset."]},{"cell_type":"code","execution_count":22,"metadata":{"cell_id":"00058-60ad6fff-42a2-4fe9-bba2-6e4d2a9e6a93","deepnote_cell_height":186,"deepnote_cell_type":"code","deepnote_to_be_reexecuted":true,"execution_millis":625,"execution_start":1658688674821,"source_hash":"11567c9b","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["X_train: (12888, 1000), y_train: (12888,)\n","X_test: (4296, 1000), y_test: (4296,)\n","CPU times: total: 62.5 ms\n","Wall time: 64 ms\n"]}],"source":["%%time\n","X_pcatrain, X_pcatest, y_pcatrain, y_pcatest = train_test_split(X_pca, df_label, \\\n","test_size=0.25, random_state=100)\n","print(f\"X_train: {X_pcatrain.shape}, y_train: {y_pcatrain.shape}\")\n","print(f\"X_test: {X_pcatest.shape}, y_test: {y_pcatest.shape}\")"]},{"cell_type":"code","execution_count":23,"metadata":{"cell_id":"60773b9298624ba5b5517c858c3670aa","deepnote_cell_height":251,"deepnote_cell_type":"code","deepnote_output_heights":[null,21.1875],"deepnote_to_be_reexecuted":true,"execution_millis":6962,"execution_start":1658689289370,"source_hash":"d310c324","tags":[],"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy: 0.6338454376163873\n","Macro-F1 score: 0.5734082986431286\n","CPU times: total: 10.2 s\n","Wall time: 1.19 s\n"]},{"data":{"text/plain":["0.5734082986431286"]},"execution_count":23,"metadata":{},"output_type":"execute_result"}],"source":["%%time\n","knn = KNN(n_neighbors=2)\n","knn.fit(X_pcatrain, y_pcatrain)\n","score(y_pcatest, knn.predict(X_pcatest))"]},{"cell_type":"markdown","metadata":{},"source":["Based on the above, the KNN model has observed a significant improvement in performance from `0.44500` to `0.57341`. Hence, there is a 29% improvement in model performance from using Principal Component Analysis. Moreover, the computation of PCA and KNN algorithm occurred less than a minute. Hence, PCA may be a viable technique to employ to improve our model performance."]},{"cell_type":"markdown","metadata":{},"source":["In preparation of our submission, we shall now observe how our model performance and computation time changes as we increase the number of principal components used in our model."]},{"cell_type":"code","execution_count":30,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["n_components = 100\n","0.20904054096604335\n","X_train: (12888, 100), y_train: (12888,)\n","X_test: (4296, 100), y_test: (4296,)\n","Accuracy: 0.6268621973929237\n","Macro-F1 score: 0.449384679410837\n","Model3: 0.449384679410837\n","\n","Accuracy: 0.686219739292365\n","Macro-F1 score: 0.62909110473182\n","Model4: 0.62909110473182\n","\n","Accuracy: 0.6859869646182495\n","Macro-F1 score: 0.629215754080892\n","Model5: 0.629215754080892\n","\n","Accuracy: 0.6859869646182495\n","Macro-F1 score: 0.628896074643623\n","Model6: 0.628896074643623\n","\n","Accuracy: 0.6559590316573557\n","Macro-F1 score: 0.5602725837280811\n","KNN: 0.5602725837280811 \n","\n","Time taken for 100 Principal Components: 40.77728796005249s \n","\n","n_components = 500\n","0.483136414608792\n","X_train: (12888, 500), y_train: (12888,)\n","X_test: (4296, 500), y_test: (4296,)\n","Accuracy: 0.6273277467411545\n","Macro-F1 score: 0.4933758890271646\n","Model3: 0.4933758890271646\n","\n","Accuracy: 0.7164804469273743\n","Macro-F1 score: 0.6767566245232456\n","Model4: 0.6767566245232456\n","\n","Accuracy: 0.7160148975791434\n","Macro-F1 score: 0.6759843704522317\n","Model5: 0.6759843704522317\n","\n","Accuracy: 0.7160148975791434\n","Macro-F1 score: 0.6759843704522317\n","Model6: 0.6759843704522317\n","\n","Accuracy: 0.6531657355679702\n","Macro-F1 score: 0.5511605103373378\n","KNN: 0.5511605103373378 \n","\n","Time taken for 500 Principal Components: 91.41721653938293s \n","\n","n_components = 1000\n","0.647878600154715\n","X_train: (12888, 1000), y_train: (12888,)\n","X_test: (4296, 1000), y_test: (4296,)\n","Accuracy: 0.616852886405959\n","Macro-F1 score: 0.4968542790485735\n","Model3: 0.4968542790485735\n","\n","Accuracy: 0.7190409683426443\n","Macro-F1 score: 0.6834221098977449\n","Model4: 0.6834221098977449\n","\n","Accuracy: 0.7190409683426443\n","Macro-F1 score: 0.6834221098977449\n","Model5: 0.6834221098977449\n","\n","Accuracy: 0.7190409683426443\n","Macro-F1 score: 0.6834221098977449\n","Model6: 0.6834221098977449\n","\n","Accuracy: 0.6338454376163873\n","Macro-F1 score: 0.5734082986431286\n","KNN: 0.5734082986431286 \n","\n","Time taken for 1000 Principal Components: 165.50782799720764s \n","\n","n_components = 1500\n","0.7515722940280163\n","X_train: (12888, 1500), y_train: (12888,)\n","X_test: (4296, 1500), y_test: (4296,)\n","Accuracy: 0.6101024208566108\n","Macro-F1 score: 0.4958661769137143\n","Model3: 0.4958661769137143\n","\n","Accuracy: 0.7213687150837989\n","Macro-F1 score: 0.6869214722857382\n","Model4: 0.6869214722857382\n","\n","Accuracy: 0.7213687150837989\n","Macro-F1 score: 0.6869214722857382\n","Model5: 0.6869214722857382\n","\n","Accuracy: 0.7213687150837989\n","Macro-F1 score: 0.6869214722857382\n","Model6: 0.6869214722857382\n","\n","Accuracy: 0.5793761638733705\n","Macro-F1 score: 0.5626968579620144\n","KNN: 0.5626968579620144 \n","\n","Time taken for 1500 Principal Components: 235.85887789726257s \n","\n","n_components = 2000\n","0.8247068485234131\n","X_train: (12888, 2000), y_train: (12888,)\n","X_test: (4296, 2000), y_test: (4296,)\n","Accuracy: 0.6110335195530726\n","Macro-F1 score: 0.5013253894944569\n","Model3: 0.5013253894944569\n","\n","Accuracy: 0.7241620111731844\n","Macro-F1 score: 0.6914394503956895\n","Model4: 0.6914394503956895\n","\n","Accuracy: 0.7241620111731844\n","Macro-F1 score: 0.6914394503956895\n","Model5: 0.6914394503956895\n","\n","Accuracy: 0.7241620111731844\n","Macro-F1 score: 0.6914394503956895\n","Model6: 0.6914394503956895\n","\n","Accuracy: 0.5174581005586593\n","Macro-F1 score: 0.5169077523878887\n","KNN: 0.5169077523878887 \n","\n","Time taken for 2000 Principal Components: 310.6528036594391s \n","\n","n_components = 2500\n","0.8790757792783456\n","X_train: (12888, 2500), y_train: (12888,)\n","X_test: (4296, 2500), y_test: (4296,)\n","Accuracy: 0.6108007448789572\n","Macro-F1 score: 0.5084915741162457\n","Model3: 0.5084915741162457\n","\n","Accuracy: 0.7269553072625698\n","Macro-F1 score: 0.6942523327996672\n","Model4: 0.6942523327996672\n","\n","Accuracy: 0.7269553072625698\n","Macro-F1 score: 0.6942523327996672\n","Model5: 0.6942523327996672\n","\n","Accuracy: 0.7269553072625698\n","Macro-F1 score: 0.6942523327996672\n","Model6: 0.6942523327996672\n","\n","Accuracy: 0.48463687150837986\n","Macro-F1 score: 0.4829689803479872\n","KNN: 0.4829689803479872 \n","\n","Time taken for 2500 Principal Components: 401.5583200454712s \n","\n","n_components = 3000\n","0.9203410112108971\n","X_train: (12888, 3000), y_train: (12888,)\n","X_test: (4296, 3000), y_test: (4296,)\n","Accuracy: 0.6019553072625698\n","Macro-F1 score: 0.49785914661154584\n","Model3: 0.49785914661154584\n","\n","Accuracy: 0.722998137802607\n","Macro-F1 score: 0.689768295953948\n","Model4: 0.689768295953948\n","\n","Accuracy: 0.722998137802607\n","Macro-F1 score: 0.689768295953948\n","Model5: 0.689768295953948\n","\n","Accuracy: 0.722998137802607\n","Macro-F1 score: 0.689768295953948\n","Model6: 0.689768295953948\n","\n","Accuracy: 0.46997206703910616\n","Macro-F1 score: 0.4635234886906304\n","KNN: 0.4635234886906304 \n","\n","Time taken for 3000 Principal Components: 500.49055099487305s \n","\n","CPU times: total: 2h 37min 26s\n","Wall time: 29min 6s\n"]}],"source":["%%time\n","bs = 2048\n","epochs = 256\n","lr = 0.1\n","C = 0\n","beta_m = 0.6 #No pattern could be clearly determined (classic rec: 0.9)\n","beta_v = 0.999 #(as close to 1) (classic rec: 0.999)\n","err = 0.01 #0.01 (classic rec: 1e-8)\n","\n","comps = [100, 500, 1000, 1500, 2000, 2500, 3000] #added 2 more n_values\n","random_state = 100\n","\n","for n_components in comps:\n","    start = time.time()\n","    pca = PCA(n_components=n_components, random_state=random_state)\n","    pca.fit(df_features.to_numpy())\n","    X_pca = pca.transform(df_features.to_numpy())\n","    \n","    print(f\"n_components = {n_components}\")\n","    print(\"Total Variance Explained:\", np.sum(pca.explained_variance_ratio_))\n","#     print(X_pca)\n","#     print(X_pca.shape)\n","#     print(df_label.shape)\n","\n","    X_pcatrain, X_pcatest, y_pcatrain, y_pcatest = train_test_split(X_pca, df_label, \\\n","                                                                    test_size=0.25, random_state=100)\n","    print(f\"X_train: {X_pcatrain.shape}, y_train: {y_pcatrain.shape}\")\n","    print(f\"X_test: {X_pcatest.shape}, y_test: {y_pcatest.shape}\")\n","    \n","    w, b, l = train3(X_pcatrain, y_pcatrain.values, bs, epochs, lr, C)\n","    Model3 = score(y_pcatest.values,  predict(X_pcatest, w, b))\n","    print(f\"Model3: {Model3}\\n\")\n","    \n","    w, b, l = train4(X_pcatrain, y_pcatrain.values, bs, epochs, lr, C, beta_m, beta_v, err)\n","    Model4 = score(y_pcatest.values,  predict(X_pcatest, w, b))\n","    print(f\"Model4: {Model4}\\n\")\n","    \n","    w, b, l = train5(X_pcatrain, y_pcatrain.values, bs, epochs, lr, C, beta_m, beta_v, err)\n","    Model5 = score(y_pcatest.values,  predict(X_pcatest, w, b))\n","    print(f\"Model5: {Model5}\\n\")\n","    \n","    w, b, l = train6(X_pcatrain, y_pcatrain.values, bs, epochs, lr, C, beta_m, beta_v, err)\n","    Model6 = score(y_pcatest.values,  predict(X_pcatest, w, b))\n","    print(f\"Model6: {Model6}\\n\")\n","    \n","    knn = KNN(n_neighbors=2)\n","    knn.fit(X_pcatrain, y_pcatrain)\n","    knnScore = score(y_pcatest, knn.predict(X_pcatest))\n","    print(f\"KNN: {knnScore} \\n\")\n","    elapsed = time.time() - start\n","    print(f\"Time taken for {n_components} Principal Components: {elapsed}s \\n\")"]},{"cell_type":"markdown","metadata":{},"source":["## Test Set Predictions using PCA\n","\n","As per the grading rubric - \"Perfectly implemented the PCA and KNN. Implemented model is able to run on test sets with reduced components and performed detail analysis of the reduced components\", we shall predict the labels of the Test dataset.\n","\n","We shall first implement a simple pipeline to generate our predictions using 100, 500, 1000 and 2000 components respectively. We will then submit the results to Kaggle and report the scores."]},{"cell_type":"code","execution_count":25,"metadata":{"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>0</th>\n","      <th>1</th>\n","      <th>2</th>\n","      <th>3</th>\n","      <th>4</th>\n","      <th>5</th>\n","      <th>6</th>\n","      <th>7</th>\n","      <th>8</th>\n","      <th>...</th>\n","      <th>4990</th>\n","      <th>4991</th>\n","      <th>4992</th>\n","      <th>4993</th>\n","      <th>4994</th>\n","      <th>4995</th>\n","      <th>4996</th>\n","      <th>4997</th>\n","      <th>4998</th>\n","      <th>4999</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>17185</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>17186</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>17187</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>17188</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>17189</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>4291</th>\n","      <td>21476</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>4292</th>\n","      <td>21477</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>4293</th>\n","      <td>21478</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>4294</th>\n","      <td>21479</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>4295</th>\n","      <td>21480</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>4296 rows × 5001 columns</p>\n","</div>"],"text/plain":["         id    0    1    2    3    4    5    6    7    8  ...  4990  4991  \\\n","0     17185  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n","1     17186  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n","2     17187  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n","3     17188  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n","4     17189  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n","...     ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   ...   ...   \n","4291  21476  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n","4292  21477  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n","4293  21478  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n","4294  21479  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n","4295  21480  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n","\n","      4992  4993  4994  4995  4996  4997  4998  4999  \n","0      0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n","1      0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n","2      0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n","3      0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n","4      0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n","...    ...   ...   ...   ...   ...   ...   ...   ...  \n","4291   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n","4292   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n","4293   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n","4294   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n","4295   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n","\n","[4296 rows x 5001 columns]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["CPU times: total: 4.95 s\n","Wall time: 5.07 s\n"]}],"source":["%%time\n","df_test = pd.read_csv(r\"./source/test_tfidf_features.csv\")\n","display(df_test)"]},{"cell_type":"code","execution_count":31,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["n_components = 100\n","0.20904054096604335\n","(4296, 1) (4296, 1)\n","Time taken for 100 Principal Components: 12.13409686088562s \n","\n","n_components = 500\n","0.483136414608792\n","(4296, 1) (4296, 1)\n","Time taken for 500 Principal Components: 24.379785776138306s \n","\n","n_components = 1000\n","0.647878600154715\n","(4296, 1) (4296, 1)\n","Time taken for 1000 Principal Components: 39.55708885192871s \n","\n","n_components = 2000\n","0.8247068485234131\n","(4296, 1) (4296, 1)\n","Time taken for 2000 Principal Components: 70.41422748565674s \n","\n","CPU times: total: 16min 34s\n","Wall time: 2min 26s\n"]}],"source":["%%time\n","comps = [100, 500, 1000, 2000]\n","random_state = 100\n","\n","for n_components in comps:\n","    start = time.time()\n","    pca = PCA(n_components=n_components, random_state=random_state)\n","    pca.fit(df_features.to_numpy())\n","    X_pca = pca.transform(df_features.to_numpy())\n","    \n","    print(f\"n_components = {n_components}\")\n","    print(\"Total Variance Explained:\", np.sum(pca.explained_variance_ratio_))\n","#     print(X_pca)\n","#     print(X_pca.shape)\n","#     print(df_label.shape)\n","    \n","    knn = KNN(n_neighbors=2)\n","    knn.fit(X_pca, df_label)\n","    \n","    features = df_test.iloc[:,1:]\n","    \n","    X_test = pca.transform(features.to_numpy())\n","    results = knn.predict(X_test)\n","#     display(results)\n","    \n","    df_ids = df_test.iloc[:, 0].to_frame()\n","    df_results = pd.DataFrame(results)\n","    print(df_results.shape, df_ids.shape)\n","    \n","    df_submission = pd.concat([df_ids, df_results], axis =1)\n","    df_submission = df_submission.rename(columns={0: 'label'})\n","    elapsed = time.time() - start\n","    print(f\"Time taken for {n_components} Principal Components: {elapsed}s \\n\")\n","#     display(df_submission)\n","    \n","#     df_submission.to_csv(f\"KNN_Prediction_PCA_{n_components}.csv\", index=False)"]},{"cell_type":"markdown","metadata":{},"source":["Note: Compare Computation Time, F1-Score and Cross Validation Improvement e.g. x% increase in score vs y% increase in computational time (To be removed)"]},{"cell_type":"markdown","metadata":{},"source":["Based on our submissions to Kaggle, we have attained the following scores for the test set:\n","\n","***Macro-F1 Score based on n principal components -***\n","* n = 100: public score = 0.54753\n","* n = 500: public score = 0.57102\n","* n = 1000: public score = 0.57573\n","* n = 2000: public score = 0.49821\n","\n","Based on the above, we can clearly observe that there is an optimal number of principal components to use to obtain the best performing model from PCA. Nonetheless, we feel that PCA is insufficient to boost our model performance. Hence, we shall explore other dimensionality reduction techniques such as TruncatedSVD and KernelPCA. However, we will first explore the use IncrementalPCA to evaluate the boost in computational time from using IncrementalPCA compared to regular PCA and the model performance attained from using it."]},{"cell_type":"markdown","metadata":{},"source":["## Trying other Dimensionality Reduction Methods"]},{"cell_type":"markdown","metadata":{},"source":["### Initializing the Control Model for Evaluation\n","\n","We shall first initialise a control model to evaluate our model from using other Dimensionality Reduction Techniques"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["n_components = 100\n","Total Variance Explained: 0.20904054096604335\n","X_train: (12888, 100), y_train: (12888,)\n","X_test: (4296, 100), y_test: (4296,)\n","Accuracy: 0.6268621973929237\n","Macro-F1 score: 0.449384679410837\n","Model3: 0.449384679410837\n","\n","Accuracy: 0.686219739292365\n","Macro-F1 score: 0.62909110473182\n","Model4: 0.62909110473182\n","\n","Accuracy: 0.6859869646182495\n","Macro-F1 score: 0.629215754080892\n","Model5: 0.629215754080892\n","\n","Accuracy: 0.6859869646182495\n","Macro-F1 score: 0.628896074643623\n","Model6: 0.628896074643623\n","\n","Accuracy: 0.6559590316573557\n","Macro-F1 score: 0.5602725837280811\n","KNN: 0.5602725837280811 \n","\n","Time taken for 100 Principal Components: 40.18019723892212s \n","\n","n_components = 500\n","Total Variance Explained: 0.483136414608792\n","X_train: (12888, 500), y_train: (12888,)\n","X_test: (4296, 500), y_test: (4296,)\n","Accuracy: 0.6273277467411545\n","Macro-F1 score: 0.4933758890271646\n","Model3: 0.4933758890271646\n","\n","Accuracy: 0.7164804469273743\n","Macro-F1 score: 0.6767566245232456\n","Model4: 0.6767566245232456\n","\n","Accuracy: 0.7160148975791434\n","Macro-F1 score: 0.6759843704522317\n","Model5: 0.6759843704522317\n","\n","Accuracy: 0.7160148975791434\n","Macro-F1 score: 0.6759843704522317\n","Model6: 0.6759843704522317\n","\n","Accuracy: 0.6531657355679702\n","Macro-F1 score: 0.5511605103373378\n","KNN: 0.5511605103373378 \n","\n","Time taken for 500 Principal Components: 103.14000916481018s \n","\n","n_components = 1000\n","Total Variance Explained: 0.647878600154715\n","X_train: (12888, 1000), y_train: (12888,)\n","X_test: (4296, 1000), y_test: (4296,)\n","Accuracy: 0.616852886405959\n","Macro-F1 score: 0.4968542790485735\n","Model3: 0.4968542790485735\n","\n","Accuracy: 0.7190409683426443\n","Macro-F1 score: 0.6834221098977449\n","Model4: 0.6834221098977449\n","\n","Accuracy: 0.7190409683426443\n","Macro-F1 score: 0.6834221098977449\n","Model5: 0.6834221098977449\n","\n","Accuracy: 0.7190409683426443\n","Macro-F1 score: 0.6834221098977449\n","Model6: 0.6834221098977449\n","\n","Accuracy: 0.6338454376163873\n","Macro-F1 score: 0.5734082986431286\n","KNN: 0.5734082986431286 \n","\n","Time taken for 1000 Principal Components: 166.6883625984192s \n","\n","n_components = 1500\n","Total Variance Explained: 0.7515722940280163\n","X_train: (12888, 1500), y_train: (12888,)\n","X_test: (4296, 1500), y_test: (4296,)\n","Accuracy: 0.6101024208566108\n","Macro-F1 score: 0.4958661769137143\n","Model3: 0.4958661769137143\n","\n","Accuracy: 0.7213687150837989\n","Macro-F1 score: 0.6869214722857382\n","Model4: 0.6869214722857382\n","\n","Accuracy: 0.7213687150837989\n","Macro-F1 score: 0.6869214722857382\n","Model5: 0.6869214722857382\n","\n","Accuracy: 0.7213687150837989\n","Macro-F1 score: 0.6869214722857382\n","Model6: 0.6869214722857382\n","\n","Accuracy: 0.5793761638733705\n","Macro-F1 score: 0.5626968579620144\n","KNN: 0.5626968579620144 \n","\n","Time taken for 1500 Principal Components: 236.03642630577087s \n","\n","n_components = 2000\n","Total Variance Explained: 0.8247068485234131\n","X_train: (12888, 2000), y_train: (12888,)\n","X_test: (4296, 2000), y_test: (4296,)\n","Accuracy: 0.6110335195530726\n","Macro-F1 score: 0.5013253894944569\n","Model3: 0.5013253894944569\n","\n","Accuracy: 0.7241620111731844\n","Macro-F1 score: 0.6914394503956895\n","Model4: 0.6914394503956895\n","\n","Accuracy: 0.7241620111731844\n","Macro-F1 score: 0.6914394503956895\n","Model5: 0.6914394503956895\n","\n","Accuracy: 0.7241620111731844\n","Macro-F1 score: 0.6914394503956895\n","Model6: 0.6914394503956895\n","\n","Accuracy: 0.5174581005586593\n","Macro-F1 score: 0.5169077523878887\n","KNN: 0.5169077523878887 \n","\n","Time taken for 2000 Principal Components: 323.81342601776123s \n","\n","CPU times: total: 1h 13min 13s\n","Wall time: 14min 29s\n"]}],"source":["%%time\n","bs = 2048\n","epochs = 256\n","lr = 0.1\n","C = 0\n","beta_m = 0.6 #No pattern could be clearly determined (classic rec: 0.9)\n","beta_v = 0.999 #(as close to 1) (classic rec: 0.999)\n","err = 0.01 #0.01 (classic rec: 1e-8)\n","\n","comps = [100, 500, 1000, 1500, 2000] #added 2 more n_values\n","random_state = 100\n","\n","for n_components in comps:\n","    start = time.time()\n","    pca = PCA(n_components=n_components, random_state=random_state)\n","    pca.fit(df_features.to_numpy())\n","    X_pca = pca.transform(df_features.to_numpy())\n","    \n","    print(f\"n_components = {n_components}\")\n","    print(\"Total Variance Explained:\", np.sum(pca.explained_variance_ratio_))\n","#     print(X_pca)\n","#     print(X_pca.shape)\n","#     print(df_label.shape)\n","\n","    X_pcatrain, X_pcatest, y_pcatrain, y_pcatest = train_test_split(X_pca, df_label, \\\n","                                                                    test_size=0.25, random_state=100)\n","    print(f\"X_train: {X_pcatrain.shape}, y_train: {y_pcatrain.shape}\")\n","    print(f\"X_test: {X_pcatest.shape}, y_test: {y_pcatest.shape}\")\n","    \n","    w, b, l = train3(X_pcatrain, y_pcatrain.values, bs, epochs, lr, C)\n","    Model3 = score(y_pcatest.values,  predict(X_pcatest, w, b))\n","    print(f\"Model3: {Model3}\\n\")\n","    \n","    w, b, l = train4(X_pcatrain, y_pcatrain.values, bs, epochs, lr, C, beta_m, beta_v, err)\n","    Model4 = score(y_pcatest.values,  predict(X_pcatest, w, b))\n","    print(f\"Model4: {Model4}\\n\")\n","    \n","    w, b, l = train5(X_pcatrain, y_pcatrain.values, bs, epochs, lr, C, beta_m, beta_v, err)\n","    Model5 = score(y_pcatest.values,  predict(X_pcatest, w, b))\n","    print(f\"Model5: {Model5}\\n\")\n","    \n","    w, b, l = train6(X_pcatrain, y_pcatrain.values, bs, epochs, lr, C, beta_m, beta_v, err)\n","    Model6 = score(y_pcatest.values,  predict(X_pcatest, w, b))\n","    print(f\"Model6: {Model6}\\n\")\n","    \n","    knn = KNN(n_neighbors=2)\n","    knn.fit(X_pcatrain, y_pcatrain)\n","    knnScore = score(y_pcatest, knn.predict(X_pcatest))\n","    print(f\"KNN: {knnScore} \\n\")\n","    elapsed = time.time() - start\n","    print(f\"Time taken for {n_components} Principal Components: {elapsed}s \\n\")"]},{"cell_type":"markdown","metadata":{},"source":["### IncrementalPCA\n","\n","We will first implement IncrementalPCA to compare the computational time compared to regular PCA. We will evaluate the performance in terms of computational time and model performance. We shall first test the function to observe the computational time to generate 1000 principal components using IncrementalPCA."]},{"cell_type":"code","execution_count":16,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["n_components=1000, batch_size=2000\n"]}],"source":["n_components = 1000\n","batch_size = 2*n_components # must be greater than n_components\n","\n","print(f\"n_components={n_components}, batch_size={batch_size}\")"]},{"cell_type":"code","execution_count":18,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["CPU times: total: 24min 47s\n","Wall time: 4min 3s\n"]}],"source":["%%time\n","increment = IncrementalPCA(n_components=n_components, batch_size=batch_size)\n","increment.fit(df_features.to_numpy())\n","X_increment = increment.transform(df_features.to_numpy())"]},{"cell_type":"code","execution_count":19,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Total Variance Explained: 0.8247068485234131\n","(17184, 1000)\n","(17184,)\n"]}],"source":["# check before running\n","print(\"Total Variance Explained:\", np.sum(pca.explained_variance_ratio_))\n","# print(X_increment)\n","print(X_increment.shape)\n","print(df_label.shape)"]},{"cell_type":"code","execution_count":20,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["X_train: (12888, 1000), y_train: (12888,)\n","X_test: (4296, 1000), y_test: (4296,)\n","CPU times: total: 46.9 ms\n","Wall time: 52 ms\n"]}],"source":["%%time\n","X_incrementtrain, X_incrementtest, y_incrementtrain, y_incrementtest = train_test_split(X_increment, df_label, test_size=0.25, random_state=100)\n","print(f\"X_train: {X_incrementtrain.shape}, y_train: {y_incrementtrain.shape}\")\n","print(f\"X_test: {X_incrementtest.shape}, y_test: {y_incrementtest.shape}\")"]},{"cell_type":"code","execution_count":21,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy: 0.7195065176908753\n","Macro-F1 score: 0.6847195618177574\n","CPU times: total: 2min 32s\n","Wall time: 32.6 s\n"]},{"data":{"text/plain":["0.6847195618177574"]},"execution_count":21,"metadata":{},"output_type":"execute_result"}],"source":["%%time\n","bs = 2048\n","epochs = 256\n","lr = 0.1\n","C = 0\n","beta_m = 0.6 #No pattern could be clearly determined (classic rec: 0.9)\n","beta_v = 0.999 #(as close to 1) (classic rec: 0.999)\n","err = 0.01 #0.01 (classic rec: 1e-8)\n","\n","w, b, l = train6(X_incrementtrain, y_incrementtrain.values, bs, epochs, lr, C, beta_m, beta_v, err)\n","score(y_incrementtest.values,  predict(X_incrementtest, w, b))"]},{"cell_type":"markdown","metadata":{},"source":["Based on the above, we were able to achieve a model performance of `0.68472` from IncrementalPCA (n=1000), which is comparable to a model performance of `0.68342` from PCA (n=1000). We shall now compare computational time from running the IncrementalPCA."]},{"cell_type":"code","execution_count":22,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["n_components = 100, batch_size = 4000\n","Total Variance Explained: 0.20455528952164242\n","X_train: (12888, 100), y_train: (12888,)\n","X_test: (4296, 100), y_test: (4296,)\n","Accuracy: 0.6343109869646183\n","Macro-F1 score: 0.4624685846725459\n","Model3: 0.4624685846725459\n","\n","Accuracy: 0.6908752327746741\n","Macro-F1 score: 0.6369257086999023\n","Model4: 0.6369257086999023\n","\n","Accuracy: 0.6911080074487895\n","Macro-F1 score: 0.6380355527962599\n","Model5: 0.6380355527962599\n","\n","Accuracy: 0.6913407821229051\n","Macro-F1 score: 0.6377775313432503\n","Model6: 0.6377775313432503\n","\n","Accuracy: 0.6613128491620112\n","Macro-F1 score: 0.5686668113488322\n","KNN: 0.5686668113488322 \n","\n","Time taken for 100 Principal Components: 239.91583132743835s \n","\n","n_components = 500, batch_size = 4000\n","Total Variance Explained: 0.4750607106451144\n","X_train: (12888, 500), y_train: (12888,)\n","X_test: (4296, 500), y_test: (4296,)\n","Accuracy: 0.6252327746741154\n","Macro-F1 score: 0.4903620199836186\n","Model3: 0.4903620199836186\n","\n","Accuracy: 0.714851024208566\n","Macro-F1 score: 0.6759182424233281\n","Model4: 0.6759182424233281\n","\n","Accuracy: 0.715316573556797\n","Macro-F1 score: 0.6762095149269713\n","Model5: 0.6762095149269713\n","\n","Accuracy: 0.715316573556797\n","Macro-F1 score: 0.6762095149269713\n","Model6: 0.6762095149269713\n","\n","Accuracy: 0.654096834264432\n","Macro-F1 score: 0.5528786189352892\n","KNN: 0.5528786189352892 \n","\n","Time taken for 500 Principal Components: 320.3189721107483s \n","\n","n_components = 1000, batch_size = 4000\n","Total Variance Explained: 0.6365485970168069\n","X_train: (12888, 1000), y_train: (12888,)\n","X_test: (4296, 1000), y_test: (4296,)\n","Accuracy: 0.6212756052141527\n","Macro-F1 score: 0.5031069005987306\n","Model3: 0.5031069005987306\n","\n","Accuracy: 0.7188081936685289\n","Macro-F1 score: 0.6849681226762188\n","Model4: 0.6849681226762188\n","\n","Accuracy: 0.7188081936685289\n","Macro-F1 score: 0.6849681226762188\n","Model5: 0.6849681226762188\n","\n","Accuracy: 0.7188081936685289\n","Macro-F1 score: 0.6849681226762188\n","Model6: 0.6849681226762188\n","\n","Accuracy: 0.6291899441340782\n","Macro-F1 score: 0.5672758840014516\n","KNN: 0.5672758840014516 \n","\n","Time taken for 1000 Principal Components: 412.80407547950745s \n","\n","n_components = 1500, batch_size = 4000\n","Total Variance Explained: 0.7401017867402803\n","X_train: (12888, 1500), y_train: (12888,)\n","X_test: (4296, 1500), y_test: (4296,)\n","Accuracy: 0.6145251396648045\n","Macro-F1 score: 0.5053861104954276\n","Model3: 0.5053861104954276\n","\n","Accuracy: 0.7241620111731844\n","Macro-F1 score: 0.6907014118132113\n","Model4: 0.6907014118132113\n","\n","Accuracy: 0.7241620111731844\n","Macro-F1 score: 0.6907014118132113\n","Model5: 0.6907014118132113\n","\n","Accuracy: 0.7241620111731844\n","Macro-F1 score: 0.6907014118132113\n","Model6: 0.6907014118132113\n","\n","Accuracy: 0.5735567970204841\n","Macro-F1 score: 0.5548794834020465\n","KNN: 0.5548794834020465 \n","\n","Time taken for 1500 Principal Components: 505.742333650589s \n","\n","n_components = 2000, batch_size = 4000\n","Total Variance Explained: 0.8148522103653635\n","X_train: (12888, 2000), y_train: (12888,)\n","X_test: (4296, 2000), y_test: (4296,)\n","Accuracy: 0.6087057728119181\n","Macro-F1 score: 0.5011204723869305\n","Model3: 0.5011204723869305\n","\n","Accuracy: 0.7248603351955307\n","Macro-F1 score: 0.6925846497192871\n","Model4: 0.6925846497192871\n","\n","Accuracy: 0.7248603351955307\n","Macro-F1 score: 0.6925846497192871\n","Model5: 0.6925846497192871\n","\n","Accuracy: 0.7248603351955307\n","Macro-F1 score: 0.6925846497192871\n","Model6: 0.6925846497192871\n","\n","Accuracy: 0.5235102420856611\n","Macro-F1 score: 0.5228566382542694\n","KNN: 0.5228566382542694 \n","\n","Time taken for 2000 Principal Components: 596.8671686649323s \n","\n","CPU times: total: 3h 22min 46s\n","Wall time: 34min 35s\n"]}],"source":["%%time\n","bs = 2048\n","epochs = 256\n","lr = 0.1\n","C = 0\n","beta_m = 0.6 #No pattern could be clearly determined (classic rec: 0.9)\n","beta_v = 0.999 #(as close to 1) (classic rec: 0.999)\n","err = 0.01 #0.01 (classic rec: 1e-8)\n","\n","comps = [100, 500, 1000, 1500, 2000]\n","\n","for n_components in comps:\n","    start = time.time()\n","    batch_size = 4000\n","    pca = IncrementalPCA(n_components=n_components, batch_size=batch_size)\n","    pca.fit(df_features.to_numpy())\n","    X_pca = pca.transform(df_features.to_numpy())\n","    \n","    print(f\"n_components = {n_components}, batch_size = {batch_size}\")\n","    print(\"Total Variance Explained:\", np.sum(pca.explained_variance_ratio_))\n","#     print(X_pca)\n","#     print(X_pca.shape)\n","#     print(df_label.shape)\n","\n","    X_pcatrain, X_pcatest, y_pcatrain, y_pcatest = train_test_split(X_pca, df_label, \\\n","                                                                    test_size=0.25, random_state=100)\n","    print(f\"X_train: {X_pcatrain.shape}, y_train: {y_pcatrain.shape}\")\n","    print(f\"X_test: {X_pcatest.shape}, y_test: {y_pcatest.shape}\")\n","    \n","    w, b, l = train3(X_pcatrain, y_pcatrain.values, bs, epochs, lr, C)\n","    Model3 = score(y_pcatest.values,  predict(X_pcatest, w, b))\n","    print(f\"Model3: {Model3}\\n\")\n","    \n","    w, b, l = train4(X_pcatrain, y_pcatrain.values, bs, epochs, lr, C, beta_m, beta_v, err)\n","    Model4 = score(y_pcatest.values,  predict(X_pcatest, w, b))\n","    print(f\"Model4: {Model4}\\n\")\n","    \n","    w, b, l = train5(X_pcatrain, y_pcatrain.values, bs, epochs, lr, C, beta_m, beta_v, err)\n","    Model5 = score(y_pcatest.values,  predict(X_pcatest, w, b))\n","    print(f\"Model5: {Model5}\\n\")\n","    \n","    w, b, l = train6(X_pcatrain, y_pcatrain.values, bs, epochs, lr, C, beta_m, beta_v, err)\n","    Model6 = score(y_pcatest.values,  predict(X_pcatest, w, b))\n","    print(f\"Model6: {Model6}\\n\")\n","    \n","    knn = KNN(n_neighbors=2)\n","    knn.fit(X_pcatrain, y_pcatrain)\n","    knnScore = score(y_pcatest, knn.predict(X_pcatest))\n","    print(f\"KNN: {knnScore} \\n\")\n","    elapsed = time.time() - start\n","    print(f\"Time taken for {n_components} Principal Components: {elapsed}s \\n\")"]},{"cell_type":"markdown","metadata":{},"source":[]},{"cell_type":"markdown","metadata":{},"source":["### TruncatedSVD\n","\n","We will now implement TruncatedSVD to compare the computational time compared to regular PCA. We will evaluate the performance in terms of computational time and model performance. We shall first test the function to observe the computational time to generate 1000 principal components using TruncatedSVD."]},{"cell_type":"code","execution_count":23,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["n_components=1000, random_state=100\n"]}],"source":["n_components = 1000\n","random_state = 100\n","\n","print(f\"n_components={n_components}, random_state={random_state}\")"]},{"cell_type":"code","execution_count":24,"metadata":{"cell_id":"65b25c6d510f4ecfa805b96d6de5ffd0","deepnote_cell_height":166,"deepnote_cell_type":"code","deepnote_to_be_reexecuted":true,"execution_millis":187050,"execution_start":1658687820350,"source_hash":"db4df06f","tags":[],"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["CPU times: total: 4min 57s\n","Wall time: 45.3 s\n"]}],"source":["%%time\n","svd = TruncatedSVD(n_components=n_components, random_state=random_state)\n","svd.fit(df_features.to_numpy())\n","X_svd = svd.transform(df_features.to_numpy())"]},{"cell_type":"code","execution_count":25,"metadata":{"cell_id":"9d2894b2c7f540b1b173e4b8f26a79cf","deepnote_cell_height":484,"deepnote_cell_type":"code","deepnote_to_be_reexecuted":true,"execution_millis":30,"execution_start":1658688007408,"source_hash":"fbc03bf3","tags":[],"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Total Variance Explained: 0.650073164448684\n","(17184, 1000)\n","(17184,)\n"]}],"source":["# check before running\n","print(\"Total Variance Explained:\", np.sum(svd.explained_variance_ratio_))\n","# print(X_svd)\n","print(X_svd.shape)\n","print(df_label.shape)"]},{"cell_type":"code","execution_count":26,"metadata":{"cell_id":"bb4aac5d0fc7440f8216a9ac85999898","deepnote_cell_height":186,"deepnote_cell_type":"code","deepnote_to_be_reexecuted":true,"execution_millis":122,"execution_start":1658689301898,"source_hash":"8dafaa66","tags":[],"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["X_train: (12888, 1000), y_train: (12888,)\n","X_test: (4296, 1000), y_test: (4296,)\n","CPU times: total: 62.5 ms\n","Wall time: 60 ms\n"]}],"source":["%%time\n","X_svdtrain, X_svdtest, y_svdtrain, y_svdtest = train_test_split(X_svd, df_label, test_size=0.25, random_state=100)\n","print(f\"X_train: {X_svdtrain.shape}, y_train: {y_svdtrain.shape}\")\n","print(f\"X_test: {X_svdtest.shape}, y_test: {y_svdtest.shape}\")"]},{"cell_type":"code","execution_count":27,"metadata":{"cell_id":"063c67f915ab44e49b397ab3e1bb8e5c","deepnote_cell_height":418.1875,"deepnote_cell_type":"code","deepnote_output_heights":[null,21.1875],"deepnote_to_be_reexecuted":true,"execution_millis":98527,"execution_start":1658689410287,"source_hash":"c8d685df","tags":[],"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy: 0.7190409683426443\n","Macro-F1 score: 0.6836444589405164\n","CPU times: total: 2min 31s\n","Wall time: 40.6 s\n"]},{"data":{"text/plain":["0.6836444589405164"]},"execution_count":27,"metadata":{},"output_type":"execute_result"}],"source":["%%time\n","bs = 2048\n","epochs = 256\n","lr = 0.1\n","C = 0\n","beta_m = 0.6 #No pattern could be clearly determined (classic rec: 0.9)\n","beta_v = 0.999 #(as close to 1) (classic rec: 0.999)\n","err = 0.01 #0.01 (classic rec: 1e-8)\n","\n","w, b, l = train6(X_svdtrain, y_svdtrain.values, bs, epochs, lr, C, beta_m, beta_v, err)\n","score(y_svdtest.values,  predict(X_svdtest, w, b))"]},{"cell_type":"markdown","metadata":{},"source":[]},{"cell_type":"code","execution_count":28,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["n_components = 100, batch_size = 4000\n","Total Variance Explained: 0.20770761680685512\n","X_train: (12888, 100), y_train: (12888,)\n","X_test: (4296, 100), y_test: (4296,)\n","Accuracy: 0.633147113594041\n","Macro-F1 score: 0.4597114735469538\n","Model3: 0.4597114735469538\n","\n","Accuracy: 0.688780260707635\n","Macro-F1 score: 0.631559639336222\n","Model4: 0.631559639336222\n","\n","Accuracy: 0.6890130353817505\n","Macro-F1 score: 0.633025514433183\n","Model5: 0.633025514433183\n","\n","Accuracy: 0.6894785847299814\n","Macro-F1 score: 0.6329432742672462\n","Model6: 0.6329432742672462\n","\n","Accuracy: 0.6585195530726257\n","Macro-F1 score: 0.565109424225936\n","KNN: 0.565109424225936 \n","\n","Time taken for 100 Principal Components: 38.85954546928406s \n","\n","n_components = 500, batch_size = 4000\n","Total Variance Explained: 0.48528147090419305\n","X_train: (12888, 500), y_train: (12888,)\n","X_test: (4296, 500), y_test: (4296,)\n","Accuracy: 0.6135940409683427\n","Macro-F1 score: 0.4738487009566307\n","Model3: 0.4738487009566307\n","\n"]}],"source":["%%time\n","bs = 2048\n","epochs = 256\n","lr = 0.1\n","C = 0\n","beta_m = 0.6 #No pattern could be clearly determined (classic rec: 0.9)\n","beta_v = 0.999 #(as close to 1) (classic rec: 0.999)\n","err = 0.01 #0.01 (classic rec: 1e-8)\n","\n","comps = [100, 500, 1000, 1500, 2000]\n","\n","for n_components in comps:\n","    start = time.time()\n","    pca = TruncatedSVD(n_components=n_components, random_state=random_state)\n","    pca.fit(df_features.to_numpy())\n","    X_pca = pca.transform(df_features.to_numpy())\n","    \n","    print(f\"n_components = {n_components}, batch_size = {batch_size}\")\n","    print(\"Total Variance Explained:\", np.sum(pca.explained_variance_ratio_))\n","#     print(X_pca)\n","#     print(X_pca.shape)\n","#     print(df_label.shape)\n","\n","    X_pcatrain, X_pcatest, y_pcatrain, y_pcatest = train_test_split(X_pca, df_label, \\\n","                                                                    test_size=0.25, random_state=100)\n","    print(f\"X_train: {X_pcatrain.shape}, y_train: {y_pcatrain.shape}\")\n","    print(f\"X_test: {X_pcatest.shape}, y_test: {y_pcatest.shape}\")\n","    \n","    w, b, l = train3(X_pcatrain, y_pcatrain.values, bs, epochs, lr, C)\n","    Model3 = score(y_pcatest.values,  predict(X_pcatest, w, b))\n","    print(f\"Model3: {Model3}\\n\")\n","    \n","    w, b, l = train4(X_pcatrain, y_pcatrain.values, bs, epochs, lr, C, beta_m, beta_v, err)\n","    Model4 = score(y_pcatest.values,  predict(X_pcatest, w, b))\n","    print(f\"Model4: {Model4}\\n\")\n","    \n","    w, b, l = train5(X_pcatrain, y_pcatrain.values, bs, epochs, lr, C, beta_m, beta_v, err)\n","    Model5 = score(y_pcatest.values,  predict(X_pcatest, w, b))\n","    print(f\"Model5: {Model5}\\n\")\n","    \n","    w, b, l = train6(X_pcatrain, y_pcatrain.values, bs, epochs, lr, C, beta_m, beta_v, err)\n","    Model6 = score(y_pcatest.values,  predict(X_pcatest, w, b))\n","    print(f\"Model6: {Model6}\\n\")\n","    \n","    knn = KNN(n_neighbors=2)\n","    knn.fit(X_pcatrain, y_pcatrain)\n","    knnScore = score(y_pcatest, knn.predict(X_pcatest))\n","    print(f\"KNN: {knnScore} \\n\")\n","    elapsed = time.time() - start\n","    print(f\"Time taken for {n_components} Principal Components: {elapsed}s \\n\")"]},{"cell_type":"markdown","metadata":{},"source":[]},{"cell_type":"markdown","metadata":{},"source":["### KernelPCA"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["n_components=1000, random_state=100, kernel=cosine, n_jobs=-1\n"]}],"source":["n_components=1000\n","kernel=\"linear\"\n","n_jobs=-1\n","random_state=100\n","print(f\"n_components={n_components}, random_state={random_state}, kernel={kernel}, n_jobs={n_jobs}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["CPU times: total: 1h 8min 13s\n","Wall time: 10min 59s\n"]}],"source":["%%time\n","kern = KernelPCA(n_components=n_components, kernel=kernel, n_jobs=n_jobs, random_state=random_state)\n","kern.fit(df_features.to_numpy())\n","X_kernel = kern.transform(df_features.to_numpy())"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import matplotlib.pyplot as plt\n","\n","var_values = kern.eigenvalues_ / sum(kern.eigenvalues_)\n","print(sum(var_values))\n","plt.plot(np.arange(1, kern.n_components + 1), var_values, \"+\", linewidth=2)\n","plt.ylabel(\"PCA explained variance ratio\")\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["[[-0.08319779 -0.01604783 -0.0105949  ...  0.00179469 -0.01203666\n","  -0.01116068]\n"," [-0.06842061 -0.04364866 -0.01844324 ...  0.00293388  0.01088659\n","  -0.02332199]\n"," [-0.08017058 -0.04464213 -0.01534231 ...  0.00314898 -0.00193481\n","   0.00612507]\n"," ...\n"," [ 0.00207858 -0.04178633 -0.01688574 ...  0.01654121  0.00395149\n","   0.00196094]\n"," [ 0.09135488 -0.05590327 -0.00847336 ...  0.00097483  0.00756974\n","   0.00577514]\n"," [-0.04061166  0.09370788 -0.03998554 ... -0.00583051 -0.012944\n","   0.03945767]]\n","(17184, 1000)\n","(17184,)\n"]}],"source":["# check before running\n","# print(X_kernel)\n","print(X_kernel.shape)\n","print(df_label.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["X_train: (12888, 1000), y_train: (12888,)\n","X_test: (4296, 1000), y_test: (4296,)\n","CPU times: total: 62.5 ms\n","Wall time: 50 ms\n"]}],"source":["%%time\n","X_kerneltrain, X_kerneltest, y_kerneltrain, y_kerneltest = train_test_split(X_kernel, df_label, test_size=0.25, random_state=100)\n","print(f\"X_train: {X_kerneltrain.shape}, y_train: {y_kerneltrain.shape}\")\n","print(f\"X_test: {X_kerneltest.shape}, y_test: {y_kerneltest.shape}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy: 0.7199720670391061\n","Macro-F1 score: 0.686003205410374\n","CPU times: total: 2min 15s\n","Wall time: 23.6 s\n"]},{"data":{"text/plain":["0.686003205410374"]},"execution_count":133,"metadata":{},"output_type":"execute_result"}],"source":["%%time\n","bs = 2048\n","epochs = 256\n","lr = 0.1\n","C = 0\n","beta_m = 0.6 #No pattern could be clearly determined (classic rec: 0.9)\n","beta_v = 0.999 #(as close to 1) (classic rec: 0.999)\n","err = 0.01 #0.01 (classic rec: 1e-8)\n","\n","w, b, l = train6(X_kerneltrain, y_kerneltrain.values, bs, epochs, lr, C, beta_m, beta_v, err)\n","score(y_kerneltest.values,  predict(X_kerneltest, w, b))"]},{"cell_type":"markdown","metadata":{},"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["n_components = 100\n","X_train: (12888, 100), y_train: (12888,)\n","X_test: (4296, 100), y_test: (4296,)\n","Accuracy: 0.6291899441340782\n","Macro-F1 score: 0.4591094845338825\n","Model3: 0.4591094845338825\n","\n","Accuracy: 0.6843575418994413\n","Macro-F1 score: 0.6283270398647594\n","Model4: 0.6283270398647594\n","\n","Accuracy: 0.6838919925512105\n","Macro-F1 score: 0.6274612976525208\n","Model5: 0.6274612976525208\n","\n","Accuracy: 0.6848230912476723\n","Macro-F1 score: 0.629190550988352\n","Model6: 0.629190550988352\n","\n","Accuracy: 0.6552607076350093\n","Macro-F1 score: 0.5604755619304248\n","KNN: 0.5604755619304248\n","\n","n_components = 500\n","X_train: (12888, 500), y_train: (12888,)\n","X_test: (4296, 500), y_test: (4296,)\n","Accuracy: 0.625465549348231\n","Macro-F1 score: 0.4881696700131085\n","Model3: 0.4881696700131085\n","\n","Accuracy: 0.7176443202979516\n","Macro-F1 score: 0.6789750953871346\n","Model4: 0.6789750953871346\n","\n","Accuracy: 0.7169459962756052\n","Macro-F1 score: 0.6780034986112806\n","Model5: 0.6780034986112806\n","\n","Accuracy: 0.7169459962756052\n","Macro-F1 score: 0.6780034986112806\n","Model6: 0.6780034986112806\n","\n","Accuracy: 0.6524674115456238\n","Macro-F1 score: 0.5456504896637868\n","KNN: 0.5456504896637868\n","\n","n_components = 1000\n","X_train: (12888, 1000), y_train: (12888,)\n","X_test: (4296, 1000), y_test: (4296,)\n","Accuracy: 0.6180167597765364\n","Macro-F1 score: 0.499724057325964\n","Model3: 0.499724057325964\n","\n","Accuracy: 0.7199720670391061\n","Macro-F1 score: 0.686003205410374\n","Model4: 0.686003205410374\n","\n","Accuracy: 0.7199720670391061\n","Macro-F1 score: 0.686003205410374\n","Model5: 0.686003205410374\n","\n","Accuracy: 0.7199720670391061\n","Macro-F1 score: 0.686003205410374\n","Model6: 0.686003205410374\n","\n","Accuracy: 0.6431564245810056\n","Macro-F1 score: 0.5741824982582959\n","KNN: 0.5741824982582959\n","\n","CPU times: total: 3h 16min 10s\n","Wall time: 26min 26s\n"]}],"source":["%%time\n","bs = 2048\n","epochs = 256\n","lr = 0.1\n","C = 0\n","beta_m = 0.6 #No pattern could be clearly determined (classic rec: 0.9)\n","beta_v = 0.999 #(as close to 1) (classic rec: 0.999)\n","err = 0.01 #0.01 (classic rec: 1e-8)\n","\n","comps = [100, 500, 1000, 1500, 2000]\n","kernel = \"linear\"\n","\n","for n_components in comps:\n","    start = time.time()\n","    pca = KernelPCA(n_components=n_components, kernel=kernel, n_jobs=n_jobs, random_state=random_state)\n","    pca.fit(df_features.to_numpy())\n","    X_pca = pca.transform(df_features.to_numpy())\n","    \n","    print(f\"n_components = {n_components}\")\n","#     print(X_pca)\n","#     print(X_pca.shape)\n","#     print(df_label.shape)\n","\n","    X_pcatrain, X_pcatest, y_pcatrain, y_pcatest = train_test_split(X_pca, df_label, \\\n","                                                                    test_size=0.25, random_state=100)\n","    print(f\"X_train: {X_pcatrain.shape}, y_train: {y_pcatrain.shape}\")\n","    print(f\"X_test: {X_pcatest.shape}, y_test: {y_pcatest.shape}\")\n","    \n","    w, b, l = train3(X_pcatrain, y_pcatrain.values, bs, epochs, lr, C)\n","    Model3 = score(y_pcatest.values,  predict(X_pcatest, w, b))\n","    print(f\"Model3: {Model3}\\n\")\n","    \n","    w, b, l = train4(X_pcatrain, y_pcatrain.values, bs, epochs, lr, C, beta_m, beta_v, err)\n","    Model4 = score(y_pcatest.values,  predict(X_pcatest, w, b))\n","    print(f\"Model4: {Model4}\\n\")\n","    \n","    w, b, l = train5(X_pcatrain, y_pcatrain.values, bs, epochs, lr, C, beta_m, beta_v, err)\n","    Model5 = score(y_pcatest.values,  predict(X_pcatest, w, b))\n","    print(f\"Model5: {Model5}\\n\")\n","    \n","    w, b, l = train6(X_pcatrain, y_pcatrain.values, bs, epochs, lr, C, beta_m, beta_v, err)\n","    Model6 = score(y_pcatest.values,  predict(X_pcatest, w, b))\n","    print(f\"Model6: {Model6}\\n\")\n","    \n","    knn = KNN(n_neighbors=2)\n","    knn.fit(X_pcatrain, y_pcatrain)\n","    knnScore = score(y_pcatest, knn.predict(X_pcatest))\n","    print(f\"KNN: {knnScore} \\n\")\n","    elapsed = time.time() - start\n","    print(f\"Time taken for {n_components} Principal Components: {elapsed}s \\n\")"]},{"cell_type":"markdown","metadata":{},"source":[]},{"cell_type":"markdown","metadata":{},"source":["We will now try different kernels and evaluate the model performance. We will use 500 components instead as it is computationally expensive to transform 5000 features using the KernelPCA using different kernels."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["n_components=500, random_state=100, kernels=['linear', 'poly', 'rbf', 'sigmoid', 'cosine'], n_jobs=-1\n","kernel = linear\n","X_train: (12888, 500), y_train: (12888,)\n","X_test: (4296, 500), y_test: (4296,)\n","Accuracy: 0.625465549348231\n","Macro-F1 score: 0.4881696700131085\n","Model3: 0.4881696700131085\n","\n","Accuracy: 0.7176443202979516\n","Macro-F1 score: 0.6789750953871346\n","Model4: 0.6789750953871346\n","\n","Accuracy: 0.7169459962756052\n","Macro-F1 score: 0.6780034986112806\n","Model5: 0.6780034986112806\n","\n","Accuracy: 0.7169459962756052\n","Macro-F1 score: 0.6780034986112806\n","Model6: 0.6780034986112806\n","\n","Accuracy: 0.6524674115456238\n","Macro-F1 score: 0.5456504896637868\n","KNN: 0.5456504896637868\n","\n","kernel = poly\n","X_train: (12888, 500), y_train: (12888,)\n","X_test: (4296, 500), y_test: (4296,)\n","Accuracy: 0.625465549348231\n","Macro-F1 score: 0.384791636832307\n","Model3: 0.384791636832307\n","\n","Accuracy: 0.625465549348231\n","Macro-F1 score: 0.384791636832307\n","Model4: 0.384791636832307\n","\n","Accuracy: 0.625465549348231\n","Macro-F1 score: 0.384791636832307\n","Model5: 0.384791636832307\n","\n","Accuracy: 0.625465549348231\n","Macro-F1 score: 0.384791636832307\n","Model6: 0.384791636832307\n","\n","Accuracy: 0.6524674115456238\n","Macro-F1 score: 0.5456504896637868\n","KNN: 0.5456504896637868\n","\n","kernel = rbf\n","X_train: (12888, 500), y_train: (12888,)\n","X_test: (4296, 500), y_test: (4296,)\n","Accuracy: 0.625465549348231\n","Macro-F1 score: 0.384791636832307\n","Model3: 0.384791636832307\n","\n","Accuracy: 0.625465549348231\n","Macro-F1 score: 0.384791636832307\n","Model4: 0.384791636832307\n","\n","Accuracy: 0.625465549348231\n","Macro-F1 score: 0.384791636832307\n","Model5: 0.384791636832307\n","\n","Accuracy: 0.625465549348231\n","Macro-F1 score: 0.384791636832307\n","Model6: 0.384791636832307\n","\n","Accuracy: 0.6524674115456238\n","Macro-F1 score: 0.5456504896637868\n","KNN: 0.5456504896637868\n","\n","kernel = sigmoid\n","X_train: (12888, 500), y_train: (12888,)\n","X_test: (4296, 500), y_test: (4296,)\n","Accuracy: 0.625465549348231\n","Macro-F1 score: 0.384791636832307\n","Model3: 0.384791636832307\n","\n","Accuracy: 0.625465549348231\n","Macro-F1 score: 0.384791636832307\n","Model4: 0.384791636832307\n","\n","Accuracy: 0.625465549348231\n","Macro-F1 score: 0.384791636832307\n","Model5: 0.384791636832307\n","\n","Accuracy: 0.625465549348231\n","Macro-F1 score: 0.384791636832307\n","Model6: 0.384791636832307\n","\n","Accuracy: 0.6524674115456238\n","Macro-F1 score: 0.5456504896637868\n","KNN: 0.5456504896637868\n","\n","kernel = cosine\n","X_train: (12888, 500), y_train: (12888,)\n","X_test: (4296, 500), y_test: (4296,)\n","Accuracy: 0.625465549348231\n","Macro-F1 score: 0.4881696700131085\n","Model3: 0.4881696700131085\n","\n","Accuracy: 0.7176443202979516\n","Macro-F1 score: 0.6789750953871346\n","Model4: 0.6789750953871346\n","\n","Accuracy: 0.7169459962756052\n","Macro-F1 score: 0.6780034986112806\n","Model5: 0.6780034986112806\n","\n","Accuracy: 0.7169459962756052\n","Macro-F1 score: 0.6780034986112806\n","Model6: 0.6780034986112806\n","\n","Accuracy: 0.6524674115456238\n","Macro-F1 score: 0.5456504896637868\n","KNN: 0.5456504896637868\n","\n","CPU times: total: 5h 23min 12s\n","Wall time: 44min 38s\n"]}],"source":["%%time\n","bs = 2048\n","epochs = 256\n","lr = 0.1\n","C = 0\n","beta_m = 0.6 #No pattern could be clearly determined (classic rec: 0.9)\n","beta_v = 0.999 #(as close to 1) (classic rec: 0.999)\n","err = 0.01 #0.01 (classic rec: 1e-8)\n","\n","n_components=500\n","Kernels=[\"linear\", \"poly\", \"rbf\", \"sigmoid\", \"cosine\"]\n","n_jobs=-1\n","random_state=100\n","print(f\"n_components={n_components}, random_state={random_state}, kernels={Kernels}, n_jobs={n_jobs}\")\n","\n","for kernel in Kernels:\n","    start = time.time()\n","    pca = KernelPCA(n_components=n_components, kernel=kernel, n_jobs=n_jobs, random_state=random_state)\n","    pca.fit(df_features.to_numpy())\n","    X_pca = pca.transform(df_features.to_numpy())\n","    \n","    print(f\"kernel = {kernel}\")\n","#     print(X_pca)\n","#     print(X_pca.shape)\n","#     print(df_label.shape)\n","\n","    X_pcatrain, X_pcatest, y_pcatrain, y_pcatest = train_test_split(X_pca, df_label, \\\n","                                                                    test_size=0.25, random_state=100)\n","    print(f\"X_train: {X_pcatrain.shape}, y_train: {y_pcatrain.shape}\")\n","    print(f\"X_test: {X_pcatest.shape}, y_test: {y_pcatest.shape}\")\n","    \n","    w, b, l = train3(X_pcatrain, y_pcatrain.values, bs, epochs, lr, C)\n","    Model3 = score(y_pcatest.values,  predict(X_pcatest, w, b))\n","    print(f\"Model3: {Model3}\\n\")\n","    \n","    w, b, l = train4(X_pcatrain, y_pcatrain.values, bs, epochs, lr, C, beta_m, beta_v, err)\n","    Model4 = score(y_pcatest.values,  predict(X_pcatest, w, b))\n","    print(f\"Model4: {Model4}\\n\")\n","    \n","    w, b, l = train5(X_pcatrain, y_pcatrain.values, bs, epochs, lr, C, beta_m, beta_v, err)\n","    Model5 = score(y_pcatest.values,  predict(X_pcatest, w, b))\n","    print(f\"Model5: {Model5}\\n\")\n","    \n","    w, b, l = train6(X_pcatrain, y_pcatrain.values, bs, epochs, lr, C, beta_m, beta_v, err)\n","    Model6 = score(y_pcatest.values,  predict(X_pcatest, w, b))\n","    print(f\"Model6: {Model6}\\n\")\n","    \n","    knn = KNN(n_neighbors=2)\n","    knn.fit(X_pcatrain, y_pcatrain)\n","    knnScore = score(y_pcatest, knn.predict(X_pcatest))\n","    print(f\"KNN: {knnScore} \\n\")\n","    elapsed = time.time() - start\n","    print(f\"Time taken for {n_components} Principal Components: {elapsed}s \\n\")"]},{"cell_type":"markdown","metadata":{},"source":[]},{"cell_type":"markdown","metadata":{},"source":["# Task 3: Try other machine learning models and race to the top!\n","In this course, you are exposed to many other machine learning models. For this task, you can apply any other machine learning models (taught in the course or not) to improve the hate speech classification performance! Nevertheless, you are NOT TO use any deep learning approach"]},{"cell_type":"markdown","metadata":{},"source":["## Initialising Setup"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Intel(R) Extension for Scikit-learn* enabled (https://github.com/intel/scikit-learn-intelex)\n"]}],"source":["from sklearnex import patch_sklearn, unpatch_sklearn\n","patch_sklearn()\n","import numpy as np\n","import pandas as pd\n","\n","from sklearn.model_selection import train_test_split, cross_val_score, RepeatedStratifiedKFold\n","from sklearn.metrics import f1_score\n","from sklearn.decomposition import TruncatedSVD\n","\n","from lightgbm import LGBMClassifier as LGBM\n","from catboost import CatBoostClassifier as CatBoost\n","from xgboost import XGBClassifier as XGB\n","from sklearn.tree import DecisionTreeClassifier as DecisionTree\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.ensemble import (RandomForestClassifier as RandomForest, AdaBoostClassifier as AdaBoost, HistGradientBoostingClassifier as HistGradBoost, StackingClassifier, VotingClassifier)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def score(y, y_hat):\n","    accuracy = np.sum(y == y_hat) / np.shape(y)[0]\n","    f1score = f1_score(y, y_hat, average='macro')\n","\n","    print(f\"Accuracy: {accuracy}\")\n","    print(f\"Macro-F1 score: {f1score}\")\n","\n","    # Return Macro-F1 score of the model\n","    return f1score"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>label</th>\n","      <th>0</th>\n","      <th>1</th>\n","      <th>2</th>\n","      <th>3</th>\n","      <th>4</th>\n","      <th>5</th>\n","      <th>6</th>\n","      <th>7</th>\n","      <th>...</th>\n","      <th>4990</th>\n","      <th>4991</th>\n","      <th>4992</th>\n","      <th>4993</th>\n","      <th>4994</th>\n","      <th>4995</th>\n","      <th>4996</th>\n","      <th>4997</th>\n","      <th>4998</th>\n","      <th>4999</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3</td>\n","      <td>1</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>4</td>\n","      <td>0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>5</td>\n","      <td>1</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>17179</th>\n","      <td>17180</td>\n","      <td>0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>17180</th>\n","      <td>17181</td>\n","      <td>0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>17181</th>\n","      <td>17182</td>\n","      <td>1</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>17182</th>\n","      <td>17183</td>\n","      <td>1</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>17183</th>\n","      <td>17184</td>\n","      <td>0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>17184 rows × 5002 columns</p>\n","</div>"],"text/plain":["          id  label    0    1    2    3    4    5    6    7  ...  4990  4991  \\\n","0          1      1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n","1          2      0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n","2          3      1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n","3          4      0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n","4          5      1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n","...      ...    ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   ...   ...   \n","17179  17180      0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n","17180  17181      0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n","17181  17182      1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n","17182  17183      1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n","17183  17184      0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n","\n","       4992  4993  4994  4995  4996  4997  4998  4999  \n","0       0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n","1       0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n","2       0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n","3       0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n","4       0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n","...     ...   ...   ...   ...   ...   ...   ...   ...  \n","17179   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n","17180   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n","17181   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n","17182   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n","17183   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n","\n","[17184 rows x 5002 columns]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["CPU times: total: 29 s\n","Wall time: 29.5 s\n"]}],"source":["%%time\n","df_train = pd.read_csv(r\"./source/train_tfidf_features.csv\")\n","display(df_train)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["0    10633\n","1     6551\n","Name: label, dtype: int64\n"]}],"source":["count_label = df_train['label'].value_counts(dropna=False) # Unique labels\n","print(count_label)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"data":{"text/plain":["array([[0., 0., 0., ..., 0., 0., 0.],\n","       [0., 0., 0., ..., 0., 0., 0.],\n","       [0., 0., 0., ..., 0., 0., 0.],\n","       ...,\n","       [0., 0., 0., ..., 0., 0., 0.],\n","       [0., 0., 0., ..., 0., 0., 0.],\n","       [0., 0., 0., ..., 0., 0., 0.]])"]},"metadata":{},"output_type":"display_data"}],"source":["X = df_train.iloc[:, 2:].to_numpy()\n","display(X)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"data":{"text/plain":["array([1, 0, 1, ..., 1, 1, 0], dtype=int64)"]},"metadata":{},"output_type":"display_data"}],"source":["y = df_train.iloc[:, 1].to_numpy()\n","display(y)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["X_train: (13747, 5000), y_train: (13747,)\n","X_test: (3437, 5000), y_test: (3437,)\n","CPU times: total: 1.12 s\n","Wall time: 529 ms\n"]}],"source":["%%time\n","X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=100)\n","print(f\"X_train: {X_train.shape}, y_train: {y_train.shape}\")\n","print(f\"X_test: {X_valid.shape}, y_test: {y_valid.shape}\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>0</th>\n","      <th>1</th>\n","      <th>2</th>\n","      <th>3</th>\n","      <th>4</th>\n","      <th>5</th>\n","      <th>6</th>\n","      <th>7</th>\n","      <th>8</th>\n","      <th>9</th>\n","      <th>...</th>\n","      <th>4990</th>\n","      <th>4991</th>\n","      <th>4992</th>\n","      <th>4993</th>\n","      <th>4994</th>\n","      <th>4995</th>\n","      <th>4996</th>\n","      <th>4997</th>\n","      <th>4998</th>\n","      <th>4999</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>17179</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>17180</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>17181</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>17182</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>17183</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>17184 rows × 5000 columns</p>\n","</div>"],"text/plain":["         0    1    2    3    4    5    6    7    8    9  ...  4990  4991  \\\n","0      0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n","1      0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n","2      0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n","3      0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n","4      0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n","...    ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   ...   ...   \n","17179  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n","17180  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n","17181  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n","17182  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n","17183  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n","\n","       4992  4993  4994  4995  4996  4997  4998  4999  \n","0       0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n","1       0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n","2       0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n","3       0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n","4       0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n","...     ...   ...   ...   ...   ...   ...   ...   ...  \n","17179   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n","17180   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n","17181   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n","17182   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n","17183   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n","\n","[17184 rows x 5000 columns]"]},"metadata":{},"output_type":"display_data"}],"source":["data = df_train.iloc[:, 2:]\n","# data.assign(const=1)\n","display(data)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>0</th>\n","      <th>1</th>\n","      <th>2</th>\n","      <th>3</th>\n","      <th>4</th>\n","      <th>5</th>\n","      <th>6</th>\n","      <th>7</th>\n","      <th>8</th>\n","      <th>...</th>\n","      <th>4990</th>\n","      <th>4991</th>\n","      <th>4992</th>\n","      <th>4993</th>\n","      <th>4994</th>\n","      <th>4995</th>\n","      <th>4996</th>\n","      <th>4997</th>\n","      <th>4998</th>\n","      <th>4999</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>17185</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>17186</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>17187</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>17188</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>17189</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>4291</th>\n","      <td>21476</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>4292</th>\n","      <td>21477</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>4293</th>\n","      <td>21478</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>4294</th>\n","      <td>21479</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>4295</th>\n","      <td>21480</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>4296 rows × 5001 columns</p>\n","</div>"],"text/plain":["         id    0    1    2    3    4    5    6    7    8  ...  4990  4991  \\\n","0     17185  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n","1     17186  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n","2     17187  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n","3     17188  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n","4     17189  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n","...     ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   ...   ...   \n","4291  21476  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n","4292  21477  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n","4293  21478  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n","4294  21479  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n","4295  21480  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n","\n","      4992  4993  4994  4995  4996  4997  4998  4999  \n","0      0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n","1      0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n","2      0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n","3      0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n","4      0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n","...    ...   ...   ...   ...   ...   ...   ...   ...  \n","4291   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n","4292   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n","4293   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n","4294   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n","4295   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n","\n","[4296 rows x 5001 columns]"]},"metadata":{},"output_type":"display_data"}],"source":["df_test = pd.read_csv(r\"./source/test_tfidf_features.csv\")\n","display(df_test)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"data":{"text/plain":["array([[0., 0., 0., ..., 0., 0., 0.],\n","       [0., 0., 0., ..., 0., 0., 0.],\n","       [0., 0., 0., ..., 0., 0., 0.],\n","       ...,\n","       [0., 0., 0., ..., 0., 0., 0.],\n","       [0., 0., 0., ..., 0., 0., 0.],\n","       [0., 0., 0., ..., 0., 0., 0.]])"]},"metadata":{},"output_type":"display_data"}],"source":["predict_label = df_test.iloc[:, 1:].to_numpy()\n","display(predict_label)"]},{"cell_type":"markdown","metadata":{},"source":["## LightGBM Decision Tree Classifier (Boosting Algorithm)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["lgbm = LGBM(boosting_type='gbdt', num_leaves=40, max_depth=-1, learning_rate=0.2, n_estimators=100, class_weight='balanced', reg_lambda=0, random_state=100, reg_alpha=0, n_jobs=-1)\n","cv = RepeatedStratifiedKFold(n_splits=8, n_repeats=5, random_state=100)\n","# boosting_type='gbdt', num_leaves=40, max_depth=-1, learning_rate=0.2, n_estimators=100, class_weight=None, reg_lambda=0, random_state=100, reg_alpha=0, n_jobs=-1"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Macro-F1: 0.682 (0.010)\n","CPU times: total: 547 ms\n","Wall time: 35.5 s\n"]}],"source":["%%time\n","n_scores = cross_val_score(lgbm, X_train, y_train, scoring='f1_macro', cv=cv, n_jobs=-1, error_score='raise')\n","print('Macro-F1: %.3f (%.3f)' % (np.mean(n_scores), np.std(n_scores)))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy: 0.7049752691300553\n","Macro-F1 score: 0.6912366629385973\n","CPU times: total: 13.6 s\n","Wall time: 2.52 s\n"]},{"data":{"text/plain":["0.6912366629385973"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["%%time\n","lgbm.fit(X_train, y_train)\n","score(y_valid, lgbm.predict(X_valid))\n","# 0.6955470999588647"]},{"cell_type":"markdown","metadata":{},"source":["## CatBoost Decision Tree Classifier (Boosting Algorithm)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["cboost = CatBoost(boosting_type=\"Plain\", loss_function=\"Logloss\", max_depth=4, learning_rate=0.2, n_estimators=100, early_stopping_rounds=10, auto_class_weights=\"Balanced\", reg_lambda=0, random_state=100, verbose=0)\n","# loss_function=\"Logloss\", max_depth=None, use_best_model=True, task_type=\"GPU\", devices='1', set max_depth to 4 due to memory error\n","#from catboost.utils import get_gpu_device_count\n","# print('I see %i GPU devices' % get_gpu_device_count())\n","cv = RepeatedStratifiedKFold(n_splits=8, n_repeats=5, random_state=100)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Macro-F1: 0.680 (0.011)\n","CPU times: total: 703 ms\n","Wall time: 4min 20s\n"]}],"source":["%%time\n","n_scores = cross_val_score(cboost, X_train, y_train, scoring=\"f1_macro\", cv=cv, n_jobs=-1, error_score=\"raise\")\n","print('Macro-F1: %.3f (%.3f)' % (np.mean(n_scores), np.std(n_scores)))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy: 0.6982833866744254\n","Macro-F1 score: 0.6839475771571616\n","CPU times: total: 21.7 s\n","Wall time: 3.61 s\n"]},{"data":{"text/plain":["0.6839475771571616"]},"execution_count":28,"metadata":{},"output_type":"execute_result"}],"source":["%%time\n","cboost.fit(X_train, y_train)\n","score(y_valid, cboost.predict(X_valid))\n","# 0.6864229482217066"]},{"cell_type":"markdown","metadata":{},"source":["## XGBoost Decision Tree Classifier (Boosting Algorithm)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["xgboost = XGB(learning_rate=0.2, booster=\"gbtree\", tree_method=\"hist\", max_depth=10, eta=0.2, subsample=0.65, scale_pos_weight=1.5,  grow_policy=\"lossguide\", n_estimators=100, reg_alpha=0, reg_lambda=0, random_state=100, verbosity=0, n_jobs=-1)\n","# num_parallel_tree=0, check GPU supported algorithms (tree_method=\"gpu_hist\", gpu_id=1), max_leaves=40, max_depth=31, \n","cv = RepeatedStratifiedKFold(n_splits=8, n_repeats=3, random_state=100)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Macro-F1: 0.686 (0.011)\n","CPU times: total: 453 ms\n","Wall time: 3min 27s\n"]}],"source":["%%time\n","n_scores = cross_val_score(xgboost, X_train, y_train, scoring='f1_macro', cv=cv, n_jobs=-1, error_score='raise')\n","print('Macro-F1: %.3f (%.3f)' % (np.mean(n_scores), np.std(n_scores)))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[16:35:05] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:177: Tree method is selected to be 'hist', which uses a single updater grow_quantile_histmaker.\n","[16:35:05] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:177: Tree method is selected to be 'hist', which uses a single updater grow_quantile_histmaker.\n","[16:35:05] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/data/simple_dmatrix.cc:102: Generating new Gradient Index.\n","[16:35:07] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:177: Tree method is selected to be 'hist', which uses a single updater grow_quantile_histmaker.\n","[16:35:07] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:177: Tree method is selected to be 'hist', which uses a single updater grow_quantile_histmaker.\n","[16:35:08] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:177: Tree method is selected to be 'hist', which uses a single updater grow_quantile_histmaker.\n","[16:35:08] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:177: Tree method is selected to be 'hist', which uses a single updater grow_quantile_histmaker.\n","[16:35:08] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:177: Tree method is selected to be 'hist', which uses a single updater grow_quantile_histmaker.\n","[16:35:09] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:177: Tree method is selected to be 'hist', which uses a single updater grow_quantile_histmaker.\n","[16:35:09] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:177: Tree method is selected to be 'hist', which uses a single updater grow_quantile_histmaker.\n","[16:35:09] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:177: Tree method is selected to be 'hist', which uses a single updater grow_quantile_histmaker.\n","[16:35:09] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:177: Tree method is selected to be 'hist', which uses a single updater grow_quantile_histmaker.\n","[16:35:09] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:177: Tree method is selected to be 'hist', which uses a single updater grow_quantile_histmaker.\n","[16:35:10] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:177: Tree method is selected to be 'hist', which uses a single updater grow_quantile_histmaker.\n","[16:35:10] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:177: Tree method is selected to be 'hist', which uses a single updater grow_quantile_histmaker.\n","[16:35:10] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:177: Tree method is selected to be 'hist', which uses a single updater grow_quantile_histmaker.\n","[16:35:10] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:177: Tree method is selected to be 'hist', which uses a single updater grow_quantile_histmaker.\n","[16:35:10] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:177: Tree method is selected to be 'hist', which uses a single updater grow_quantile_histmaker.\n","[16:35:11] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:177: Tree method is selected to be 'hist', which uses a single updater grow_quantile_histmaker.\n","[16:35:11] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:177: Tree method is selected to be 'hist', which uses a single updater grow_quantile_histmaker.\n","[16:35:11] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:177: Tree method is selected to be 'hist', which uses a single updater grow_quantile_histmaker.\n","[16:35:11] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:177: Tree method is selected to be 'hist', which uses a single updater grow_quantile_histmaker.\n","[16:35:11] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:177: Tree method is selected to be 'hist', which uses a single updater grow_quantile_histmaker.\n","[16:35:11] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:177: Tree method is selected to be 'hist', which uses a single updater grow_quantile_histmaker.\n","[16:35:12] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:177: Tree method is selected to be 'hist', which uses a single updater grow_quantile_histmaker.\n","[16:35:12] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:177: Tree method is selected to be 'hist', which uses a single updater grow_quantile_histmaker.\n","[16:35:12] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:177: Tree method is selected to be 'hist', which uses a single updater grow_quantile_histmaker.\n","[16:35:12] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:177: Tree method is selected to be 'hist', which uses a single updater grow_quantile_histmaker.\n","[16:35:12] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:177: Tree method is selected to be 'hist', which uses a single updater grow_quantile_histmaker.\n","[16:35:13] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:177: Tree method is selected to be 'hist', which uses a single updater grow_quantile_histmaker.\n","[16:35:13] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:177: Tree method is selected to be 'hist', which uses a single updater grow_quantile_histmaker.\n","[16:35:13] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:177: Tree method is selected to be 'hist', which uses a single updater grow_quantile_histmaker.\n","[16:35:13] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:177: Tree method is selected to be 'hist', which uses a single updater grow_quantile_histmaker.\n","[16:35:13] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:177: Tree method is selected to be 'hist', which uses a single updater grow_quantile_histmaker.\n","[16:35:13] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:177: Tree method is selected to be 'hist', which uses a single updater grow_quantile_histmaker.\n","[16:35:14] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:177: Tree method is selected to be 'hist', which uses a single updater grow_quantile_histmaker.\n","[16:35:14] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:177: Tree method is selected to be 'hist', which uses a single updater grow_quantile_histmaker.\n","[16:35:14] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:177: Tree method is selected to be 'hist', which uses a single updater grow_quantile_histmaker.\n","[16:35:14] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:177: Tree method is selected to be 'hist', which uses a single updater grow_quantile_histmaker.\n","[16:35:14] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:177: Tree method is selected to be 'hist', which uses a single updater grow_quantile_histmaker.\n","[16:35:14] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:177: Tree method is selected to be 'hist', which uses a single updater grow_quantile_histmaker.\n","[16:35:14] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:177: Tree method is selected to be 'hist', which uses a single updater grow_quantile_histmaker.\n","[16:35:15] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:177: Tree method is selected to be 'hist', which uses a single updater grow_quantile_histmaker.\n","[16:35:15] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:177: Tree method is selected to be 'hist', which uses a single updater grow_quantile_histmaker.\n","[16:35:15] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:177: Tree method is selected to be 'hist', which uses a single updater grow_quantile_histmaker.\n","[16:35:15] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:177: Tree method is selected to be 'hist', which uses a single updater grow_quantile_histmaker.\n","[16:35:15] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:177: Tree method is selected to be 'hist', which uses a single updater grow_quantile_histmaker.\n","[16:35:15] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:177: Tree method is selected to be 'hist', which uses a single updater grow_quantile_histmaker.\n","[16:35:15] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:177: Tree method is selected to be 'hist', which uses a single updater grow_quantile_histmaker.\n","[16:35:16] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:177: Tree method is selected to be 'hist', which uses a single updater grow_quantile_histmaker.\n","[16:35:16] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:177: Tree method is selected to be 'hist', which uses a single updater grow_quantile_histmaker.\n","[16:35:16] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:177: Tree method is selected to be 'hist', which uses a single updater grow_quantile_histmaker.\n","[16:35:16] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:177: Tree method is selected to be 'hist', which uses a single updater grow_quantile_histmaker.\n","[16:35:16] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:177: Tree method is selected to be 'hist', which uses a single updater grow_quantile_histmaker.\n","[16:35:16] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:177: Tree method is selected to be 'hist', which uses a single updater grow_quantile_histmaker.\n","[16:35:16] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:177: Tree method is selected to be 'hist', which uses a single updater grow_quantile_histmaker.\n","[16:35:16] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:177: Tree method is selected to be 'hist', which uses a single updater grow_quantile_histmaker.\n","[16:35:17] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:177: Tree method is selected to be 'hist', which uses a single updater grow_quantile_histmaker.\n","[16:35:17] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:177: Tree method is selected to be 'hist', which uses a single updater grow_quantile_histmaker.\n","[16:35:17] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:177: Tree method is selected to be 'hist', which uses a single updater grow_quantile_histmaker.\n","[16:35:17] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:177: Tree method is selected to be 'hist', which uses a single updater grow_quantile_histmaker.\n","[16:35:17] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:177: Tree method is selected to be 'hist', which uses a single updater grow_quantile_histmaker.\n","[16:35:17] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:177: Tree method is selected to be 'hist', which uses a single updater grow_quantile_histmaker.\n","[16:35:17] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:177: Tree method is selected to be 'hist', which uses a single updater grow_quantile_histmaker.\n","[16:35:18] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:177: Tree method is selected to be 'hist', which uses a single updater grow_quantile_histmaker.\n","[16:35:18] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:177: Tree method is selected to be 'hist', which uses a single updater grow_quantile_histmaker.\n","[16:35:18] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:177: Tree method is selected to be 'hist', which uses a single updater grow_quantile_histmaker.\n","[16:35:18] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:177: Tree method is selected to be 'hist', which uses a single updater grow_quantile_histmaker.\n","[16:35:18] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:177: Tree method is selected to be 'hist', which uses a single updater grow_quantile_histmaker.\n","[16:35:18] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:177: Tree method is selected to be 'hist', which uses a single updater grow_quantile_histmaker.\n","[16:35:18] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:177: Tree method is selected to be 'hist', which uses a single updater grow_quantile_histmaker.\n","[16:35:19] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:177: Tree method is selected to be 'hist', which uses a single updater grow_quantile_histmaker.\n","[16:35:19] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:177: Tree method is selected to be 'hist', which uses a single updater grow_quantile_histmaker.\n","[16:35:19] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:177: Tree method is selected to be 'hist', which uses a single updater grow_quantile_histmaker.\n","[16:35:19] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:177: Tree method is selected to be 'hist', which uses a single updater grow_quantile_histmaker.\n","[16:35:19] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:177: Tree method is selected to be 'hist', which uses a single updater grow_quantile_histmaker.\n","[16:35:19] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:177: Tree method is selected to be 'hist', which uses a single updater grow_quantile_histmaker.\n","[16:35:20] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:177: Tree method is selected to be 'hist', which uses a single updater grow_quantile_histmaker.\n","[16:35:20] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:177: Tree method is selected to be 'hist', which uses a single updater grow_quantile_histmaker.\n","[16:35:20] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:177: Tree method is selected to be 'hist', which uses a single updater grow_quantile_histmaker.\n","[16:35:20] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:177: Tree method is selected to be 'hist', which uses a single updater grow_quantile_histmaker.\n","[16:35:20] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:177: Tree method is selected to be 'hist', which uses a single updater grow_quantile_histmaker.\n","[16:35:20] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:177: Tree method is selected to be 'hist', which uses a single updater grow_quantile_histmaker.\n","[16:35:21] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:177: Tree method is selected to be 'hist', which uses a single updater grow_quantile_histmaker.\n","[16:35:21] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:177: Tree method is selected to be 'hist', which uses a single updater grow_quantile_histmaker.\n","[16:35:21] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:177: Tree method is selected to be 'hist', which uses a single updater grow_quantile_histmaker.\n","[16:35:21] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:177: Tree method is selected to be 'hist', which uses a single updater grow_quantile_histmaker.\n","[16:35:21] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:177: Tree method is selected to be 'hist', which uses a single updater grow_quantile_histmaker.\n","[16:35:21] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:177: Tree method is selected to be 'hist', which uses a single updater grow_quantile_histmaker.\n","[16:35:22] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:177: Tree method is selected to be 'hist', which uses a single updater grow_quantile_histmaker.\n","[16:35:22] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:177: Tree method is selected to be 'hist', which uses a single updater grow_quantile_histmaker.\n","[16:35:22] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:177: Tree method is selected to be 'hist', which uses a single updater grow_quantile_histmaker.\n","[16:35:22] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:177: Tree method is selected to be 'hist', which uses a single updater grow_quantile_histmaker.\n","[16:35:22] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:177: Tree method is selected to be 'hist', which uses a single updater grow_quantile_histmaker.\n","[16:35:22] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:177: Tree method is selected to be 'hist', which uses a single updater grow_quantile_histmaker.\n","[16:35:22] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:177: Tree method is selected to be 'hist', which uses a single updater grow_quantile_histmaker.\n","[16:35:23] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:177: Tree method is selected to be 'hist', which uses a single updater grow_quantile_histmaker.\n","[16:35:23] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:177: Tree method is selected to be 'hist', which uses a single updater grow_quantile_histmaker.\n","[16:35:23] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:177: Tree method is selected to be 'hist', which uses a single updater grow_quantile_histmaker.\n","[16:35:23] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:177: Tree method is selected to be 'hist', which uses a single updater grow_quantile_histmaker.\n","[16:35:23] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:177: Tree method is selected to be 'hist', which uses a single updater grow_quantile_histmaker.\n","[16:35:23] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:177: Tree method is selected to be 'hist', which uses a single updater grow_quantile_histmaker.\n","[16:35:24] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:177: Tree method is selected to be 'hist', which uses a single updater grow_quantile_histmaker.\n","Accuracy: 0.7160314227524004\n","Macro-F1 score: 0.6939604271127557\n","CPU times: total: 2min 40s\n","Wall time: 20.7 s\n"]},{"data":{"text/plain":["0.6939604271127557"]},"execution_count":31,"metadata":{},"output_type":"execute_result"}],"source":["%%time\n","xgboost.fit(X_train, y_train)\n","score(y_valid, xgboost.predict(X_valid))"]},{"cell_type":"markdown","metadata":{},"source":["## Decision Tree Classifier"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["dtrees = DecisionTree(criterion=\"gini\", splitter=\"best\", max_features=\"sqrt\", max_depth=20, class_weight=\"balanced\", max_leaf_nodes=40, random_state=100)\n","# criterion = \"log_loss\" or \"entropy\", min_samples_split, min_samples_leaf, ccp_alpha\n","cv = RepeatedStratifiedKFold(n_splits=8, n_repeats=3, random_state=100)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Macro-F1: 0.519 (0.027)\n","CPU times: total: 594 ms\n","Wall time: 14 s\n"]}],"source":["%%time\n","n_scores = cross_val_score(dtrees, X_train, y_train, scoring='f1_macro', cv=cv, n_jobs=-1, error_score='raise')\n","print('Macro-F1: %.3f (%.3f)' % (np.mean(n_scores), np.std(n_scores)))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy: 0.6325283677625837\n","Macro-F1 score: 0.5217792210925485\n","CPU times: total: 453 ms\n","Wall time: 449 ms\n"]},{"data":{"text/plain":["0.5217792210925485"]},"execution_count":34,"metadata":{},"output_type":"execute_result"}],"source":["%%time\n","dtrees.fit(X_train, y_train)\n","score(y_valid, dtrees.predict(X_valid))"]},{"cell_type":"markdown","metadata":{},"source":["## Random Forest Classifier (Bagging Algorithm)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["randforest = RandomForest(n_estimators=100, criterion=\"gini\", max_features=\"sqrt\", max_depth=20, n_jobs=-1, max_leaf_nodes=40, verbose=0, random_state=100, warm_start=True, oob_score=True, class_weight=\"balanced\")\n","# criterion = \"log_loss\" or \"entropy\", min_samples_split, max_depth, min_samples_leaf, ccp_alpha, max_samples\n","cv = RepeatedStratifiedKFold(n_splits=8, n_repeats=3, random_state=100)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Macro-F1: 0.685 (0.014)\n","CPU times: total: 469 ms\n","Wall time: 2min 21s\n"]}],"source":["%%time\n","n_scores = cross_val_score(randforest, X_train, y_train, scoring='f1_macro', cv=cv, n_jobs=-1, error_score='raise')\n","print('Macro-F1: %.3f (%.3f)' % (np.mean(n_scores), np.std(n_scores)))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Users\\issac\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:789: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n","  warn(\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy: 0.7142857142857143\n","Macro-F1 score: 0.6946775959606049\n","CPU times: total: 50.3 s\n","Wall time: 41.4 s\n"]},{"data":{"text/plain":["0.6946775959606049"]},"execution_count":37,"metadata":{},"output_type":"execute_result"}],"source":["%%time\n","randforest.fit(X_train, y_train)\n","score(y_valid, randforest.predict(X_valid))"]},{"cell_type":"markdown","metadata":{},"source":["## Adaboost Decision Tree Classifier (Boosting Algorithm)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["base = DecisionTree(criterion=\"gini\", splitter=\"best\", max_features=\"sqrt\", max_depth=20, class_weight=\"balanced\", max_leaf_nodes=40, random_state=100)\n","adaboost = AdaBoost(base_estimator=base, n_estimators=100, learning_rate=0.2, random_state=100)\n","cv = RepeatedStratifiedKFold(n_splits=8, n_repeats=3, random_state=100)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Macro-F1: 0.668 (0.011)\n","CPU times: total: 562 ms\n","Wall time: 8min 21s\n"]}],"source":["%%time\n","n_scores = cross_val_score(adaboost, X_train, y_train, scoring='f1_macro', cv=cv, n_jobs=-1, error_score='raise')\n","print('Macro-F1: %.3f (%.3f)' % (np.mean(n_scores), np.std(n_scores)))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy: 0.6994471923188827\n","Macro-F1 score: 0.6848375761149057\n","CPU times: total: 46.3 s\n","Wall time: 46.5 s\n"]},{"data":{"text/plain":["0.6848375761149057"]},"execution_count":40,"metadata":{},"output_type":"execute_result"}],"source":["%%time\n","adaboost.fit(X_train, y_train)\n","score(y_valid, adaboost.predict(X_valid))"]},{"cell_type":"markdown","metadata":{},"source":["## Histogram-based Gradient Boosting Classifier (Boosting Algorithm)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["histboost = HistGradBoost(loss=\"log_loss\", learning_rate=0.2, max_depth=20, max_leaf_nodes=40, warm_start=True, scoring=\"f1_macro\", verbose=0, random_state=100)\n","# max_iter, max_depth, min_samples_leaf (default = 20), l2_regularization, max_bins, monotonic_cst, validation_fraction, n_iter_no_change, tol\n","cv = RepeatedStratifiedKFold(n_splits=8, n_repeats=3, random_state=100)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Macro-F1: 0.664 (0.010)\n","CPU times: total: 422 ms\n","Wall time: 8min 45s\n"]}],"source":["%%time\n","n_scores = cross_val_score(histboost, X_train, y_train, scoring='f1_macro', cv=cv, n_jobs=-1, error_score='raise')\n","print('Macro-F1: %.3f (%.3f)' % (np.mean(n_scores), np.std(n_scores)))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy: 0.7209775967413442\n","Macro-F1 score: 0.6763721167430697\n","CPU times: total: 6min 47s\n","Wall time: 1min 33s\n"]},{"data":{"text/plain":["0.6763721167430697"]},"execution_count":43,"metadata":{},"output_type":"execute_result"}],"source":["%%time\n","histboost.fit(X_train, y_train)\n","score(y_valid, histboost.predict(X_valid))"]},{"cell_type":"markdown","metadata":{},"source":["## Advanced Ensemble Methods (Voting Classifier and StackingClassifier)"]},{"cell_type":"markdown","metadata":{},"source":["### Voting Classifier"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[17:27:07] INFO: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/gbm/gbtree.cc:177: Tree method is selected to be 'hist', which uses a single updater grow_quantile_histmaker.\n","Accuracy: 0.7294151876636602\n","Macro-F1 score: 0.7077900351265191\n","CPU times: total: 10.2 s\n","Wall time: 2min 28s\n"]},{"data":{"text/plain":["0.7077900351265191"]},"execution_count":76,"metadata":{},"output_type":"execute_result"}],"source":["%%time\n","models = [('lgbm', lgbm), ('catboost', cboost), ('xgboost', xgboost), ('randforest', randforest), ('adaboost', adaboost), ('histboost', histboost)]\n","votes = VotingClassifier(estimators=models, voting='soft', n_jobs=-1, verbose=True)\n","votes = votes.fit(X_train, y_train)\n","score(y_valid, votes.predict(X_valid))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"data":{"text/plain":["array([1, 0, 1, ..., 1, 0, 0], dtype=int64)"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["CPU times: total: 10.5 s\n","Wall time: 4.99 s\n"]}],"source":["%%time\n","results = votes.predict(predict_label)\n","display(results)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["(4296, 1) (4296, 1)\n","\n","\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>17185</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>17186</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>17187</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>17188</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>17189</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>4291</th>\n","      <td>21476</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4292</th>\n","      <td>21477</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4293</th>\n","      <td>21478</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4294</th>\n","      <td>21479</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4295</th>\n","      <td>21480</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>4296 rows × 2 columns</p>\n","</div>"],"text/plain":["         id  label\n","0     17185      1\n","1     17186      0\n","2     17187      1\n","3     17188      0\n","4     17189      0\n","...     ...    ...\n","4291  21476      1\n","4292  21477      1\n","4293  21478      1\n","4294  21479      0\n","4295  21480      0\n","\n","[4296 rows x 2 columns]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["CPU times: total: 15.6 ms\n","Wall time: 10 ms\n"]},{"data":{"text/plain":["0    2750\n","1    1546\n","Name: label, dtype: int64"]},"execution_count":79,"metadata":{},"output_type":"execute_result"}],"source":["%%time\n","df_ids = df_test.iloc[:, 0].to_frame()\n","df_results = pd.DataFrame(results)\n","print(df_results.shape, df_ids.shape)\n","print(\"\\n\")\n","    \n","df_submission = pd.concat([df_ids, df_results], axis =1)\n","df_submission = df_submission.rename(columns={0: 'label'})\n","\n","display(df_submission)\n","df_submission['label'].value_counts()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df_submission.to_csv(f\"Voting_Ensemble_Predictions.csv\", index=False)"]},{"cell_type":"markdown","metadata":{},"source":["### Stacking Classifier"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["## Other Machine Learning Models (Naive Bayes Classifier, Support Vector Machines, Logistic Regression)"]},{"cell_type":"markdown","metadata":{},"source":["### Initialise Setup"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.naive_bayes import MultinomialNB, ComplementNB # Other NB have poor performance\n","from sklearn.pipeline import make_pipeline\n","from imblearn.under_sampling import RandomUnderSampler\n","from imblearn.pipeline import make_pipeline as make_pipeline_imb\n","\n","from sklearn.svm import SVC, NuSVC, LinearSVC # One-Class SVM too slow\n","from sklearn.neighbors import KNeighborsClassifier as KNN\n","from sklearn.linear_model import LogisticRegression, LogisticRegressionCV"]},{"cell_type":"markdown","metadata":{},"source":["### Naive Bayes Classifier"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy: 0.6947919697410533\n","Macro-F1 score: 0.6874150189198457\n","CPU times: total: 1.5 s\n","Wall time: 1.3 s\n"]},{"data":{"text/plain":["0.6874150189198457"]},"execution_count":51,"metadata":{},"output_type":"execute_result"}],"source":["%%time\n","bayes = make_pipeline_imb(RandomUnderSampler(random_state=100), MultinomialNB())\n","bayes.fit(X_train, y_train)\n","score(y_valid, bayes.predict(X_valid))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy: 0.6947919697410533\n","Macro-F1 score: 0.6874150189198457\n","CPU times: total: 1.14 s\n","Wall time: 1.03 s\n"]},{"data":{"text/plain":["0.6874150189198457"]},"execution_count":82,"metadata":{},"output_type":"execute_result"}],"source":["%%time\n","cbayes = make_pipeline_imb(RandomUnderSampler(random_state=100), ComplementNB())\n","cbayes.fit(X_train, y_train)\n","score(y_valid, cbayes.predict(X_valid))"]},{"cell_type":"markdown","metadata":{},"source":["### Support Vector Machines"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy: 0.6942100669188246\n","Macro-F1 score: 0.6804989375134054\n","CPU times: total: 1.42 s\n","Wall time: 1.42 s\n"]},{"data":{"text/plain":["0.6804989375134054"]},"execution_count":53,"metadata":{},"output_type":"execute_result"}],"source":["%%time\n","lsvm = make_pipeline(LinearSVC(penalty=\"l2\", loss=\"squared_hinge\", dual=False, C=1.0, class_weight=\"balanced\", verbose=0, random_state=100, max_iter=1000))\n","lsvm.fit(X_train, y_train)\n","score(y_valid, lsvm.predict(X_valid))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy: 0.7436718068082631\n","Macro-F1 score: 0.7139433393747274\n","CPU times: total: 7min 19s\n","Wall time: 1min 15s\n"]},{"data":{"text/plain":["0.7139433393747274"]},"execution_count":54,"metadata":{},"output_type":"execute_result"}],"source":["%%time\n","nu_svm = make_pipeline(NuSVC(nu=0.5, kernel=\"rbf\", gamma=\"scale\", class_weight=\"balanced\", verbose=True, max_iter=1000, random_state=100))\n","nu_svm.fit(X_train, y_train)\n","score(y_valid, nu_svm.predict(X_valid))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy: 0.7471632237416351\n","Macro-F1 score: 0.7079075581526691\n","CPU times: total: 4min 47s\n","Wall time: 50.1 s\n"]},{"data":{"text/plain":["0.7079075581526691"]},"execution_count":55,"metadata":{},"output_type":"execute_result"}],"source":["%%time\n","svm = make_pipeline(SVC(C=1, kernel=\"rbf\", coef0=0.0, gamma=\"scale\", class_weight=\"balanced\", verbose=True, max_iter=1000, random_state=100))\n","svm.fit(X_train, y_train)\n","score(y_valid, svm.predict(X_valid))"]},{"cell_type":"markdown","metadata":{},"source":["### K-Nearest Neighbors Classifier"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy: 0.44893802734943267\n","Macro-F1 score: 0.431794709755808\n","CPU times: total: 19 s\n","Wall time: 4.78 s\n"]},{"data":{"text/plain":["0.431794709755808"]},"execution_count":56,"metadata":{},"output_type":"execute_result"}],"source":["%%time\n","knn = make_pipeline_imb(RandomUnderSampler(random_state=100), KNN(n_neighbors=2, n_jobs=-1))\n","knn.fit(X_train, y_train)\n","score(y_valid, knn.predict(X_valid))"]},{"cell_type":"markdown","metadata":{},"source":["### Logistic Regression"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[[-0.4189785  -0.98889631 -0.41457269 ... -0.16840873  0.48542735\n","   0.39037736]]\n","[-0.48920718]\n","Accuracy: 0.7244690136747163\n","Macro-F1 score: 0.7108468360326294\n","CPU times: total: 25.2 s\n","Wall time: 4.32 s\n"]},{"data":{"text/plain":["0.7108468360326294"]},"execution_count":57,"metadata":{},"output_type":"execute_result"}],"source":["%%time\n","LogReg = LogisticRegression(random_state = 100, class_weight='balanced')\n","LogReg.fit(X_train, y_train)\n","print(LogReg.coef_)\n","print(LogReg.intercept_)\n","score(y_valid, LogReg.predict(X_valid))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[[-0.1133715  -0.54403646 -0.15144215 ... -0.06760471  0.23374256\n","   0.13277173]]\n","[-0.34888196]\n","Accuracy: 0.723305208030259\n","Macro-F1 score: 0.709855309666288\n","CPU times: total: 3.61 s\n","Wall time: 4min 59s\n"]},{"name":"stderr","output_type":"stream","text":["c:\\Users\\issac\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\daal4py\\sklearn\\linear_model\\logistic_path.py:548: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  n_iter_i = _check_optimize_result(\n"]},{"data":{"text/plain":["0.709855309666288"]},"execution_count":58,"metadata":{},"output_type":"execute_result"}],"source":["%%time\n","cv = RepeatedStratifiedKFold(n_splits=8, n_repeats=3, random_state=100)\n","LogRegCV = LogisticRegressionCV(Cs=100, cv=cv, dual=False, penalty=\"l2\", scoring=\"f1_macro\", n_jobs=-1, verbose=0, random_state = 100, class_weight=\"balanced\", max_iter=1)\n","LogRegCV.fit(X_train, y_train)\n","print(LogRegCV.coef_)\n","print(LogRegCV.intercept_)\n","score(y_valid, LogRegCV.predict(X_valid))"]},{"cell_type":"markdown","metadata":{},"source":["### Stacking Classifier"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["estimators = [('NuSVC', nu_svm), ('bayes', bayes), ('cbayes', cbayes), ('SVC', svm), ('LinearSVC', lsvm), ('LogReg', LogReg)]\n","cv = RepeatedStratifiedKFold(n_splits=8, n_repeats=3, random_state=100)\n","final = LogisticRegression(random_state = 100)\n","stack = StackingClassifier(estimators=estimators, final_estimator=final, cv=\"prefit\", stack_method=\"auto\", n_jobs=-1, passthrough=False, verbose=0)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy: 0.7230142566191446\n","Macro-F1 score: 0.6975657549339307\n","CPU times: total: 5min 7s\n","Wall time: 51.6 s\n"]},{"data":{"text/plain":["0.6975657549339307"]},"execution_count":84,"metadata":{},"output_type":"execute_result"}],"source":["%%time\n","stack.fit(X_train, y_train)\n","score(y_valid, stack.predict(X_valid))"]},{"cell_type":"markdown","metadata":{},"source":["### Submission of Results"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"data":{"text/plain":["array([1, 0, 1, ..., 0, 0, 0], dtype=int64)"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["CPU times: total: 1min 15s\n","Wall time: 12.8 s\n"]}],"source":["%%time\n","results = stack.predict(predict_label)\n","display(results)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["(4296, 1) (4296, 1)\n","\n","\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>17185</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>17186</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>17187</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>17188</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>17189</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>4291</th>\n","      <td>21476</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4292</th>\n","      <td>21477</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4293</th>\n","      <td>21478</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4294</th>\n","      <td>21479</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4295</th>\n","      <td>21480</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>4296 rows × 2 columns</p>\n","</div>"],"text/plain":["         id  label\n","0     17185      1\n","1     17186      0\n","2     17187      1\n","3     17188      0\n","4     17189      0\n","...     ...    ...\n","4291  21476      0\n","4292  21477      0\n","4293  21478      0\n","4294  21479      0\n","4295  21480      0\n","\n","[4296 rows x 2 columns]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["CPU times: total: 0 ns\n","Wall time: 6.05 ms\n"]},{"data":{"text/plain":["0    2838\n","1    1458\n","Name: label, dtype: int64"]},"execution_count":75,"metadata":{},"output_type":"execute_result"}],"source":["%%time\n","df_ids = df_test.iloc[:, 0].to_frame()\n","df_results = pd.DataFrame(results)\n","print(df_results.shape, df_ids.shape)\n","print(\"\\n\")\n","    \n","df_submission = pd.concat([df_ids, df_results], axis =1)\n","df_submission = df_submission.rename(columns={0: 'label'})\n","\n","display(df_submission)\n","df_submission['label'].value_counts()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df_submission.to_csv(f\"Stacking_Ensemble_Predictions.csv\", index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["elapsed = time.time() - start_time"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Process finished --- 38540.790263175964 seconds ---\n"]}],"source":["print(\"Process finished --- %s seconds ---\" % elapsed)"]},{"cell_type":"markdown","metadata":{},"source":["### Appendix"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","from imblearn.combine import SMOTEENN, SMOTETomek\n","from imblearn.pipeline import Pipeline\n","\n","from lightgbm import LGBMClassifier as LGBM\n","\n","from sklearn.metrics import f1_score\n","from sklearn.decomposition import TruncatedSVD\n","from sklearn.calibration import CalibratedClassifierCV, CalibrationDisplay\n","from sklearn.model_selection import train_test_split, cross_val_score, RepeatedStratifiedKFold"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>label</th>\n","      <th>0</th>\n","      <th>1</th>\n","      <th>2</th>\n","      <th>3</th>\n","      <th>4</th>\n","      <th>5</th>\n","      <th>6</th>\n","      <th>7</th>\n","      <th>...</th>\n","      <th>4990</th>\n","      <th>4991</th>\n","      <th>4992</th>\n","      <th>4993</th>\n","      <th>4994</th>\n","      <th>4995</th>\n","      <th>4996</th>\n","      <th>4997</th>\n","      <th>4998</th>\n","      <th>4999</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3</td>\n","      <td>1</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>4</td>\n","      <td>0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>5</td>\n","      <td>1</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>17179</th>\n","      <td>17180</td>\n","      <td>0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>17180</th>\n","      <td>17181</td>\n","      <td>0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>17181</th>\n","      <td>17182</td>\n","      <td>1</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>17182</th>\n","      <td>17183</td>\n","      <td>1</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>17183</th>\n","      <td>17184</td>\n","      <td>0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>17184 rows × 5002 columns</p>\n","</div>"],"text/plain":["          id  label    0    1    2    3    4    5    6    7  ...  4990  4991  \\\n","0          1      1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n","1          2      0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n","2          3      1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n","3          4      0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n","4          5      1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n","...      ...    ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   ...   ...   \n","17179  17180      0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n","17180  17181      0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n","17181  17182      1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n","17182  17183      1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n","17183  17184      0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n","\n","       4992  4993  4994  4995  4996  4997  4998  4999  \n","0       0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n","1       0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n","2       0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n","3       0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n","4       0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n","...     ...   ...   ...   ...   ...   ...   ...   ...  \n","17179   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n","17180   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n","17181   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n","17182   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n","17183   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n","\n","[17184 rows x 5002 columns]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["CPU times: total: 13.9 s\n","Wall time: 13.9 s\n"]}],"source":["%%time\n","df_train = pd.read_csv(r\"./source/train_tfidf_features.csv\")\n","display(df_train)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["X_train: (15465, 5000), y_train: (15465,)\n","X_test: (1719, 5000), y_test: (1719,)\n","CPU times: total: 859 ms\n","Wall time: 386 ms\n"]}],"source":["%%time\n","X = df_train.iloc[:, 2:]\n","y = df_train.iloc[:, 1]\n","X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=100) # change test size to 0.2\n","print(f\"X_train: {X_train.shape}, y_train: {y_train.shape}\")\n","print(f\"X_test: {X_valid.shape}, y_test: {y_valid.shape}\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["0.9256683101733221\n"]}],"source":["svd = TruncatedSVD(n_components=3000, random_state=100)\n","svd.fit(X_train)\n","X_red = svd.transform(X_train)\n","print(np.sum(svd.explained_variance_ratio_))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["X_res: (18746, 3000), y_res: (18746,)\n"]}],"source":["smt = SMOTETomek(sampling_strategy=\"auto\", random_state=100, n_jobs=-1)\n","X_res, y_res = smt.fit_resample(X_red, y_train)\n","print(f\"X_res: {X_res.shape}, y_res: {y_res.shape}\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["lgbm = LGBM(boosting_type='gbdt', num_leaves=40, max_depth=-1, learning_rate=0.2, n_estimators=100, class_weight='balanced', reg_lambda=0, random_state=100, verbose=100, reg_alpha=0, n_jobs=-1)\n","cv = RepeatedStratifiedKFold(n_splits=8, n_repeats=3, random_state=100)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["calibrated = CalibratedClassifierCV(base_estimator=lgbm, method=\"isotonic\", cv=2, n_jobs=-1, ensemble=True)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n","[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:  6.2min\n","[Parallel(n_jobs=-1)]: Done   2 out of  24 | elapsed:  6.3min remaining: 68.8min\n","[Parallel(n_jobs=-1)]: Done   3 out of  24 | elapsed:  6.3min remaining: 43.9min\n","[Parallel(n_jobs=-1)]: Done   4 out of  24 | elapsed:  6.4min remaining: 31.9min\n","[Parallel(n_jobs=-1)]: Done   5 out of  24 | elapsed:  6.5min remaining: 24.7min\n","[Parallel(n_jobs=-1)]: Done   6 out of  24 | elapsed:  6.5min remaining: 19.6min\n","[Parallel(n_jobs=-1)]: Done   7 out of  24 | elapsed:  6.5min remaining: 15.9min\n","[Parallel(n_jobs=-1)]: Done   8 out of  24 | elapsed:  6.6min remaining: 13.1min\n","[Parallel(n_jobs=-1)]: Done   9 out of  24 | elapsed:  6.6min remaining: 11.0min\n","[Parallel(n_jobs=-1)]: Done  10 out of  24 | elapsed:  6.6min remaining:  9.3min\n","[Parallel(n_jobs=-1)]: Done  11 out of  24 | elapsed:  6.6min remaining:  7.9min\n","[Parallel(n_jobs=-1)]: Done  12 out of  24 | elapsed:  6.9min remaining:  6.9min\n","[Parallel(n_jobs=-1)]: Done  13 out of  24 | elapsed: 12.8min remaining: 10.8min\n","[Parallel(n_jobs=-1)]: Done  14 out of  24 | elapsed: 12.8min remaining:  9.2min\n","[Parallel(n_jobs=-1)]: Done  15 out of  24 | elapsed: 12.9min remaining:  7.7min\n","[Parallel(n_jobs=-1)]: Done  16 out of  24 | elapsed: 12.9min remaining:  6.5min\n","[Parallel(n_jobs=-1)]: Done  17 out of  24 | elapsed: 13.0min remaining:  5.4min\n","[Parallel(n_jobs=-1)]: Done  18 out of  24 | elapsed: 13.2min remaining:  4.4min\n","[Parallel(n_jobs=-1)]: Done  19 out of  24 | elapsed: 13.3min remaining:  3.5min\n","[Parallel(n_jobs=-1)]: Done  20 out of  24 | elapsed: 13.4min remaining:  2.7min\n","[Parallel(n_jobs=-1)]: Done  21 out of  24 | elapsed: 13.5min remaining:  1.9min\n","[Parallel(n_jobs=-1)]: Done  22 out of  24 | elapsed: 13.6min remaining:  1.2min\n","[Parallel(n_jobs=-1)]: Done  24 out of  24 | elapsed: 13.8min remaining:    0.0s\n","[Parallel(n_jobs=-1)]: Done  24 out of  24 | elapsed: 13.8min finished\n","Macro-F1: 0.761 (0.008)\n"]}],"source":["n_scores = cross_val_score(calibrated, X_res, y_res, scoring='f1_macro', cv=cv, n_jobs=-1, verbose=100, error_score='raise')\n","print('Macro-F1: %.3f (%.3f)' % (np.mean(n_scores), np.std(n_scores)))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["X_trans = svd.transform(X_valid)\n","calibrated.fit(X_res, y_res)\n","y_pred = calibrated.predict(X_trans)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy: 0.6759744037230948\n","Macro-F1 score: 0.661960747208876\n"]},{"data":{"text/plain":["0.661960747208876"]},"execution_count":176,"metadata":{},"output_type":"execute_result"}],"source":["score(y_valid, y_pred)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"data":{"image/png":"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","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"needs_background":"light"},"output_type":"display_data"}],"source":["disp = CalibrationDisplay.from_estimator(calibrated, X_trans, y_valid)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.model_selection import cross_val_score\n","from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n","from hyperopt.pyll import scope\n","import time \n","\n","def hyperopt(param_space, model, X_train, y_train, X_test, y_test, num_eval):\n","    \n","    start = time.time()\n","    \n","    def objective_function(params):\n","        clf = model(**params)\n","        cv = RepeatedStratifiedKFold(n_splits=8, n_repeats=3, random_state=100)\n","        n_scores = cross_val_score(clf, X_train, y_train, scoring='f1_macro', cv=cv, n_jobs=-1, verbose=100, error_score='raise')\n","        print('Macro-F1: %.3f (%.3f)' % (np.mean(n_scores), np.std(n_scores)))\n","        clf.fit(X_train, y_train)\n","        result = score(y_test, clf.predict(X_test))\n","        return {'loss': -result, 'status': STATUS_OK}\n","\n","    trials = Trials()\n","    best_param = fmin(objective_function, \n","                      param_space, \n","                      algo=tpe.suggest, \n","                      max_evals=num_eval, \n","                      trials=trials)\n","    loss = [x['result']['loss'] for x in trials.trials]\n","    \n","    print(\"\")\n","    print(\"##### Results\")\n","    print(\"Score best parameters: \", min(loss)*-1)\n","    print(\"Best parameters: \", best_param)\n","    print(\"Time elapsed: \", time.time() - start)\n","    print(\"Parameter combinations evaluated: \", num_eval)\n","    \n","    return trials, best_param"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["LGBM_params = {\n","    'boosting_type': hp.choice('boosting_type', ['gbdt', 'dart']),\n","    'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(1)),\n","    'max_depth': scope.int(hp.quniform('max_depth', 5, 50, 1)),\n","    'num_leaves': scope.int(hp.quniform('num_leaves', 5, 50, 1)),\n","\n","    'colsample_bytree': hp.uniform('colsample_bytree', 0.4, 1.0),\n","    'subsample': hp.uniform('subsample', 0.4, 1.0),\n","\n","    'min_child_samples': scope.int(hp.quniform('min_child_samples', 10, 200, 10)),\n","    'min_child_weight': hp.loguniform('min_child_weight', np.log(1e-3), np.log(1)),\n","    'min_split_gain': hp.loguniform('min_split_gain', np.log(1e-8), np.log(1)),\n","    \n","    'max_delta_step': hp.uniform('min_delta_step', 0, 2),\n","    'n_estimators': scope.int(hp.quniform('n_estimators', 5, 100, 1)),\n","    # 'scale_pos_weight':hp.uniform('scale_pos_weight', 1.0, 5.0),\n","    # 'reg_lambda': hp.uniform('reg_lambda', 0.0, 1.0),\n","    # 'reg_alpha': hp.uniform('reg_alpha', 0.0, 1.0),\n","    'random_state': hp.choice('random_state', ['100']),\n","    'n_jobs': hp.choice('n_jobs', ['-1'])\n","}\n","# LGBM(boosting_type='gbdt', num_leaves=40, max_depth=-1, learning_rate=0.2, n_estimators=100, class_weight='balanced', reg_lambda=0, random_state=100, reg_alpha=0, n_jobs=-1)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n","\n","[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:  3.5min\n","\n","[Parallel(n_jobs=-1)]: Done   2 out of  24 | elapsed:  3.5min remaining: 38.3min\n","\n","[Parallel(n_jobs=-1)]: Done   3 out of  24 | elapsed:  3.5min remaining: 24.4min\n","\n","[Parallel(n_jobs=-1)]: Done   4 out of  24 | elapsed:  3.5min remaining: 17.5min\n","\n","[Parallel(n_jobs=-1)]: Done   5 out of  24 | elapsed:  3.5min remaining: 13.4min\n","\n","[Parallel(n_jobs=-1)]: Done   6 out of  24 | elapsed:  3.5min remaining: 10.6min\n","\n","[Parallel(n_jobs=-1)]: Done   7 out of  24 | elapsed:  3.5min remaining:  8.6min\n","\n","[Parallel(n_jobs=-1)]: Done   8 out of  24 | elapsed:  3.5min remaining:  7.1min\n","\n","[Parallel(n_jobs=-1)]: Done   9 out of  24 | elapsed:  3.5min remaining:  5.9min\n","\n","[Parallel(n_jobs=-1)]: Done  10 out of  24 | elapsed:  3.6min remaining:  5.1min\n","\n","[Parallel(n_jobs=-1)]: Done  11 out of  24 | elapsed:  3.7min remaining:  4.3min\n","\n","[Parallel(n_jobs=-1)]: Done  12 out of  24 | elapsed:  3.7min remaining:  3.7min\n","\n","[Parallel(n_jobs=-1)]: Done  13 out of  24 | elapsed:  6.8min remaining:  5.7min\n","\n","[Parallel(n_jobs=-1)]: Done  14 out of  24 | elapsed:  6.8min remaining:  4.8min\n","\n","[Parallel(n_jobs=-1)]: Done  15 out of  24 | elapsed:  6.8min remaining:  4.1min\n","\n","[Parallel(n_jobs=-1)]: Done  16 out of  24 | elapsed:  6.8min remaining:  3.4min\n","\n","[Parallel(n_jobs=-1)]: Done  17 out of  24 | elapsed:  6.9min remaining:  2.8min\n","\n","[Parallel(n_jobs=-1)]: Done  18 out of  24 | elapsed:  6.9min remaining:  2.3min\n","\n","[Parallel(n_jobs=-1)]: Done  19 out of  24 | elapsed:  6.9min remaining:  1.8min\n","\n","[Parallel(n_jobs=-1)]: Done  20 out of  24 | elapsed:  6.9min remaining:  1.4min\n","\n","[Parallel(n_jobs=-1)]: Done  21 out of  24 | elapsed:  6.9min remaining:   59.3s\n","\n","[Parallel(n_jobs=-1)]: Done  22 out of  24 | elapsed:  6.9min remaining:   37.7s\n","\n","[Parallel(n_jobs=-1)]: Done  24 out of  24 | elapsed:  7.0min remaining:    0.0s\n","\n","[Parallel(n_jobs=-1)]: Done  24 out of  24 | elapsed:  7.0min finished\n","\n","Macro-F1: 0.500 (0.007)                               \n","[LightGBM] [Warning] Unknown parameter: colsample_by_tree\n","Accuracy: 0.45782431646305993                         \n","Macro-F1 score: 0.42792317976141503                   \n","[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.         \n","\n","[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:  2.2min                        \n","\n","[Parallel(n_jobs=-1)]: Done   2 out of  24 | elapsed:  2.2min remaining: 24.4min     \n","\n","[Parallel(n_jobs=-1)]: Done   3 out of  24 | elapsed:  2.2min remaining: 15.7min     \n","\n","[Parallel(n_jobs=-1)]: Done   4 out of  24 | elapsed:  2.2min remaining: 11.2min     \n","\n","[Parallel(n_jobs=-1)]: Done   5 out of  24 | elapsed:  2.2min remaining:  8.6min     \n","\n","[Parallel(n_jobs=-1)]: Done   6 out of  24 | elapsed:  2.3min remaining:  6.8min     \n","\n","[Parallel(n_jobs=-1)]: Done   7 out of  24 | elapsed:  2.3min remaining:  5.5min     \n","\n","[Parallel(n_jobs=-1)]: Done   8 out of  24 | elapsed:  2.3min remaining:  4.5min     \n","\n","[Parallel(n_jobs=-1)]: Done   9 out of  24 | elapsed:  2.3min remaining:  3.8min     \n","\n","[Parallel(n_jobs=-1)]: Done  10 out of  24 | elapsed:  2.3min remaining:  3.2min     \n","\n","[Parallel(n_jobs=-1)]: Done  11 out of  24 | elapsed:  2.3min remaining:  2.7min     \n","\n","[Parallel(n_jobs=-1)]: Done  12 out of  24 | elapsed:  2.3min remaining:  2.3min     \n","\n","[Parallel(n_jobs=-1)]: Done  13 out of  24 | elapsed:  4.5min remaining:  3.8min     \n","\n","[Parallel(n_jobs=-1)]: Done  14 out of  24 | elapsed:  4.5min remaining:  3.2min     \n","\n","[Parallel(n_jobs=-1)]: Done  15 out of  24 | elapsed:  4.6min remaining:  2.7min     \n","\n","[Parallel(n_jobs=-1)]: Done  16 out of  24 | elapsed:  4.6min remaining:  2.3min     \n","\n","[Parallel(n_jobs=-1)]: Done  17 out of  24 | elapsed:  4.6min remaining:  1.9min     \n","\n","[Parallel(n_jobs=-1)]: Done  18 out of  24 | elapsed:  4.6min remaining:  1.5min     \n","\n","[Parallel(n_jobs=-1)]: Done  19 out of  24 | elapsed:  4.6min remaining:  1.2min     \n","\n","[Parallel(n_jobs=-1)]: Done  20 out of  24 | elapsed:  4.6min remaining:   55.6s     \n","\n","[Parallel(n_jobs=-1)]: Done  21 out of  24 | elapsed:  4.6min remaining:   39.7s     \n","\n","[Parallel(n_jobs=-1)]: Done  22 out of  24 | elapsed:  4.7min remaining:   25.3s     \n","\n","[Parallel(n_jobs=-1)]: Done  24 out of  24 | elapsed:  4.7min remaining:    0.0s     \n","\n","[Parallel(n_jobs=-1)]: Done  24 out of  24 | elapsed:  4.7min finished               \n","\n","Macro-F1: 0.754 (0.009)                                                              \n","[LightGBM] [Warning] Unknown parameter: colsample_by_tree                            \n","Accuracy: 0.6614310645724258                                                         \n","Macro-F1 score: 0.6443235856247369                                                   \n","[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.         \n","\n","[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:  1.8min                       \n","\n","[Parallel(n_jobs=-1)]: Done   2 out of  24 | elapsed:  1.8min remaining: 20.3min    \n","\n","[Parallel(n_jobs=-1)]: Done   3 out of  24 | elapsed:  1.9min remaining: 13.0min    \n","\n","[Parallel(n_jobs=-1)]: Done   4 out of  24 | elapsed:  1.9min remaining:  9.4min    \n","\n","[Parallel(n_jobs=-1)]: Done   5 out of  24 | elapsed:  1.9min remaining:  7.1min    \n","\n","[Parallel(n_jobs=-1)]: Done   6 out of  24 | elapsed:  1.9min remaining:  5.7min    \n","\n","[Parallel(n_jobs=-1)]: Done   7 out of  24 | elapsed:  1.9min remaining:  4.6min    \n","\n","[Parallel(n_jobs=-1)]: Done   8 out of  24 | elapsed:  1.9min remaining:  3.9min    \n","\n","[Parallel(n_jobs=-1)]: Done   9 out of  24 | elapsed:  1.9min remaining:  3.2min    \n","\n","[Parallel(n_jobs=-1)]: Done  10 out of  24 | elapsed:  2.0min remaining:  2.7min    \n","\n","[Parallel(n_jobs=-1)]: Done  11 out of  24 | elapsed:  2.0min remaining:  2.3min    \n","\n","[Parallel(n_jobs=-1)]: Done  12 out of  24 | elapsed:  2.0min remaining:  2.0min    \n","\n","[Parallel(n_jobs=-1)]: Done  13 out of  24 | elapsed:  3.5min remaining:  2.9min    \n","\n","[Parallel(n_jobs=-1)]: Done  14 out of  24 | elapsed:  3.6min remaining:  2.6min    \n","\n","[Parallel(n_jobs=-1)]: Done  15 out of  24 | elapsed:  3.7min remaining:  2.2min    \n","\n","[Parallel(n_jobs=-1)]: Done  16 out of  24 | elapsed:  3.7min remaining:  1.8min    \n","\n","[Parallel(n_jobs=-1)]: Done  17 out of  24 | elapsed:  3.7min remaining:  1.5min    \n","\n","[Parallel(n_jobs=-1)]: Done  18 out of  24 | elapsed:  3.7min remaining:  1.2min    \n","\n","[Parallel(n_jobs=-1)]: Done  19 out of  24 | elapsed:  3.7min remaining:   58.6s    \n","\n","[Parallel(n_jobs=-1)]: Done  20 out of  24 | elapsed:  3.7min remaining:   44.7s    \n","\n","[Parallel(n_jobs=-1)]: Done  21 out of  24 | elapsed:  3.7min remaining:   31.9s    \n","\n","[Parallel(n_jobs=-1)]: Done  22 out of  24 | elapsed:  3.8min remaining:   20.4s    \n","\n","[Parallel(n_jobs=-1)]: Done  24 out of  24 | elapsed:  3.8min remaining:    0.0s    \n","\n","[Parallel(n_jobs=-1)]: Done  24 out of  24 | elapsed:  3.8min finished              \n","\n","Macro-F1: 0.714 (0.007)                                                             \n","[LightGBM] [Warning] Unknown parameter: colsample_by_tree                           \n","Accuracy: 0.6218731820826061                                                        \n","Macro-F1 score: 0.612381183653646                                                   \n","[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.        \n","\n","[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:  1.9min                       \n","\n","[Parallel(n_jobs=-1)]: Done   2 out of  24 | elapsed:  1.9min remaining: 20.5min    \n","\n","[Parallel(n_jobs=-1)]: Done   3 out of  24 | elapsed:  1.9min remaining: 13.1min    \n","\n","[Parallel(n_jobs=-1)]: Done   4 out of  24 | elapsed:  1.9min remaining:  9.4min    \n","\n","[Parallel(n_jobs=-1)]: Done   5 out of  24 | elapsed:  1.9min remaining:  7.2min    \n","\n","[Parallel(n_jobs=-1)]: Done   6 out of  24 | elapsed:  1.9min remaining:  5.7min    \n","\n","[Parallel(n_jobs=-1)]: Done   7 out of  24 | elapsed:  1.9min remaining:  4.6min    \n","\n","[Parallel(n_jobs=-1)]: Done   8 out of  24 | elapsed:  1.9min remaining:  3.8min    \n","\n","[Parallel(n_jobs=-1)]: Done   9 out of  24 | elapsed:  1.9min remaining:  3.2min    \n","\n","[Parallel(n_jobs=-1)]: Done  10 out of  24 | elapsed:  2.0min remaining:  2.7min    \n","\n","[Parallel(n_jobs=-1)]: Done  11 out of  24 | elapsed:  2.0min remaining:  2.3min    \n","\n","[Parallel(n_jobs=-1)]: Done  12 out of  24 | elapsed:  2.0min remaining:  2.0min    \n","\n","[Parallel(n_jobs=-1)]: Done  13 out of  24 | elapsed:  3.7min remaining:  3.1min    \n","\n","[Parallel(n_jobs=-1)]: Done  14 out of  24 | elapsed:  3.7min remaining:  2.6min    \n","\n","[Parallel(n_jobs=-1)]: Done  15 out of  24 | elapsed:  3.7min remaining:  2.2min    \n","\n","[Parallel(n_jobs=-1)]: Done  16 out of  24 | elapsed:  3.7min remaining:  1.9min    \n","\n","[Parallel(n_jobs=-1)]: Done  17 out of  24 | elapsed:  3.7min remaining:  1.5min    \n","\n","[Parallel(n_jobs=-1)]: Done  18 out of  24 | elapsed:  3.8min remaining:  1.3min    \n","\n","[Parallel(n_jobs=-1)]: Done  19 out of  24 | elapsed:  3.8min remaining:   59.6s    \n","\n","[Parallel(n_jobs=-1)]: Done  20 out of  24 | elapsed:  3.8min remaining:   45.3s    \n","\n","[Parallel(n_jobs=-1)]: Done  21 out of  24 | elapsed:  3.8min remaining:   32.5s    \n","\n","[Parallel(n_jobs=-1)]: Done  22 out of  24 | elapsed:  3.8min remaining:   20.9s    \n","\n","[Parallel(n_jobs=-1)]: Done  24 out of  24 | elapsed:  3.9min remaining:    0.0s    \n","\n","[Parallel(n_jobs=-1)]: Done  24 out of  24 | elapsed:  3.9min finished              \n","\n","Macro-F1: 0.690 (0.007)                                                             \n","[LightGBM] [Warning] Unknown parameter: colsample_by_tree                           \n","Accuracy: 0.6259453170447935                                                        \n","Macro-F1 score: 0.6200569579243529                                                  \n","[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.        \n","\n","[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:  1.9min                       \n","\n","[Parallel(n_jobs=-1)]: Done   2 out of  24 | elapsed:  1.9min remaining: 21.4min    \n","\n","[Parallel(n_jobs=-1)]: Done   3 out of  24 | elapsed:  1.9min remaining: 13.6min    \n","\n","[Parallel(n_jobs=-1)]: Done   4 out of  24 | elapsed:  2.0min remaining:  9.8min    \n","\n","[Parallel(n_jobs=-1)]: Done   5 out of  24 | elapsed:  2.0min remaining:  7.4min    \n","\n","[Parallel(n_jobs=-1)]: Done   6 out of  24 | elapsed:  2.0min remaining:  5.9min    \n","\n","[Parallel(n_jobs=-1)]: Done   7 out of  24 | elapsed:  2.0min remaining:  4.8min    \n","\n","[Parallel(n_jobs=-1)]: Done   8 out of  24 | elapsed:  2.0min remaining:  4.0min    \n","\n","[Parallel(n_jobs=-1)]: Done   9 out of  24 | elapsed:  2.0min remaining:  3.3min    \n","\n","[Parallel(n_jobs=-1)]: Done  10 out of  24 | elapsed:  2.0min remaining:  2.8min    \n","\n","[Parallel(n_jobs=-1)]: Done  11 out of  24 | elapsed:  2.0min remaining:  2.4min    \n","\n","[Parallel(n_jobs=-1)]: Done  12 out of  24 | elapsed:  2.1min remaining:  2.1min    \n","\n","[Parallel(n_jobs=-1)]: Done  13 out of  24 | elapsed:  3.9min remaining:  3.3min    \n","\n","[Parallel(n_jobs=-1)]: Done  14 out of  24 | elapsed:  3.9min remaining:  2.8min    \n","\n","[Parallel(n_jobs=-1)]: Done  15 out of  24 | elapsed:  3.9min remaining:  2.3min    \n","\n","[Parallel(n_jobs=-1)]: Done  16 out of  24 | elapsed:  3.9min remaining:  2.0min    \n","\n","[Parallel(n_jobs=-1)]: Done  17 out of  24 | elapsed:  3.9min remaining:  1.6min    \n","\n","[Parallel(n_jobs=-1)]: Done  18 out of  24 | elapsed:  3.9min remaining:  1.3min    \n","\n","[Parallel(n_jobs=-1)]: Done  19 out of  24 | elapsed:  3.9min remaining:  1.0min    \n","\n","[Parallel(n_jobs=-1)]: Done  20 out of  24 | elapsed:  4.0min remaining:   47.4s    \n","\n","[Parallel(n_jobs=-1)]: Done  21 out of  24 | elapsed:  4.0min remaining:   34.0s    \n","\n","[Parallel(n_jobs=-1)]: Done  22 out of  24 | elapsed:  4.0min remaining:   21.6s    \n","\n","[Parallel(n_jobs=-1)]: Done  24 out of  24 | elapsed:  4.0min remaining:    0.0s    \n","\n","[Parallel(n_jobs=-1)]: Done  24 out of  24 | elapsed:  4.0min finished              \n","\n","Macro-F1: 0.475 (0.008)                                                             \n","[LightGBM] [Warning] Unknown parameter: colsample_by_tree                           \n","Accuracy: 0.4316463059918557                                                        \n","Macro-F1 score: 0.38656588033114114                                                 \n","[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.        \n","\n","[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:  1.6min                       \n","\n","[Parallel(n_jobs=-1)]: Done   2 out of  24 | elapsed:  1.6min remaining: 17.6min    \n","\n","[Parallel(n_jobs=-1)]: Done   3 out of  24 | elapsed:  1.6min remaining: 11.3min    \n","\n","[Parallel(n_jobs=-1)]: Done   4 out of  24 | elapsed:  1.6min remaining:  8.0min    \n","\n","[Parallel(n_jobs=-1)]: Done   5 out of  24 | elapsed:  1.6min remaining:  6.1min    \n","\n","[Parallel(n_jobs=-1)]: Done   6 out of  24 | elapsed:  1.6min remaining:  4.8min    \n","\n","[Parallel(n_jobs=-1)]: Done   7 out of  24 | elapsed:  1.6min remaining:  3.9min    \n","\n","[Parallel(n_jobs=-1)]: Done   8 out of  24 | elapsed:  1.6min remaining:  3.2min    \n","\n","[Parallel(n_jobs=-1)]: Done   9 out of  24 | elapsed:  1.6min remaining:  2.7min    \n","\n","[Parallel(n_jobs=-1)]: Done  10 out of  24 | elapsed:  1.6min remaining:  2.3min    \n","\n","[Parallel(n_jobs=-1)]: Done  11 out of  24 | elapsed:  1.6min remaining:  1.9min    \n","\n","[Parallel(n_jobs=-1)]: Done  12 out of  24 | elapsed:  1.6min remaining:  1.6min    \n","\n","[Parallel(n_jobs=-1)]: Done  13 out of  24 | elapsed:  3.2min remaining:  2.7min    \n","\n","[Parallel(n_jobs=-1)]: Done  14 out of  24 | elapsed:  3.3min remaining:  2.4min    \n","\n","[Parallel(n_jobs=-1)]: Done  15 out of  24 | elapsed:  3.3min remaining:  2.0min    \n","\n","[Parallel(n_jobs=-1)]: Done  16 out of  24 | elapsed:  3.3min remaining:  1.7min    \n","\n","[Parallel(n_jobs=-1)]: Done  17 out of  24 | elapsed:  3.3min remaining:  1.4min    \n","\n","[Parallel(n_jobs=-1)]: Done  18 out of  24 | elapsed:  3.3min remaining:  1.1min    \n","\n","[Parallel(n_jobs=-1)]: Done  19 out of  24 | elapsed:  3.3min remaining:   52.5s    \n","\n","[Parallel(n_jobs=-1)]: Done  20 out of  24 | elapsed:  3.3min remaining:   39.9s    \n","\n","[Parallel(n_jobs=-1)]: Done  21 out of  24 | elapsed:  3.3min remaining:   28.5s    \n","\n","[Parallel(n_jobs=-1)]: Done  22 out of  24 | elapsed:  3.3min remaining:   18.1s    \n","\n","[Parallel(n_jobs=-1)]: Done  24 out of  24 | elapsed:  3.3min remaining:    0.0s    \n","\n","[Parallel(n_jobs=-1)]: Done  24 out of  24 | elapsed:  3.3min finished              \n","\n","Macro-F1: 0.363 (0.006)                                                             \n","[LightGBM] [Warning] Unknown parameter: colsample_by_tree                           \n","Accuracy: 0.3787085514834206                                                        \n","Macro-F1 score: 0.29542691351201994                                                 \n","[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.        \n","\n","[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:  1.4min                     \n","\n","[Parallel(n_jobs=-1)]: Done   2 out of  24 | elapsed:  1.4min remaining: 15.9min  \n","\n","[Parallel(n_jobs=-1)]: Done   3 out of  24 | elapsed:  1.4min remaining: 10.1min  \n","\n","[Parallel(n_jobs=-1)]: Done   4 out of  24 | elapsed:  1.4min remaining:  7.3min  \n","\n","[Parallel(n_jobs=-1)]: Done   5 out of  24 | elapsed:  1.5min remaining:  5.5min  \n","\n","[Parallel(n_jobs=-1)]: Done   6 out of  24 | elapsed:  1.5min remaining:  4.4min  \n","\n","[Parallel(n_jobs=-1)]: Done   7 out of  24 | elapsed:  1.5min remaining:  3.5min  \n","\n","[Parallel(n_jobs=-1)]: Done   8 out of  24 | elapsed:  1.5min remaining:  2.9min  \n","\n","[Parallel(n_jobs=-1)]: Done   9 out of  24 | elapsed:  1.5min remaining:  2.4min  \n","\n","[Parallel(n_jobs=-1)]: Done  10 out of  24 | elapsed:  1.5min remaining:  2.0min  \n","\n","[Parallel(n_jobs=-1)]: Done  11 out of  24 | elapsed:  1.5min remaining:  1.7min  \n","\n","[Parallel(n_jobs=-1)]: Done  12 out of  24 | elapsed:  1.5min remaining:  1.5min  \n","\n","[Parallel(n_jobs=-1)]: Done  13 out of  24 | elapsed:  2.9min remaining:  2.4min  \n","\n","[Parallel(n_jobs=-1)]: Done  14 out of  24 | elapsed:  2.9min remaining:  2.0min  \n","\n","[Parallel(n_jobs=-1)]: Done  15 out of  24 | elapsed:  2.9min remaining:  1.7min  \n","\n","[Parallel(n_jobs=-1)]: Done  16 out of  24 | elapsed:  2.9min remaining:  1.4min  \n","\n","[Parallel(n_jobs=-1)]: Done  17 out of  24 | elapsed:  2.9min remaining:  1.2min  \n","\n","[Parallel(n_jobs=-1)]: Done  18 out of  24 | elapsed:  2.9min remaining:   57.8s  \n","\n","[Parallel(n_jobs=-1)]: Done  19 out of  24 | elapsed:  2.9min remaining:   45.7s  \n","\n","[Parallel(n_jobs=-1)]: Done  20 out of  24 | elapsed:  2.9min remaining:   34.8s  \n","\n","[Parallel(n_jobs=-1)]: Done  21 out of  24 | elapsed:  2.9min remaining:   24.8s  \n","\n","[Parallel(n_jobs=-1)]: Done  22 out of  24 | elapsed:  2.9min remaining:   15.8s  \n","\n","[Parallel(n_jobs=-1)]: Done  24 out of  24 | elapsed:  2.9min remaining:    0.0s  \n","\n","[Parallel(n_jobs=-1)]: Done  24 out of  24 | elapsed:  2.9min finished            \n","\n","Macro-F1: 0.617 (0.006)                                                           \n","[LightGBM] [Warning] Unknown parameter: colsample_by_tree                         \n","Accuracy: 0.5293775450843514                                                      \n","Macro-F1 score: 0.5248390955524873                                                \n","[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.      \n","\n","[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:  2.1min                     \n","\n","[Parallel(n_jobs=-1)]: Done   2 out of  24 | elapsed:  2.1min remaining: 22.8min  \n","\n","[Parallel(n_jobs=-1)]: Done   3 out of  24 | elapsed:  2.1min remaining: 14.5min  \n","\n","[Parallel(n_jobs=-1)]: Done   4 out of  24 | elapsed:  2.1min remaining: 10.4min  \n","\n","[Parallel(n_jobs=-1)]: Done   5 out of  24 | elapsed:  2.1min remaining:  7.9min  \n","\n","[Parallel(n_jobs=-1)]: Done   6 out of  24 | elapsed:  2.1min remaining:  6.3min  \n","\n","[Parallel(n_jobs=-1)]: Done   7 out of  24 | elapsed:  2.1min remaining:  5.1min  \n","\n","[Parallel(n_jobs=-1)]: Done   8 out of  24 | elapsed:  2.1min remaining:  4.2min  \n","\n","[Parallel(n_jobs=-1)]: Done   9 out of  24 | elapsed:  2.1min remaining:  3.5min  \n","\n","[Parallel(n_jobs=-1)]: Done  10 out of  24 | elapsed:  2.1min remaining:  3.0min  \n","\n","[Parallel(n_jobs=-1)]: Done  11 out of  24 | elapsed:  2.1min remaining:  2.5min  \n","\n","[Parallel(n_jobs=-1)]: Done  12 out of  24 | elapsed:  2.2min remaining:  2.2min  \n","\n","[Parallel(n_jobs=-1)]: Done  13 out of  24 | elapsed:  4.1min remaining:  3.5min  \n","\n","[Parallel(n_jobs=-1)]: Done  14 out of  24 | elapsed:  4.1min remaining:  2.9min  \n","\n","[Parallel(n_jobs=-1)]: Done  15 out of  24 | elapsed:  4.2min remaining:  2.5min  \n","\n","[Parallel(n_jobs=-1)]: Done  16 out of  24 | elapsed:  4.2min remaining:  2.1min  \n","\n","[Parallel(n_jobs=-1)]: Done  17 out of  24 | elapsed:  4.2min remaining:  1.7min  \n","\n","[Parallel(n_jobs=-1)]: Done  18 out of  24 | elapsed:  4.2min remaining:  1.4min  \n","\n","[Parallel(n_jobs=-1)]: Done  19 out of  24 | elapsed:  4.2min remaining:  1.1min  \n","\n","[Parallel(n_jobs=-1)]: Done  20 out of  24 | elapsed:  4.2min remaining:   50.0s  \n","\n","[Parallel(n_jobs=-1)]: Done  21 out of  24 | elapsed:  4.2min remaining:   35.9s  \n","\n","[Parallel(n_jobs=-1)]: Done  22 out of  24 | elapsed:  4.2min remaining:   22.8s  \n","\n","[Parallel(n_jobs=-1)]: Done  24 out of  24 | elapsed:  4.2min remaining:    0.0s  \n","\n","[Parallel(n_jobs=-1)]: Done  24 out of  24 | elapsed:  4.2min finished            \n","\n","Macro-F1: 0.419 (0.009)                                                           \n","[LightGBM] [Warning] Unknown parameter: colsample_by_tree                         \n","Accuracy: 0.41535776614310643                                                     \n","Macro-F1 score: 0.3617944429852414                                                \n","[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.      \n","\n","[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:  1.5min                     \n","\n","[Parallel(n_jobs=-1)]: Done   2 out of  24 | elapsed:  1.5min remaining: 16.9min  \n","\n","[Parallel(n_jobs=-1)]: Done   3 out of  24 | elapsed:  1.5min remaining: 10.8min  \n","\n","[Parallel(n_jobs=-1)]: Done   4 out of  24 | elapsed:  1.5min remaining:  7.7min  \n","\n","[Parallel(n_jobs=-1)]: Done   5 out of  24 | elapsed:  1.5min remaining:  5.9min  \n","\n","[Parallel(n_jobs=-1)]: Done   6 out of  24 | elapsed:  1.5min remaining:  4.6min  \n","\n","[Parallel(n_jobs=-1)]: Done   7 out of  24 | elapsed:  1.6min remaining:  3.8min  \n","\n","[Parallel(n_jobs=-1)]: Done   8 out of  24 | elapsed:  1.6min remaining:  3.1min  \n","\n","[Parallel(n_jobs=-1)]: Done   9 out of  24 | elapsed:  1.6min remaining:  2.6min  \n","\n","[Parallel(n_jobs=-1)]: Done  10 out of  24 | elapsed:  1.6min remaining:  2.2min  \n","\n","[Parallel(n_jobs=-1)]: Done  11 out of  24 | elapsed:  1.6min remaining:  1.9min  \n","\n","[Parallel(n_jobs=-1)]: Done  12 out of  24 | elapsed:  1.6min remaining:  1.6min  \n","\n","[Parallel(n_jobs=-1)]: Done  13 out of  24 | elapsed:  3.0min remaining:  2.5min  \n","\n","[Parallel(n_jobs=-1)]: Done  14 out of  24 | elapsed:  3.0min remaining:  2.2min  \n","\n","[Parallel(n_jobs=-1)]: Done  15 out of  24 | elapsed:  3.1min remaining:  1.8min  \n","\n","[Parallel(n_jobs=-1)]: Done  16 out of  24 | elapsed:  3.1min remaining:  1.5min  \n","\n","[Parallel(n_jobs=-1)]: Done  17 out of  24 | elapsed:  3.1min remaining:  1.3min  \n","\n","[Parallel(n_jobs=-1)]: Done  18 out of  24 | elapsed:  3.1min remaining:  1.0min  \n","\n","[Parallel(n_jobs=-1)]: Done  19 out of  24 | elapsed:  3.1min remaining:   48.5s  \n","\n","[Parallel(n_jobs=-1)]: Done  20 out of  24 | elapsed:  3.1min remaining:   36.9s  \n","\n","[Parallel(n_jobs=-1)]: Done  21 out of  24 | elapsed:  3.1min remaining:   26.5s  \n","\n","[Parallel(n_jobs=-1)]: Done  22 out of  24 | elapsed:  3.1min remaining:   16.8s  \n","\n","[Parallel(n_jobs=-1)]: Done  24 out of  24 | elapsed:  3.1min remaining:    0.0s  \n","\n","[Parallel(n_jobs=-1)]: Done  24 out of  24 | elapsed:  3.1min finished            \n","\n","Macro-F1: 0.335 (0.003)                                                           \n","[LightGBM] [Warning] Unknown parameter: colsample_by_tree                         \n","Accuracy: 0.3624200116346713                                                      \n","Macro-F1 score: 0.2660119555935098                                                \n","[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.      \n","\n","[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:  1.2min                     \n","\n","[Parallel(n_jobs=-1)]: Done   2 out of  24 | elapsed:  1.2min remaining: 13.5min  \n","\n","[Parallel(n_jobs=-1)]: Done   3 out of  24 | elapsed:  1.2min remaining:  8.6min  \n","\n","[Parallel(n_jobs=-1)]: Done   4 out of  24 | elapsed:  1.2min remaining:  6.2min  \n","\n","[Parallel(n_jobs=-1)]: Done   5 out of  24 | elapsed:  1.2min remaining:  4.7min  \n","\n","[Parallel(n_jobs=-1)]: Done   6 out of  24 | elapsed:  1.3min remaining:  3.8min  \n","\n","[Parallel(n_jobs=-1)]: Done   7 out of  24 | elapsed:  1.3min remaining:  3.0min  \n","\n","[Parallel(n_jobs=-1)]: Done   8 out of  24 | elapsed:  1.3min remaining:  2.5min  \n","\n","[Parallel(n_jobs=-1)]: Done   9 out of  24 | elapsed:  1.3min remaining:  2.1min  \n","\n","[Parallel(n_jobs=-1)]: Done  10 out of  24 | elapsed:  1.3min remaining:  1.8min  \n","\n","[Parallel(n_jobs=-1)]: Done  11 out of  24 | elapsed:  1.3min remaining:  1.5min  \n","\n","[Parallel(n_jobs=-1)]: Done  12 out of  24 | elapsed:  1.3min remaining:  1.3min  \n","\n","[Parallel(n_jobs=-1)]: Done  13 out of  24 | elapsed:  2.4min remaining:  2.0min  \n","\n","[Parallel(n_jobs=-1)]: Done  14 out of  24 | elapsed:  2.5min remaining:  1.8min  \n","\n","[Parallel(n_jobs=-1)]: Done  15 out of  24 | elapsed:  2.5min remaining:  1.5min  \n","\n","[Parallel(n_jobs=-1)]: Done  16 out of  24 | elapsed:  2.5min remaining:  1.2min  \n","\n","[Parallel(n_jobs=-1)]: Done  17 out of  24 | elapsed:  2.5min remaining:  1.0min  \n","\n","[Parallel(n_jobs=-1)]: Done  18 out of  24 | elapsed:  2.5min remaining:   50.0s  \n","\n","[Parallel(n_jobs=-1)]: Done  19 out of  24 | elapsed:  2.5min remaining:   39.5s  \n","\n","[Parallel(n_jobs=-1)]: Done  20 out of  24 | elapsed:  2.5min remaining:   30.1s  \n","\n","[Parallel(n_jobs=-1)]: Done  21 out of  24 | elapsed:  2.5min remaining:   21.5s  \n","\n","[Parallel(n_jobs=-1)]: Done  22 out of  24 | elapsed:  2.5min remaining:   13.6s  \n","\n","[Parallel(n_jobs=-1)]: Done  24 out of  24 | elapsed:  2.5min remaining:    0.0s  \n","\n","[Parallel(n_jobs=-1)]: Done  24 out of  24 | elapsed:  2.5min finished            \n","\n","Macro-F1: 0.500 (0.007)                                                           \n","[LightGBM] [Warning] Unknown parameter: colsample_by_tree                         \n","Accuracy: 0.4584060500290867                                                      \n","Macro-F1 score: 0.4311922333961832                                                \n","[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.       \n","\n","[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:  2.3min                      \n","\n","[Parallel(n_jobs=-1)]: Done   2 out of  24 | elapsed:  2.3min remaining: 25.8min   \n","\n","[Parallel(n_jobs=-1)]: Done   3 out of  24 | elapsed:  2.4min remaining: 16.5min   \n","\n","[Parallel(n_jobs=-1)]: Done   4 out of  24 | elapsed:  2.4min remaining: 11.8min   \n","\n","[Parallel(n_jobs=-1)]: Done   5 out of  24 | elapsed:  2.4min remaining:  9.0min   \n","\n","[Parallel(n_jobs=-1)]: Done   6 out of  24 | elapsed:  2.4min remaining:  7.1min   \n","\n","[Parallel(n_jobs=-1)]: Done   7 out of  24 | elapsed:  2.4min remaining:  5.7min   \n","\n","[Parallel(n_jobs=-1)]: Done   8 out of  24 | elapsed:  2.4min remaining:  4.7min   \n","\n","[Parallel(n_jobs=-1)]: Done   9 out of  24 | elapsed:  2.4min remaining:  4.0min   \n","\n","[Parallel(n_jobs=-1)]: Done  10 out of  24 | elapsed:  2.4min remaining:  3.3min   \n","\n","[Parallel(n_jobs=-1)]: Done  11 out of  24 | elapsed:  2.4min remaining:  2.8min   \n","\n","[Parallel(n_jobs=-1)]: Done  12 out of  24 | elapsed:  2.4min remaining:  2.4min   \n","\n","[Parallel(n_jobs=-1)]: Done  13 out of  24 | elapsed:  4.6min remaining:  3.9min   \n","\n","[Parallel(n_jobs=-1)]: Done  14 out of  24 | elapsed:  4.7min remaining:  3.4min   \n","\n","[Parallel(n_jobs=-1)]: Done  15 out of  24 | elapsed:  4.7min remaining:  2.8min   \n","\n","[Parallel(n_jobs=-1)]: Done  16 out of  24 | elapsed:  4.7min remaining:  2.4min   \n","\n","[Parallel(n_jobs=-1)]: Done  17 out of  24 | elapsed:  4.7min remaining:  1.9min   \n","\n","[Parallel(n_jobs=-1)]: Done  18 out of  24 | elapsed:  4.7min remaining:  1.6min   \n","\n","[Parallel(n_jobs=-1)]: Done  19 out of  24 | elapsed:  4.7min remaining:  1.2min   \n","\n","[Parallel(n_jobs=-1)]: Done  20 out of  24 | elapsed:  4.8min remaining:   56.9s   \n","\n","[Parallel(n_jobs=-1)]: Done  21 out of  24 | elapsed:  4.8min remaining:   40.7s   \n","\n","[Parallel(n_jobs=-1)]: Done  22 out of  24 | elapsed:  4.8min remaining:   25.8s   \n","\n","[Parallel(n_jobs=-1)]: Done  24 out of  24 | elapsed:  5.1min remaining:    0.0s   \n","\n","[Parallel(n_jobs=-1)]: Done  24 out of  24 | elapsed:  5.1min finished             \n","\n","Macro-F1: 0.502 (0.008)                                                            \n","[LightGBM] [Warning] Unknown parameter: colsample_by_tree                          \n","Accuracy: 0.45782431646305993                                                      \n","Macro-F1 score: 0.4319313881781615                                                 \n","[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.       \n","\n","[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:  1.6min                      \n","\n","[Parallel(n_jobs=-1)]: Done   2 out of  24 | elapsed:  1.6min remaining: 17.6min   \n","\n","[Parallel(n_jobs=-1)]: Done   3 out of  24 | elapsed:  1.6min remaining: 11.2min   \n","\n","[Parallel(n_jobs=-1)]: Done   4 out of  24 | elapsed:  1.6min remaining:  8.0min   \n","\n","[Parallel(n_jobs=-1)]: Done   5 out of  24 | elapsed:  1.6min remaining:  6.1min   \n","\n","[Parallel(n_jobs=-1)]: Done   6 out of  24 | elapsed:  1.6min remaining:  4.9min   \n","\n","[Parallel(n_jobs=-1)]: Done   7 out of  24 | elapsed:  1.7min remaining:  4.0min   \n","\n","[Parallel(n_jobs=-1)]: Done   8 out of  24 | elapsed:  1.7min remaining:  3.4min   \n","\n","[Parallel(n_jobs=-1)]: Done   9 out of  24 | elapsed:  1.7min remaining:  2.8min   \n","\n","[Parallel(n_jobs=-1)]: Done  10 out of  24 | elapsed:  1.7min remaining:  2.4min   \n","\n","[Parallel(n_jobs=-1)]: Done  11 out of  24 | elapsed:  1.8min remaining:  2.1min   \n","\n","[Parallel(n_jobs=-1)]: Done  12 out of  24 | elapsed:  2.1min remaining:  2.1min   \n","\n","[Parallel(n_jobs=-1)]: Done  13 out of  24 | elapsed:  3.0min remaining:  2.5min   \n","\n","[Parallel(n_jobs=-1)]: Done  14 out of  24 | elapsed:  3.0min remaining:  2.2min   \n","\n","[Parallel(n_jobs=-1)]: Done  15 out of  24 | elapsed:  3.0min remaining:  1.8min   \n","\n","[Parallel(n_jobs=-1)]: Done  16 out of  24 | elapsed:  3.0min remaining:  1.5min   \n","\n","[Parallel(n_jobs=-1)]: Done  17 out of  24 | elapsed:  3.0min remaining:  1.3min   \n","\n","[Parallel(n_jobs=-1)]: Done  18 out of  24 | elapsed:  3.1min remaining:  1.0min   \n","\n","[Parallel(n_jobs=-1)]: Done  19 out of  24 | elapsed:  3.1min remaining:   48.9s   \n","\n","[Parallel(n_jobs=-1)]: Done  20 out of  24 | elapsed:  3.1min remaining:   37.3s   \n","\n","[Parallel(n_jobs=-1)]: Done  21 out of  24 | elapsed:  3.1min remaining:   26.7s   \n","\n","[Parallel(n_jobs=-1)]: Done  22 out of  24 | elapsed:  3.2min remaining:   17.1s   \n","\n","[Parallel(n_jobs=-1)]: Done  24 out of  24 | elapsed:  3.4min remaining:    0.0s   \n","\n","[Parallel(n_jobs=-1)]: Done  24 out of  24 | elapsed:  3.4min finished             \n","\n","Macro-F1: 0.647 (0.008)                                                            \n","[LightGBM] [Warning] Unknown parameter: colsample_by_tree                          \n","Accuracy: 0.5474112856311809                                                       \n","Macro-F1 score: 0.5437972787110719                                                 \n","[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.       \n","\n","[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:  1.0min                      \n","\n","[Parallel(n_jobs=-1)]: Done   2 out of  24 | elapsed:  1.1min remaining: 12.2min   \n","\n","[Parallel(n_jobs=-1)]: Done   3 out of  24 | elapsed:  1.1min remaining:  7.8min   \n","\n","[Parallel(n_jobs=-1)]: Done   4 out of  24 | elapsed:  1.2min remaining:  5.8min   \n","\n","[Parallel(n_jobs=-1)]: Done   5 out of  24 | elapsed:  1.2min remaining:  4.4min   \n","\n","[Parallel(n_jobs=-1)]: Done   6 out of  24 | elapsed:  1.2min remaining:  3.5min   \n","\n","[Parallel(n_jobs=-1)]: Done   7 out of  24 | elapsed:  1.2min remaining:  3.0min   \n","\n","[Parallel(n_jobs=-1)]: Done   8 out of  24 | elapsed:  1.2min remaining:  2.5min   \n","\n","[Parallel(n_jobs=-1)]: Done   9 out of  24 | elapsed:  1.3min remaining:  2.2min   \n","\n","[Parallel(n_jobs=-1)]: Done  10 out of  24 | elapsed:  1.5min remaining:  2.1min   \n","\n","[Parallel(n_jobs=-1)]: Done  11 out of  24 | elapsed:  2.0min remaining:  2.4min   \n","\n","[Parallel(n_jobs=-1)]: Done  12 out of  24 | elapsed:  2.2min remaining:  2.2min   \n","\n","[Parallel(n_jobs=-1)]: Done  13 out of  24 | elapsed:  2.3min remaining:  1.9min   \n","\n","[Parallel(n_jobs=-1)]: Done  14 out of  24 | elapsed:  2.3min remaining:  1.6min   \n","\n","[Parallel(n_jobs=-1)]: Done  15 out of  24 | elapsed:  2.3min remaining:  1.4min   \n","\n","[Parallel(n_jobs=-1)]: Done  16 out of  24 | elapsed:  2.4min remaining:  1.2min   \n","\n","[Parallel(n_jobs=-1)]: Done  17 out of  24 | elapsed:  2.4min remaining:   59.8s   \n","\n","[Parallel(n_jobs=-1)]: Done  18 out of  24 | elapsed:  2.4min remaining:   48.4s   \n","\n","[Parallel(n_jobs=-1)]: Done  19 out of  24 | elapsed:  2.4min remaining:   38.3s   \n","\n","[Parallel(n_jobs=-1)]: Done  20 out of  24 | elapsed:  2.5min remaining:   29.4s   \n","\n","[Parallel(n_jobs=-1)]: Done  21 out of  24 | elapsed:  2.6min remaining:   22.0s   \n","\n","[Parallel(n_jobs=-1)]: Done  22 out of  24 | elapsed:  2.8min remaining:   15.0s   \n","\n","[Parallel(n_jobs=-1)]: Done  24 out of  24 | elapsed:  3.1min remaining:    0.0s   \n","\n","[Parallel(n_jobs=-1)]: Done  24 out of  24 | elapsed:  3.1min finished             \n","\n","Macro-F1: 0.424 (0.008)                                                            \n","[LightGBM] [Warning] Unknown parameter: colsample_by_tree                          \n","Accuracy: 0.41128563118091915                                                      \n","Macro-F1 score: 0.35372866848552936                                                \n","[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.       \n","\n","[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:  1.6min                      \n","\n","[Parallel(n_jobs=-1)]: Done   2 out of  24 | elapsed:  1.6min remaining: 17.6min   \n","\n","[Parallel(n_jobs=-1)]: Done   3 out of  24 | elapsed:  1.6min remaining: 11.3min   \n","\n","[Parallel(n_jobs=-1)]: Done   4 out of  24 | elapsed:  1.6min remaining:  8.1min   \n","\n","[Parallel(n_jobs=-1)]: Done   5 out of  24 | elapsed:  1.6min remaining:  6.2min   \n","\n","[Parallel(n_jobs=-1)]: Done   6 out of  24 | elapsed:  1.6min remaining:  4.9min   \n","\n","[Parallel(n_jobs=-1)]: Done   7 out of  24 | elapsed:  1.6min remaining:  3.9min   \n","\n","[Parallel(n_jobs=-1)]: Done   8 out of  24 | elapsed:  1.6min remaining:  3.3min   \n","\n","[Parallel(n_jobs=-1)]: Done   9 out of  24 | elapsed:  1.7min remaining:  2.8min   \n","\n","[Parallel(n_jobs=-1)]: Done  10 out of  24 | elapsed:  1.8min remaining:  2.5min   \n","\n","[Parallel(n_jobs=-1)]: Done  11 out of  24 | elapsed:  1.9min remaining:  2.3min   \n","\n","[Parallel(n_jobs=-1)]: Done  12 out of  24 | elapsed:  2.0min remaining:  2.0min   \n","\n","[Parallel(n_jobs=-1)]: Done  13 out of  24 | elapsed:  3.2min remaining:  2.7min   \n","\n","[Parallel(n_jobs=-1)]: Done  14 out of  24 | elapsed:  3.2min remaining:  2.3min   \n","\n","[Parallel(n_jobs=-1)]: Done  15 out of  24 | elapsed:  3.2min remaining:  1.9min   \n","\n","[Parallel(n_jobs=-1)]: Done  16 out of  24 | elapsed:  3.2min remaining:  1.6min   \n","\n","[Parallel(n_jobs=-1)]: Done  17 out of  24 | elapsed:  3.3min remaining:  1.3min   \n","\n","[Parallel(n_jobs=-1)]: Done  18 out of  24 | elapsed:  3.3min remaining:  1.1min   \n","\n","[Parallel(n_jobs=-1)]: Done  19 out of  24 | elapsed:  3.3min remaining:   51.7s   \n","\n","[Parallel(n_jobs=-1)]: Done  20 out of  24 | elapsed:  3.3min remaining:   39.3s   \n","\n","[Parallel(n_jobs=-1)]: Done  21 out of  24 | elapsed:  3.3min remaining:   28.2s   \n","\n","[Parallel(n_jobs=-1)]: Done  22 out of  24 | elapsed:  3.4min remaining:   18.3s   \n","\n","[Parallel(n_jobs=-1)]: Done  24 out of  24 | elapsed:  3.5min remaining:    0.0s   \n","\n","[Parallel(n_jobs=-1)]: Done  24 out of  24 | elapsed:  3.5min finished             \n","\n","Macro-F1: 0.604 (0.008)                                                            \n","[LightGBM] [Warning] Unknown parameter: colsample_by_tree                          \n","Accuracy: 0.5287958115183246                                                       \n","Macro-F1 score: 0.5236304791099311                                                 \n","[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.       \n","\n","[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:  1.2min                      \n","\n","[Parallel(n_jobs=-1)]: Done   2 out of  24 | elapsed:  1.2min remaining: 13.2min   \n","\n","[Parallel(n_jobs=-1)]: Done   3 out of  24 | elapsed:  1.2min remaining:  8.4min   \n","\n","[Parallel(n_jobs=-1)]: Done   4 out of  24 | elapsed:  1.2min remaining:  6.1min   \n","\n","[Parallel(n_jobs=-1)]: Done   5 out of  24 | elapsed:  1.2min remaining:  4.6min   \n","\n","[Parallel(n_jobs=-1)]: Done   6 out of  24 | elapsed:  1.2min remaining:  3.7min   \n","\n","[Parallel(n_jobs=-1)]: Done   7 out of  24 | elapsed:  1.2min remaining:  3.0min   \n","\n","[Parallel(n_jobs=-1)]: Done   8 out of  24 | elapsed:  1.2min remaining:  2.5min   \n","\n","[Parallel(n_jobs=-1)]: Done   9 out of  24 | elapsed:  1.2min remaining:  2.1min   \n","\n","[Parallel(n_jobs=-1)]: Done  10 out of  24 | elapsed:  1.2min remaining:  1.7min   \n","\n","[Parallel(n_jobs=-1)]: Done  11 out of  24 | elapsed:  1.2min remaining:  1.5min   \n","\n","[Parallel(n_jobs=-1)]: Done  12 out of  24 | elapsed:  1.3min remaining:  1.3min   \n","\n","[Parallel(n_jobs=-1)]: Done  13 out of  24 | elapsed:  2.2min remaining:  1.9min   \n","\n","[Parallel(n_jobs=-1)]: Done  14 out of  24 | elapsed:  2.3min remaining:  1.6min     \n","\n","[Parallel(n_jobs=-1)]: Done  15 out of  24 | elapsed:  2.3min remaining:  1.4min     \n","\n","[Parallel(n_jobs=-1)]: Done  16 out of  24 | elapsed:  2.3min remaining:  1.2min     \n","\n","[Parallel(n_jobs=-1)]: Done  17 out of  24 | elapsed:  2.3min remaining:   57.1s     \n","\n","[Parallel(n_jobs=-1)]: Done  18 out of  24 | elapsed:  2.3min remaining:   46.2s     \n","\n","[Parallel(n_jobs=-1)]: Done  19 out of  24 | elapsed:  2.3min remaining:   36.6s     \n","\n","[Parallel(n_jobs=-1)]: Done  20 out of  24 | elapsed:  2.3min remaining:   27.8s     \n","\n","[Parallel(n_jobs=-1)]: Done  21 out of  24 | elapsed:  2.3min remaining:   19.9s     \n","\n","[Parallel(n_jobs=-1)]: Done  22 out of  24 | elapsed:  2.3min remaining:   12.6s     \n","\n","[Parallel(n_jobs=-1)]: Done  24 out of  24 | elapsed:  2.4min remaining:    0.0s     \n","\n","[Parallel(n_jobs=-1)]: Done  24 out of  24 | elapsed:  2.4min finished               \n","\n","Macro-F1: 0.420 (0.007)                                                              \n","[LightGBM] [Warning] Unknown parameter: colsample_by_tree                            \n","Accuracy: 0.40430482838859805                                                        \n","Macro-F1 score: 0.3460653720644091                                                   \n","[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.         \n","\n","[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:  1.6min                        \n","\n","[Parallel(n_jobs=-1)]: Done   2 out of  24 | elapsed:  1.7min remaining: 18.3min     \n","\n","[Parallel(n_jobs=-1)]: Done   3 out of  24 | elapsed:  1.7min remaining: 11.8min     \n","\n","[Parallel(n_jobs=-1)]: Done   4 out of  24 | elapsed:  1.7min remaining:  8.5min     \n","\n","[Parallel(n_jobs=-1)]: Done   5 out of  24 | elapsed:  1.7min remaining:  6.5min     \n","\n","[Parallel(n_jobs=-1)]: Done   6 out of  24 | elapsed:  1.7min remaining:  5.1min     \n","\n","[Parallel(n_jobs=-1)]: Done   7 out of  24 | elapsed:  1.7min remaining:  4.2min     \n","\n","[Parallel(n_jobs=-1)]: Done   8 out of  24 | elapsed:  1.7min remaining:  3.5min     \n","\n","[Parallel(n_jobs=-1)]: Done   9 out of  24 | elapsed:  1.8min remaining:  2.9min     \n","\n","[Parallel(n_jobs=-1)]: Done  10 out of  24 | elapsed:  1.8min remaining:  2.5min     \n","\n","[Parallel(n_jobs=-1)]: Done  11 out of  24 | elapsed:  1.8min remaining:  2.1min     \n","\n","[Parallel(n_jobs=-1)]: Done  12 out of  24 | elapsed:  1.8min remaining:  1.8min     \n","\n","[Parallel(n_jobs=-1)]: Done  13 out of  24 | elapsed:  3.3min remaining:  2.8min     \n","\n","[Parallel(n_jobs=-1)]: Done  14 out of  24 | elapsed:  3.3min remaining:  2.4min     \n","\n","[Parallel(n_jobs=-1)]: Done  15 out of  24 | elapsed:  3.4min remaining:  2.0min     \n","\n","[Parallel(n_jobs=-1)]: Done  16 out of  24 | elapsed:  3.4min remaining:  1.7min     \n","\n","[Parallel(n_jobs=-1)]: Done  17 out of  24 | elapsed:  3.4min remaining:  1.4min     \n","\n","[Parallel(n_jobs=-1)]: Done  18 out of  24 | elapsed:  3.4min remaining:  1.1min     \n","\n","[Parallel(n_jobs=-1)]: Done  19 out of  24 | elapsed:  3.5min remaining:   54.4s     \n","\n","[Parallel(n_jobs=-1)]: Done  20 out of  24 | elapsed:  3.5min remaining:   41.4s     \n","\n","[Parallel(n_jobs=-1)]: Done  21 out of  24 | elapsed:  3.5min remaining:   30.1s     \n","\n","[Parallel(n_jobs=-1)]: Done  22 out of  24 | elapsed:  3.5min remaining:   19.2s     \n","\n","[Parallel(n_jobs=-1)]: Done  24 out of  24 | elapsed:  3.6min remaining:    0.0s     \n","\n","[Parallel(n_jobs=-1)]: Done  24 out of  24 | elapsed:  3.6min finished               \n","\n","Macro-F1: 0.405 (0.008)                                                              \n","[LightGBM] [Warning] Unknown parameter: colsample_by_tree                            \n","Accuracy: 0.401977894124491                                                          \n","Macro-F1 score: 0.3399406551243971                                                   \n","[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.         \n","\n","[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:  4.0min                        \n","\n","[Parallel(n_jobs=-1)]: Done   2 out of  24 | elapsed:  4.0min remaining: 44.4min     \n","\n","[Parallel(n_jobs=-1)]: Done   3 out of  24 | elapsed:  4.0min remaining: 28.3min     \n","\n","[Parallel(n_jobs=-1)]: Done   4 out of  24 | elapsed:  4.1min remaining: 20.4min     \n","\n","[Parallel(n_jobs=-1)]: Done   5 out of  24 | elapsed:  4.1min remaining: 15.5min     \n","\n","[Parallel(n_jobs=-1)]: Done   6 out of  24 | elapsed:  4.1min remaining: 12.3min     \n","\n","[Parallel(n_jobs=-1)]: Done   7 out of  24 | elapsed:  4.1min remaining: 10.0min     \n","\n","[Parallel(n_jobs=-1)]: Done   8 out of  24 | elapsed:  4.1min remaining:  8.3min     \n","\n","[Parallel(n_jobs=-1)]: Done   9 out of  24 | elapsed:  4.1min remaining:  6.9min     \n","\n","[Parallel(n_jobs=-1)]: Done  10 out of  24 | elapsed:  4.1min remaining:  5.8min     \n","\n","[Parallel(n_jobs=-1)]: Done  11 out of  24 | elapsed:  4.2min remaining:  4.9min     \n","\n","[Parallel(n_jobs=-1)]: Done  12 out of  24 | elapsed:  4.2min remaining:  4.2min     \n","\n","[Parallel(n_jobs=-1)]: Done  13 out of  24 | elapsed:  7.6min remaining:  6.5min     \n","\n","[Parallel(n_jobs=-1)]: Done  14 out of  24 | elapsed:  7.7min remaining:  5.5min     \n","\n","[Parallel(n_jobs=-1)]: Done  15 out of  24 | elapsed:  7.8min remaining:  4.7min     \n","\n","[Parallel(n_jobs=-1)]: Done  16 out of  24 | elapsed:  7.8min remaining:  3.9min     \n","\n","[Parallel(n_jobs=-1)]: Done  17 out of  24 | elapsed:  7.8min remaining:  3.2min     \n","\n","[Parallel(n_jobs=-1)]: Done  18 out of  24 | elapsed:  7.8min remaining:  2.6min     \n","\n","[Parallel(n_jobs=-1)]: Done  19 out of  24 | elapsed:  7.8min remaining:  2.1min     \n","\n","[Parallel(n_jobs=-1)]: Done  20 out of  24 | elapsed:  7.8min remaining:  1.6min     \n","\n","[Parallel(n_jobs=-1)]: Done  21 out of  24 | elapsed:  7.9min remaining:  1.1min     \n","\n","[Parallel(n_jobs=-1)]: Done  22 out of  24 | elapsed:  7.9min remaining:   42.8s     \n","\n","[Parallel(n_jobs=-1)]: Done  24 out of  24 | elapsed:  7.9min remaining:    0.0s     \n","\n","[Parallel(n_jobs=-1)]: Done  24 out of  24 | elapsed:  7.9min finished               \n","\n","Macro-F1: 0.644 (0.009)                                                              \n","[LightGBM] [Warning] Unknown parameter: colsample_by_tree                            \n","Accuracy: 0.5491564863292612                                                         \n","Macro-F1 score: 0.5454126679462572                                                   \n","[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.         \n","\n","[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:  1.6min                        \n","\n","[Parallel(n_jobs=-1)]: Done   2 out of  24 | elapsed:  1.6min remaining: 17.9min     \n","\n","[Parallel(n_jobs=-1)]: Done   3 out of  24 | elapsed:  1.6min remaining: 11.5min     \n","\n","[Parallel(n_jobs=-1)]: Done   4 out of  24 | elapsed:  1.6min remaining:  8.2min     \n","\n","[Parallel(n_jobs=-1)]: Done   5 out of  24 | elapsed:  1.6min remaining:  6.3min     \n","\n","[Parallel(n_jobs=-1)]: Done   6 out of  24 | elapsed:  1.7min remaining:  5.0min     \n","\n","[Parallel(n_jobs=-1)]: Done   7 out of  24 | elapsed:  1.7min remaining:  4.0min     \n","\n","[Parallel(n_jobs=-1)]: Done   8 out of  24 | elapsed:  1.7min remaining:  3.3min     \n","\n","[Parallel(n_jobs=-1)]: Done   9 out of  24 | elapsed:  1.7min remaining:  2.8min     \n","\n","[Parallel(n_jobs=-1)]: Done  10 out of  24 | elapsed:  1.7min remaining:  2.3min     \n","\n","[Parallel(n_jobs=-1)]: Done  11 out of  24 | elapsed:  1.7min remaining:  2.0min     \n","\n","[Parallel(n_jobs=-1)]: Done  12 out of  24 | elapsed:  1.7min remaining:  1.7min     \n","\n","[Parallel(n_jobs=-1)]: Done  13 out of  24 | elapsed:  3.2min remaining:  2.7min     \n","\n","[Parallel(n_jobs=-1)]: Done  14 out of  24 | elapsed:  3.2min remaining:  2.3min     \n","\n","[Parallel(n_jobs=-1)]: Done  15 out of  24 | elapsed:  3.3min remaining:  2.0min     \n","\n","[Parallel(n_jobs=-1)]: Done  16 out of  24 | elapsed:  3.3min remaining:  1.6min     \n","\n","[Parallel(n_jobs=-1)]: Done  17 out of  24 | elapsed:  3.3min remaining:  1.4min     \n","\n","[Parallel(n_jobs=-1)]: Done  18 out of  24 | elapsed:  3.3min remaining:  1.1min     \n","\n","[Parallel(n_jobs=-1)]: Done  19 out of  24 | elapsed:  3.3min remaining:   52.1s     \n","\n","[Parallel(n_jobs=-1)]: Done  20 out of  24 | elapsed:  3.3min remaining:   39.6s     \n","\n","[Parallel(n_jobs=-1)]: Done  21 out of  24 | elapsed:  3.3min remaining:   28.3s     \n","\n","[Parallel(n_jobs=-1)]: Done  22 out of  24 | elapsed:  3.3min remaining:   18.1s     \n","\n","[Parallel(n_jobs=-1)]: Done  24 out of  24 | elapsed:  3.3min remaining:    0.0s     \n","\n","[Parallel(n_jobs=-1)]: Done  24 out of  24 | elapsed:  3.3min finished               \n","\n","Macro-F1: 0.687 (0.008)                                                              \n","[LightGBM] [Warning] Unknown parameter: colsample_by_tree                            \n","Accuracy: 0.5817335660267597                                                         \n","Macro-F1 score: 0.5796161120950234                                                   \n","[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.         \n","\n","[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:  1.7min                        \n","\n","[Parallel(n_jobs=-1)]: Done   2 out of  24 | elapsed:  1.7min remaining: 18.4min     \n","\n","[Parallel(n_jobs=-1)]: Done   3 out of  24 | elapsed:  1.7min remaining: 11.9min     \n","\n","[Parallel(n_jobs=-1)]: Done   4 out of  24 | elapsed:  1.7min remaining:  8.5min     \n","\n","[Parallel(n_jobs=-1)]: Done   5 out of  24 | elapsed:  1.7min remaining:  6.5min     \n","\n","[Parallel(n_jobs=-1)]: Done   6 out of  24 | elapsed:  1.7min remaining:  5.2min     \n","\n","[Parallel(n_jobs=-1)]: Done   7 out of  24 | elapsed:  1.7min remaining:  4.2min     \n","\n","[Parallel(n_jobs=-1)]: Done   8 out of  24 | elapsed:  1.7min remaining:  3.5min     \n","\n","[Parallel(n_jobs=-1)]: Done   9 out of  24 | elapsed:  1.8min remaining:  2.9min     \n","\n","[Parallel(n_jobs=-1)]: Done  10 out of  24 | elapsed:  1.8min remaining:  2.5min     \n","\n","[Parallel(n_jobs=-1)]: Done  11 out of  24 | elapsed:  1.8min remaining:  2.1min     \n","\n","[Parallel(n_jobs=-1)]: Done  12 out of  24 | elapsed:  1.8min remaining:  1.8min     \n","\n","[Parallel(n_jobs=-1)]: Done  13 out of  24 | elapsed:  3.2min remaining:  2.7min     \n","\n","[Parallel(n_jobs=-1)]: Done  14 out of  24 | elapsed:  3.3min remaining:  2.4min     \n","\n","[Parallel(n_jobs=-1)]: Done  15 out of  24 | elapsed:  3.4min remaining:  2.0min     \n","\n","[Parallel(n_jobs=-1)]: Done  16 out of  24 | elapsed:  3.4min remaining:  1.7min     \n","\n","[Parallel(n_jobs=-1)]: Done  17 out of  24 | elapsed:  3.4min remaining:  1.4min     \n","\n","[Parallel(n_jobs=-1)]: Done  18 out of  24 | elapsed:  3.4min remaining:  1.1min     \n","\n","[Parallel(n_jobs=-1)]: Done  19 out of  24 | elapsed:  3.4min remaining:   54.3s     \n","\n","[Parallel(n_jobs=-1)]: Done  20 out of  24 | elapsed:  3.5min remaining:   41.4s     \n","\n","[Parallel(n_jobs=-1)]: Done  21 out of  24 | elapsed:  3.5min remaining:   29.6s     \n","\n","[Parallel(n_jobs=-1)]: Done  22 out of  24 | elapsed:  3.5min remaining:   18.8s     \n","\n","[Parallel(n_jobs=-1)]: Done  24 out of  24 | elapsed:  3.5min remaining:    0.0s     \n","\n","[Parallel(n_jobs=-1)]: Done  24 out of  24 | elapsed:  3.5min finished               \n","\n","Macro-F1: 0.426 (0.008)                                                              \n","[LightGBM] [Warning] Unknown parameter: colsample_by_tree                            \n","Accuracy: 0.40372309482257124                                                        \n","Macro-F1 score: 0.3466658163066665                                                   \n","[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.         \n","\n","[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:  1.2min                        \n","\n","[Parallel(n_jobs=-1)]: Done   2 out of  24 | elapsed:  1.2min remaining: 13.5min     \n","\n","[Parallel(n_jobs=-1)]: Done   3 out of  24 | elapsed:  1.3min remaining:  8.9min     \n","\n","[Parallel(n_jobs=-1)]: Done   4 out of  24 | elapsed:  1.3min remaining:  6.4min     \n","\n","[Parallel(n_jobs=-1)]: Done   5 out of  24 | elapsed:  1.3min remaining:  4.9min     \n","\n","[Parallel(n_jobs=-1)]: Done   6 out of  24 | elapsed:  1.3min remaining:  3.9min     \n","\n","[Parallel(n_jobs=-1)]: Done   7 out of  24 | elapsed:  1.3min remaining:  3.2min     \n","\n","[Parallel(n_jobs=-1)]: Done   8 out of  24 | elapsed:  1.3min remaining:  2.6min     \n","\n","[Parallel(n_jobs=-1)]: Done   9 out of  24 | elapsed:  1.3min remaining:  2.3min     \n","\n","[Parallel(n_jobs=-1)]: Done  10 out of  24 | elapsed:  1.4min remaining:  1.9min     \n","\n","[Parallel(n_jobs=-1)]: Done  11 out of  24 | elapsed:  1.4min remaining:  1.6min     \n","\n","[Parallel(n_jobs=-1)]: Done  12 out of  24 | elapsed:  1.4min remaining:  1.4min     \n","\n","[Parallel(n_jobs=-1)]: Done  13 out of  24 | elapsed:  2.4min remaining:  2.0min     \n","\n","[Parallel(n_jobs=-1)]: Done  14 out of  24 | elapsed:  2.5min remaining:  1.8min     \n","\n","[Parallel(n_jobs=-1)]: Done  15 out of  24 | elapsed:  2.5min remaining:  1.5min     \n","\n","[Parallel(n_jobs=-1)]: Done  16 out of  24 | elapsed:  2.5min remaining:  1.3min     \n","\n","[Parallel(n_jobs=-1)]: Done  17 out of  24 | elapsed:  2.5min remaining:  1.0min     \n","\n","[Parallel(n_jobs=-1)]: Done  18 out of  24 | elapsed:  2.6min remaining:   51.1s     \n","\n","[Parallel(n_jobs=-1)]: Done  19 out of  24 | elapsed:  2.6min remaining:   40.4s     \n","\n","[Parallel(n_jobs=-1)]: Done  20 out of  24 | elapsed:  2.6min remaining:   30.8s     \n","\n","[Parallel(n_jobs=-1)]: Done  21 out of  24 | elapsed:  2.6min remaining:   22.1s     \n","\n","[Parallel(n_jobs=-1)]: Done  22 out of  24 | elapsed:  2.6min remaining:   14.1s     \n","\n","[Parallel(n_jobs=-1)]: Done  24 out of  24 | elapsed:  2.6min remaining:    0.0s     \n","\n","[Parallel(n_jobs=-1)]: Done  24 out of  24 | elapsed:  2.6min finished               \n","\n","Macro-F1: 0.428 (0.012)                                                              \n","[LightGBM] [Warning] Unknown parameter: colsample_by_tree                            \n","Accuracy: 0.4083769633507853                                                         \n","Macro-F1 score: 0.35176500993549253                                                  \n","100%|██████████| 20/20 [1:24:03<00:00, 252.15s/trial, best loss: -0.6443235856247369]\n","\n","##### Results\n","Score best parameters:  0.6443235856247369\n","Best parameters:  {'boosting_type': 0, 'colsample_by_tree': 0.8360954191582883, 'learning_rate': 0.17292549690407305, 'max_depth': 34.0, 'min_child_samples': 160.0, 'n_estimators': 94.0, 'n_jobs': 0, 'num_leaves': 17.0, 'random_state': 0, 'scale_pos_weight': 1.2405835005274635, 'subsample': 0.7206617671584264}\n"]},{"ename":"NameError","evalue":"name 'clf_best' is not defined","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[1;32mc:\\Users\\issac\\Desktop\\SUTD\\School Year\\Year 2\\Term 5\\50.007 Machine Learning\\Project\\work\\2022-50-007-ml-project.ipynb Cell 249\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/issac/Desktop/SUTD/School%20Year/Year%202/Term%205/50.007%20Machine%20Learning/Project/work/2022-50-007-ml-project.ipynb#ch0000248?line=0'>1</a>\u001b[0m iterations, best_params \u001b[39m=\u001b[39m hyperopt(LGBM_params, LGBM,  X_res, y_res, X_trans, y_valid, \u001b[39m20\u001b[39;49m)\n","\u001b[1;32mc:\\Users\\issac\\Desktop\\SUTD\\School Year\\Year 2\\Term 5\\50.007 Machine Learning\\Project\\work\\2022-50-007-ml-project.ipynb Cell 249\u001b[0m in \u001b[0;36mhyperopt\u001b[1;34m(param_space, model, X_train, y_train, X_test, y_test, num_eval)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/issac/Desktop/SUTD/School%20Year/Year%202/Term%205/50.007%20Machine%20Learning/Project/work/2022-50-007-ml-project.ipynb#ch0000248?line=28'>29</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mScore best parameters: \u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mmin\u001b[39m(loss)\u001b[39m*\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/issac/Desktop/SUTD/School%20Year/Year%202/Term%205/50.007%20Machine%20Learning/Project/work/2022-50-007-ml-project.ipynb#ch0000248?line=29'>30</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mBest parameters: \u001b[39m\u001b[39m\"\u001b[39m, best_param)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/issac/Desktop/SUTD/School%20Year/Year%202/Term%205/50.007%20Machine%20Learning/Project/work/2022-50-007-ml-project.ipynb#ch0000248?line=30'>31</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mTest Score: \u001b[39m\u001b[39m\"\u001b[39m, score(y_test, clf_best\u001b[39m.\u001b[39mpredict(X_test)))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/issac/Desktop/SUTD/School%20Year/Year%202/Term%205/50.007%20Machine%20Learning/Project/work/2022-50-007-ml-project.ipynb#ch0000248?line=31'>32</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mTime elapsed: \u001b[39m\u001b[39m\"\u001b[39m, time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m start)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/issac/Desktop/SUTD/School%20Year/Year%202/Term%205/50.007%20Machine%20Learning/Project/work/2022-50-007-ml-project.ipynb#ch0000248?line=32'>33</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mParameter combinations evaluated: \u001b[39m\u001b[39m\"\u001b[39m, num_eval)\n","\u001b[1;31mNameError\u001b[0m: name 'clf_best' is not defined"]}],"source":["iterations, best_params = hyperopt(LGBM_params, LGBM,  X_res, y_res, X_trans, y_valid, 20)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["NuSVC_params = {\n","    'kernel': hp.choice('kernel', ['rbf', 'linear', 'sigmoid']),\n","    'nu': hp.uniform('nu', 0.1, 1.0),\n","    'gamma': hp.uniform('gamma', 1e-2, 1),\n","    'class_weight': hp.choice('class_weight', [None, 'balanced']),\n","    'max_iter': hp.choice('max_iter', [100, 200]),\n","    'random_state': hp.choice('random_state', [-1, 1000]),\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n","\n","[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed: 10.8min\n","\n","[Parallel(n_jobs=-1)]: Done   2 out of  24 | elapsed: 10.9min remaining: 120.4min\n","\n","[Parallel(n_jobs=-1)]: Done   3 out of  24 | elapsed: 10.9min remaining: 76.6min\n","\n","[Parallel(n_jobs=-1)]: Done   4 out of  24 | elapsed: 11.0min remaining: 54.8min\n","\n","[Parallel(n_jobs=-1)]: Done   5 out of  24 | elapsed: 11.0min remaining: 41.6min\n","\n","[Parallel(n_jobs=-1)]: Done   6 out of  24 | elapsed: 11.0min remaining: 32.9min\n","\n","[Parallel(n_jobs=-1)]: Done   7 out of  24 | elapsed: 11.0min remaining: 26.7min\n","\n","[Parallel(n_jobs=-1)]: Done   8 out of  24 | elapsed: 11.1min remaining: 22.1min\n","\n","[Parallel(n_jobs=-1)]: Done   9 out of  24 | elapsed: 11.1min remaining: 18.5min\n","\n","[Parallel(n_jobs=-1)]: Done  10 out of  24 | elapsed: 11.1min remaining: 15.5min\n","\n","[Parallel(n_jobs=-1)]: Done  11 out of  24 | elapsed: 11.1min remaining: 13.1min\n","\n","[Parallel(n_jobs=-1)]: Done  12 out of  24 | elapsed: 11.5min remaining: 11.5min\n","\n","  0%|          | 0/2 [11:31<?, ?trial/s, best loss=?]"]}],"source":["iterations, best_params = hyperopt(NuSVC_params, NuSVC,  X_res, y_res, X_trans, y_valid, 2)"]}],"metadata":{"kernelspec":{"display_name":"Python 3.10.5 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.5"},"vscode":{"interpreter":{"hash":"d81bf16a70e6e7c42043031d5c6c9e6d55e73e0e666fe713b30c2db86836c05c"}}},"nbformat":4,"nbformat_minor":4}
